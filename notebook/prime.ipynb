{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import os.path as op\n",
    "from math import pi\n",
    "import shap\n",
    "from itertools import product\n",
    "from tqdm.notebook import tqdm\n",
    "# from mne_icalabel import label_components\n",
    "import mne_features\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from fooof import FOOOFGroup\n",
    "from fooof import FOOOF\n",
    "import itertools\n",
    "from scipy.stats import chi2_contingency\n",
    "import fooof\n",
    "from mne.datasets import fetch_fsaverage\n",
    "from mne.minimum_norm import make_inverse_operator, apply_inverse, compute_source_psd_epochs, apply_inverse_epochs\n",
    "from mne_connectivity import spectral_connectivity_time\n",
    "from mne_connectivity.viz import plot_connectivity_circle\n",
    "import joblib\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. a. Loading and preprocessing the recordings (aaeeg data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to find the nearest event\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx]\n",
    "\n",
    "# selecting .vhdr files\n",
    "directory = '/Users/payamsadeghishabestari/eeg_data/Regensburg/transfer_147155_files_ac07b771/aaeeg_rsEEG' \n",
    "files_list = []\n",
    "for filename in sorted(os.listdir(directory)): ## iterate over files in that directory\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f) and f.endswith(\".vhdr\"):\n",
    "        files_list.append(f)\n",
    "\n",
    "# importing recordings all in a list\n",
    "sfreq = 250\n",
    "l_freq = .1 # Hz\n",
    "h_freq = 80 # Hz\n",
    "cut_out_sec = 3 # cutting out first few seconds to avoid eye related artifacts\n",
    "epoch_duration = 2 # seconds\n",
    "\n",
    "for file_vhdr in tqdm(files_list):\n",
    "    raw_rs = mne.io.read_raw_brainvision(vhdr_fname=file_vhdr, preload=True, verbose=False)\n",
    "    events = mne.events_from_annotations(raw_rs, verbose=False)\n",
    "    \n",
    "    # montaging\n",
    "    new_ch_names = raw_rs.info['ch_names'].copy()\n",
    "    new_ch_names[58] = 'Fpz'\n",
    "    mapping = dict(zip(raw_rs.info['ch_names'], new_ch_names))\n",
    "    mne.rename_channels(raw_rs.info, mapping)\n",
    "    raw_rs.set_montage('standard_1020', verbose=False, on_missing='warn')\n",
    "    \n",
    "    # resampling and filtering\n",
    "    raw_rs, events_resampled = raw_rs.resample(sfreq=sfreq, events=events[0], verbose=False)\n",
    "    raw_rs = raw_rs.filter(l_freq=l_freq, h_freq=h_freq, verbose=False) \n",
    "    raw_rs = raw_rs.notch_filter(freqs=50, verbose=False) \n",
    "    \n",
    "    # eye blink removal on whole recording\n",
    "    ica_again = 0\n",
    "    ica = mne.preprocessing.ICA(n_components=0.95, max_iter=800, method='infomax', fit_params=dict(extended=True))\n",
    "    ica.fit(raw_rs, verbose=False) \n",
    "    ic_labels = label_components(raw_rs, ica, method=\"iclabel\")\n",
    "    labels = ic_labels[\"labels\"]\n",
    "    exclude_idxs = [idx for idx, label in enumerate(labels) if label in [\"eye blink\"]] \n",
    "    ica.apply(raw_rs, exclude=exclude_idxs, verbose=False)\n",
    "    if len(exclude_idxs)==0:\n",
    "        print(f'could not find blinking artifact in subject {file_vhdr[-14:-10]}')\n",
    "        ica_again = 1\n",
    "    \n",
    "    # ec/eo stimuli timing\n",
    "    ec_start = np.array([event[0] for event in events_resampled if event[2]==11])  # stimulus 11: eyes_close\n",
    "    eo_start = np.array([event[0] for event in events_resampled if event[2]==31])  # stimulus 31: eyes_open\n",
    "\n",
    "    # fixing for some subjects:\n",
    "    if len(ec_start) == 4: # for subject index 26\n",
    "        ec_start = np.insert(ec_start, 0, 0) \n",
    "        eo_start = np.insert(eo_start, 2, 78939)\n",
    "    \n",
    "    if len(eo_start) == 4: # for subject indeices 0, 2, 6, 7, 15, 16, 17, 20, 21, 23, 24, 25, 28, 30, 33, 34, 43\n",
    "        possible_events = []\n",
    "        if eo_start[0] > 150 * sfreq:\n",
    "            for event in events_resampled:\n",
    "                if event[2]==33:\n",
    "                    if eo_start[0] > event[0]:\n",
    "                        possible_events.append(event[0])\n",
    "            possible_event = find_nearest(np.array(possible_events), 60 * sfreq)\n",
    "            eo_start = np.insert(eo_start, 0, possible_event)\n",
    "        \n",
    "        if eo_start[-1] < 500 * sfreq:\n",
    "            for event in events_resampled:\n",
    "                if event[2]==33:\n",
    "                    if eo_start[-1] < event[0]:\n",
    "                        possible_events.append(event[0])\n",
    "            possible_event = find_nearest(np.array(possible_events), 570 * sfreq)\n",
    "            eo_start = np.append(eo_start, possible_event)\n",
    "        \n",
    "        if (eo_start[0] < 150 * sfreq) and (eo_start[-1] > 500 * sfreq) and (len(eo_start) == 4):\n",
    "            index = np.where(np.diff(eo_start) > 200 * sfreq)[0][0] # where discontinuety happend\n",
    "            for event in events_resampled:\n",
    "                if event[2]==33:\n",
    "                    if eo_start[index+1] > event[0] & event[0] > eo_start[index]:\n",
    "                        possible_events.append(event[0])\n",
    "            possible_event = find_nearest(np.array(possible_events), (eo_start[3] + eo_start[2]) / 2)\n",
    "            eo_start = np.insert(eo_start, index + 1, possible_event)\n",
    "    \n",
    "        ec_start_t = ec_start / sfreq\n",
    "        eo_start_t = eo_start / sfreq\n",
    "        print(ec_start_t)\n",
    "        print(eo_start_t)\n",
    "    # cropping and concatenating the data\n",
    "    cropped_raw_eo = []\n",
    "    cropped_raw_ec = []\n",
    "    if ec_start[0] <= eo_start[0]:\n",
    "        for ec_st, eo_st in zip(ec_start_t, eo_start_t):\n",
    "            cropped_raw_ec.append(raw_rs.copy().crop(tmin=ec_st + cut_out_sec, tmax=eo_st))\n",
    "        for ec_st, eo_st in zip(ec_start_t[1:], eo_start_t[:-1]):\n",
    "            cropped_raw_eo.append(raw_rs.copy().crop(tmin=eo_st + 3, tmax=ec_st))\n",
    "    else:\n",
    "        for ec_st, eo_st in zip(ec_start_t, eo_start_t):\n",
    "            cropped_raw_eo.append(raw_rs.copy().crop(tmin=eo_st + 3, tmax=ec_st))\n",
    "        for ec_st, eo_st in zip(ec_start_t[:-1], eo_start_t[1:]):\n",
    "            cropped_raw_ec.append(raw_rs.copy().crop(tmin=ec_st + 3, tmax=eo_st))\n",
    "\n",
    "    raw_ec_concat = cropped_raw_ec[0]\n",
    "    raw_eo_concat = cropped_raw_eo[0]\n",
    "    for raw_cr in cropped_raw_ec[1:]:\n",
    "        raw_ec_concat = mne.io.concatenate_raws([raw_ec_concat, raw_cr], on_mismatch='ignore')\n",
    "    for raw_cr in cropped_raw_eo[1:]:\n",
    "        raw_eo_concat = mne.io.concatenate_raws([raw_eo_concat, raw_cr], on_mismatch='ignore')\n",
    "\n",
    "    # applying ica again on only eo condition, if no blink found previously\n",
    "    if ica_again == 1:\n",
    "        ica = mne.preprocessing.ICA(n_components=0.95, max_iter=800, method='infomax', fit_params=dict(extended=True))\n",
    "        ica.fit(raw_eo_concat, verbose=False) \n",
    "        ic_labels = label_components(raw_eo_concat, ica, method=\"iclabel\")\n",
    "        labels = ic_labels[\"labels\"]\n",
    "        exclude_idxs = [idx for idx, label in enumerate(labels) if label in [\"eye blink\"]] \n",
    "        ica.apply(raw_eo_concat, exclude=exclude_idxs, verbose=False)\n",
    "    \n",
    "    # average re-referencing\n",
    "    raw_eo_concat.set_eeg_reference(ref_channels='average', projection=False, verbose=False)\n",
    "    raw_ec_concat.set_eeg_reference(ref_channels='average', projection=False, verbose=False)\n",
    "    \n",
    "    # epoching (2- concat and epoching into 2 seconds)\n",
    "    epochs_eo = mne.make_fixed_length_epochs(raw_eo_concat, duration=epoch_duration, preload=True)\n",
    "    epochs_ec = mne.make_fixed_length_epochs(raw_ec_concat, duration=epoch_duration, preload=True)\n",
    "    \n",
    "    # saving epochs\n",
    "    fname_save_eo = f'/Users/payamsadeghishabestari/eeg_data/Regensburg/transfer_147155_files_ac07b771/aaeeg_rsEEG/epochs/{file_vhdr[-14:-10]}-eo-epo.fif' \n",
    "    fname_save_ec = f'/Users/payamsadeghishabestari/eeg_data/Regensburg/transfer_147155_files_ac07b771/aaeeg_rsEEG/epochs/{file_vhdr[-14:-10]}-ec-epo.fif' \n",
    "    epochs_eo.save(fname=fname_save_eo, overwrite=True, verbose=False)\n",
    "    epochs_ec.save(fname=fname_save_ec, overwrite=True, verbose=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. b. Loading and preprocessing the recordings (neuropren data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting .vhdr files\n",
    "directory = '/Users/payamsadeghishabestari/eeg_data/Regensburg/transfer_147155_files_ac07b771/neuropren_rsEEG' \n",
    "files_list = []\n",
    "for filename in sorted(os.listdir(directory)): ## iterate over files in that directory\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f) and f.endswith(\".vhdr\"):\n",
    "        files_list.append(f)\n",
    "\n",
    "# importing recordings all in a list\n",
    "sfreq = 250\n",
    "l_freq = .1 # Hz\n",
    "h_freq = 80 # Hz\n",
    "cut_out_sec = 3 # cutting out first few seconds to avoid eye related artifacts\n",
    "epoch_duration = 2 # seconds\n",
    "\n",
    "for file_vhdr in tqdm(files_list[17:]):\n",
    "    print(file_vhdr)\n",
    "    raw_rs = mne.io.read_raw_brainvision(vhdr_fname=file_vhdr, preload=True, verbose=False)\n",
    "    events = mne.events_from_annotations(raw_rs, verbose=False)\n",
    "    \n",
    "    # montaging (removing ecg channels for now)\n",
    "    new_ch_names = raw_rs.info['ch_names'].copy()\n",
    "    new_ch_names[58] = 'Fpz' # change FPz to Fpz\n",
    "    mapping = dict(zip(raw_rs.info['ch_names'], new_ch_names))\n",
    "    mne.rename_channels(raw_rs.info, mapping)\n",
    "    raw_rs.info[\"bads\"].append(\"HRli\")\n",
    "    raw_rs.info[\"bads\"].append(\"HRre\")\n",
    "    raw_rs.set_montage('standard_1020', verbose=False, on_missing='warn')\n",
    "    \n",
    "    # resampling and filtering\n",
    "    raw_rs, events_resampled = raw_rs.resample(sfreq=sfreq, events=events[0], verbose=False)\n",
    "    raw_rs = raw_rs.filter(l_freq=l_freq, h_freq=h_freq, verbose=False) \n",
    "    raw_rs = raw_rs.notch_filter(freqs=50, verbose=False) \n",
    "    \n",
    "    # eye blink removal on whole recording\n",
    "    ica_again = 0\n",
    "    ica = mne.preprocessing.ICA(n_components=0.95, max_iter=800, method='infomax', fit_params=dict(extended=True))\n",
    "    ica.fit(raw_rs, verbose=False) \n",
    "    ic_labels = label_components(raw_rs, ica, method=\"iclabel\")\n",
    "    labels = ic_labels[\"labels\"]\n",
    "    exclude_idxs = [idx for idx, label in enumerate(labels) if label in [\"eye blink\"]]\n",
    "    ica.apply(raw_rs, exclude=exclude_idxs, verbose=False)\n",
    "    if len(exclude_idxs)==0:\n",
    "        print(f'could not find blinking artifact in subject {file_vhdr[-14:-10]}')\n",
    "        ica_again = 1\n",
    "\n",
    "    # ec/eo stimuli timing\n",
    "    ec_start = np.array([event[0] for event in events_resampled if event[2]==44])  # stimulus 44: eyes_close\n",
    "    eo_start = np.array([event[0] for event in events_resampled if event[2]==31])  # stimulus 31: eyes_open\n",
    "    ec_init_start = np.array([event[0] for event in events_resampled if event[2]==10001]) # init stimulus for eyes close\n",
    "    ec_start = np.insert(ec_start, 0, ec_init_start[0])\n",
    "\n",
    "    # fixing for some subjects: (consider that subject indexes 13 and 19 have incorrect .mrk and .vhdr files)\n",
    "    check = 0\n",
    "    if (len(eo_start)==4) and (eo_start[-1] < 500 * sfreq): # last stim missing\n",
    "        last_eo_stim = np.array([event[0] for event in events_resampled if event[2]==22])[-1]\n",
    "        eo_start = np.append(eo_start, last_eo_stim)\n",
    "        check = 1\n",
    "    if (len(eo_start)==4) and (eo_start[0] > 100 * sfreq): # first stim missing\n",
    "        first_eo_stim = np.array([event[0] for event in events_resampled if event[2]==22])[0]\n",
    "        eo_start = np.insert(eo_start, 0, first_eo_stim)\n",
    "        check = 1\n",
    "    if (len(eo_start)==4) and check == 0:\n",
    "        eo_start = np.array([event[0] for event in events_resampled if event[2]==22])\n",
    "\n",
    "    ec_start_t = ec_start / sfreq\n",
    "    eo_start_t = eo_start / sfreq\n",
    "    \n",
    "    # cropping and concatenating the data\n",
    "    cropped_raw_eo = []\n",
    "    cropped_raw_ec = []\n",
    "    \n",
    "    for ec_st, eo_st in zip(ec_start_t[:-1], eo_start_t):\n",
    "        cropped_raw_ec.append(raw_rs.copy().crop(tmin=ec_st + cut_out_sec, tmax=eo_st))\n",
    "    for ec_st, eo_st in zip(ec_start_t[1:], eo_start_t):\n",
    "        cropped_raw_eo.append(raw_rs.copy().crop(tmin=eo_st + 3, tmax=ec_st))\n",
    "    \n",
    "    raw_ec_concat = cropped_raw_ec[0]\n",
    "    raw_eo_concat = cropped_raw_eo[0]\n",
    "    for raw_cr in cropped_raw_ec[1:]:\n",
    "        raw_ec_concat = mne.io.concatenate_raws([raw_ec_concat, raw_cr], on_mismatch='ignore')\n",
    "    for raw_cr in cropped_raw_eo[1:]:\n",
    "        raw_eo_concat = mne.io.concatenate_raws([raw_eo_concat, raw_cr], on_mismatch='ignore')\n",
    "\n",
    "    # applying ica again on only eo condition, if no blink found previously\n",
    "    if ica_again == 1:\n",
    "        ica = mne.preprocessing.ICA(n_components=0.95, max_iter=800, method='infomax', fit_params=dict(extended=True))\n",
    "        ica.fit(raw_eo_concat, verbose=False) \n",
    "        ic_labels = label_components(raw_eo_concat, ica, method=\"iclabel\")\n",
    "        labels = ic_labels[\"labels\"]\n",
    "        exclude_idxs = [idx for idx, label in enumerate(labels) if label in [\"eye blink\"]] \n",
    "        ica.apply(raw_eo_concat, exclude=exclude_idxs, verbose=False)\n",
    "    \n",
    "    # average re-referencing\n",
    "    raw_eo_concat.set_eeg_reference(ref_channels='average', projection=False, verbose=False)\n",
    "    raw_ec_concat.set_eeg_reference(ref_channels='average', projection=False, verbose=False)\n",
    "    \n",
    "    # epoching (2- concat and epoching into 2 seconds)\n",
    "    epochs_eo = mne.make_fixed_length_epochs(raw_eo_concat, duration=epoch_duration, preload=True)\n",
    "    epochs_ec = mne.make_fixed_length_epochs(raw_ec_concat, duration=epoch_duration, preload=True)\n",
    "    \n",
    "    # saving epochs\n",
    "    fname_save_eo = f'/Users/payamsadeghishabestari/eeg_data/Regensburg/transfer_147155_files_ac07b771/neuropren_rsEEG/epochs/{file_vhdr[-17:-13]}-eo-epo.fif' \n",
    "    fname_save_ec = f'/Users/payamsadeghishabestari/eeg_data/Regensburg/transfer_147155_files_ac07b771/neuropren_rsEEG/epochs/{file_vhdr[-17:-13]}-ec-epo.fif' \n",
    "    epochs_eo.save(fname=fname_save_eo, overwrite=True, verbose=False)\n",
    "    epochs_ec.save(fname=fname_save_ec, overwrite=True, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. c. Loading and preprocessing TEAS resting-state (preprocessed before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting .fif files (epochs) (64 channels)\n",
    "directory = '/Users/payamsadeghishabestari/eeg_data/teas/teas_rs_epochs' \n",
    "files_list = []\n",
    "for filename in sorted(os.listdir(directory)): ## iterate over files in that directory\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f) and (f.endswith(\"_visit_3_run_RS1-ec-epo.fif\") or f.endswith(\"_visit_3_run_rs1-ec-epo.fif\")):\n",
    "        files_list.append(f)\n",
    "        \n",
    "# subject 650 and 999 had no behavioral result \n",
    "epochs_ec = []\n",
    "epochs_ec_labels = []\n",
    "df_teas = pd.read_excel('/Users/payamsadeghishabestari/Prime/expertEAS_behav.xlsx')\n",
    "df_v_0 = df_teas[df_teas['Visit']==0] # only look at visit 0\n",
    "for file in files_list:\n",
    "    epoch = mne.read_epochs(fname=file, preload=True, verbose=False)\n",
    "    \n",
    "    # montaging\n",
    "    if epoch.info['ch_names'][-1] == 'Iz':\n",
    "        old_channel_names = epoch.info['ch_names']\n",
    "        new_channel_names = old_channel_names[:-1]  ### rename the Iz to FCz in our recordings\n",
    "        new_channel_names.append('FCz')\n",
    "        mapping = dict(zip(old_channel_names, new_channel_names))\n",
    "        mne.rename_channels(epoch.info, mapping)\n",
    "        epoch.set_montage('standard_1020', on_missing='raise')\n",
    "    else:\n",
    "        epoch.add_reference_channels(ref_channels=['FCz'])\n",
    "        epoch.set_montage('standard_1020', on_missing='raise')\n",
    "        epoch.info['bads'] = ['FCz']\n",
    "        epoch.interpolate_bads(verbose=False)\n",
    "    epochs_ec.append(epoch)\n",
    "    \n",
    "    # labeling\n",
    "    sub_id = int(file[-30:-27])\n",
    "    if len(df_v_0[df_v_0['patient']==sub_id]['RI'].values):\n",
    "        epochs_ec_labels.append(len(epoch) * [df_v_0[df_v_0['patient']==sub_id]['RI'].values[0]])\n",
    "\n",
    "# coding suppresion levels\n",
    "for idx, i in enumerate(epochs_ec_labels):\n",
    "    if all(j <= -1 for j in i):\n",
    "        epochs_ec_labels[idx] = len(i) * [1]\n",
    "        print('can suppress')\n",
    "    if all(j > -1 for j in i):\n",
    "        epochs_ec_labels[idx] = len(i) * [0]\n",
    "        print('can not suppress')\n",
    "\n",
    "# getting epochs data (each epoch shape is n_epochs * n_chs * n_times for each subject)\n",
    "epochs_ec_data = []\n",
    "for idx in range(len(epochs_ec)):\n",
    "    epochs_ec_data.append(epochs_ec[idx].get_data(picks='eeg')) # not ecg channels\n",
    "\n",
    "# cleaning and renaming for ML (choose ec or eo) ... for now I choose ec\n",
    "data_list = epochs_ec_data\n",
    "label_list = epochs_ec_labels\n",
    "group_list = [[i]*len(j) for i,j in enumerate(data_list)]\n",
    "data_array = np.vstack(data_list)\n",
    "label_array = np.hstack(label_list)\n",
    "group_array = np.hstack(group_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. d. Loading and preprocessing Tinnoice resting-state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '/Users/payamsadeghishabestari/eeg_data/tinnoice' \n",
    "files_list = []\n",
    "for filename in sorted(os.listdir(directory)): ## iterate over files in that directory\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f) and f.endswith(\"run_1-ec_excl-ip-epo.fif\"):\n",
    "        files_list.append(f)\n",
    "\n",
    "epochs_ec = []\n",
    "epochs_ec_labels = []\n",
    "df_tinnoice = pd.read_excel('/Users/payamsadeghishabestari/Prime/behav_tinnoice.xlsx')\n",
    "for file in files_list:\n",
    "    epoch = mne.read_epochs(file, preload=True, verbose=False)\n",
    "    epoch.drop_channels(['1-EXG1', '1-EXG2', '1-EXG3', '1-EXG4',\n",
    "                        '1-EXG5', '1-EXG6', '1-EXG7', '1-EXG8', '1-GSR1',\n",
    "                        '1-GSR2', '1-Erg1', '1-Erg2', '1-Resp', '1-Plet', '1-Temp'], on_missing='warn')\n",
    "    epoch.set_montage(\"biosemi128\", verbose=False)\n",
    "    epochs_ec.append(epoch)\n",
    "    \n",
    "    # labeling\n",
    "    sub_id = int(file[56:59])\n",
    "    if len(df_tinnoice[df_tinnoice['ID']==sub_id]['RI'].values):\n",
    "        epochs_ec_labels.append(len(epoch) * [df_tinnoice[df_tinnoice['ID']==sub_id]['RI'].values[0]])\n",
    "\n",
    "# coding suppresion levels\n",
    "for idx, i in enumerate(epochs_ec_labels):\n",
    "    if all(j <= -1 for j in i):\n",
    "        epochs_ec_labels[idx] = len(i) * [1]\n",
    "        print('can suppress')\n",
    "    if all(j > -1 for j in i):\n",
    "        epochs_ec_labels[idx] = len(i) * [0]\n",
    "        print('can not suppress')\n",
    "\n",
    "# getting epochs data (each epoch shape is n_epochs * n_chs * n_times for each subject)\n",
    "epochs_ec_data = []\n",
    "for idx in range(len(epochs_ec)):\n",
    "    epochs_ec_data.append(epochs_ec[idx].get_data(picks='eeg')) # not ecg channels\n",
    "\n",
    "# cleaning and renaming for ML (choose ec or eo) ... for now I choose ec\n",
    "data_list = epochs_ec_data\n",
    "label_list = epochs_ec_labels\n",
    "group_list = [[i]*len(j) for i,j in enumerate(data_list)]\n",
    "data_array = np.vstack(data_list)\n",
    "label_array = np.hstack(label_list)\n",
    "group_array = np.hstack(group_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. a. Loading epochs, combining two lists and labeling (check thr=90 for inhibition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading epochs and excel files (subject 'fvlz' is missing from excel; I changed subject 'fvlZ' in excel file to 'fvlz')\n",
    "directory_aaeeg = '/Users/payamsadeghishabestari/eeg_data/Regensburg/transfer_147155_files_ac07b771/aaeeg_rsEEG/epochs' \n",
    "directory_neuropren = '/Users/payamsadeghishabestari/eeg_data/Regensburg/transfer_147155_files_ac07b771/neuropren_rsEEG/epochs' \n",
    "df_aaeeg = pd.read_excel('/Users/payamsadeghishabestari/eeg_data/Regensburg/transfer_147155_files_ac07b771/aaeeg_rsEEG/AAEEG_TL_data.xlsx')\n",
    "df_neuropren = pd.read_excel('/Users/payamsadeghishabestari/eeg_data/Regensburg/transfer_147155_files_ac07b771/neuropren_rsEEG/neuropren_TL_data.xlsx')\n",
    "files_list_eo = []; files_list_ec = []\n",
    "epochs_eo = []; epochs_ec = []\n",
    "epochs_eo_labels = []; epochs_ec_labels = []\n",
    "\n",
    "# combining two datasets (aaeeg and neuropren)\n",
    "for filename in sorted(os.listdir(directory_aaeeg)): ## iterate over files in that directory\n",
    "    f = os.path.join(directory_aaeeg, filename)\n",
    "    # checking if it is a file\n",
    "    if (os.path.isfile(f)) and (f.endswith(\".fif\")) and (f[-10:-8] == 'eo'):\n",
    "        files_list_eo.append(f)\n",
    "    if (os.path.isfile(f)) and (f.endswith(\".fif\")) and (f[-10:-8] == 'ec'):\n",
    "        files_list_ec.append(f)\n",
    "for filename in sorted(os.listdir(directory_neuropren)): ## iterate over files in that directory\n",
    "    f = os.path.join(directory_neuropren, filename)\n",
    "    # checking if it is a file\n",
    "    if (os.path.isfile(f)) and (f.endswith(\".fif\")) and (f[-10:-8] == 'eo'):\n",
    "        files_list_eo.append(f)\n",
    "    if (os.path.isfile(f)) and (f.endswith(\".fif\")) and (f[-10:-8] == 'ec'):\n",
    "        files_list_ec.append(f)\n",
    "\n",
    "# labeling\n",
    "for file in files_list_eo:\n",
    "    epoch_eo = mne.read_epochs(fname=file, preload=True, verbose=False)\n",
    "    epoch_eo.drop_channels(['HRli', 'HRre'], on_missing='warn')\n",
    "    epochs_eo.append(epoch_eo)\n",
    "    \n",
    "    if len(df_aaeeg[df_aaeeg['random_id']==file[-15:-11]]['WN_TL_0'].values):\n",
    "        epochs_eo_labels.append(len(epoch_eo) * [df_aaeeg[df_aaeeg['random_id']==file[-15:-11]]['WN_TL_0'].values[0]])\n",
    "    if len(df_neuropren[df_neuropren['random_id']==file[-15:-11]]['b1_WN_0'].values):\n",
    "        epochs_eo_labels.append(len(epoch_eo) * [df_neuropren[df_neuropren['random_id']==file[-15:-11]]['b1_WN_0'].values[0]])   \n",
    "\n",
    "for file in files_list_ec:\n",
    "    epoch_ec = mne.read_epochs(fname=file, preload=True, verbose=False)\n",
    "    epoch_ec.drop_channels(['HRli', 'HRre'], on_missing='warn')\n",
    "    epochs_ec.append(epoch_ec)\n",
    "    \n",
    "    if len(df_aaeeg[df_aaeeg['random_id']==file[-15:-11]]['WN_TL_0'].values):\n",
    "        epochs_ec_labels.append(len(epoch_ec) * [df_aaeeg[df_aaeeg['random_id']==file[-15:-11]]['WN_TL_0'].values[0]])\n",
    "    if len(df_neuropren[df_neuropren['random_id']==file[-15: -11]]['b1_WN_0'].values):\n",
    "        epochs_ec_labels.append(len(epoch_ec) * [df_neuropren[df_neuropren['random_id']==file[-15:-11]]['b1_WN_0'].values[0]]) \n",
    "\n",
    "# coding suppresion levels\n",
    "for idx, i in enumerate(epochs_eo_labels):\n",
    "    if all(j > 90 for j in i):\n",
    "        epochs_eo_labels[idx] = len(i) * [0]\n",
    "    if all(j <= 90 for j in i):\n",
    "        epochs_eo_labels[idx] = len(i) * [1]\n",
    "\n",
    "for idx, i in enumerate(epochs_ec_labels):\n",
    "    if all(j > 90 for j in i):\n",
    "        epochs_ec_labels[idx] = len(i) * [0]\n",
    "    if all(j <= 90 for j in i):\n",
    "        epochs_ec_labels[idx] = len(i) * [1]\n",
    "\n",
    "\n",
    "####### multi class\n",
    "#for idx, i in enumerate(epochs_eo_labels):\n",
    "#    if all(90 < j for j in i):\n",
    "#        epochs_eo_labels[idx] = len(i) * [0]\n",
    "#    if all(70 < j <= 90 for j in i):\n",
    "#        epochs_eo_labels[idx] = len(i) * [1]\n",
    "#    if all(50 < j <= 70 for j in i):\n",
    "#        epochs_eo_labels[idx] = len(i) * [2]\n",
    "#    if all(j <= 50 for j in i):\n",
    "#        epochs_eo_labels[idx] = len(i) * [3]\n",
    "\n",
    "#for idx, i in enumerate(epochs_ec_labels):\n",
    "#    if all(90 < j for j in i):\n",
    "#        epochs_ec_labels[idx] = len(i) * [0]\n",
    "#    if all(70 < j <= 90 for j in i):\n",
    "#        epochs_ec_labels[idx] = len(i) * [1]\n",
    "#    if all(50 < j <= 70 for j in i):\n",
    "#        epochs_ec_labels[idx] = len(i) * [2]\n",
    "#    if all(j <= 50 for j in i):\n",
    "#        epochs_ec_labels[idx] = len(i) * [3]\n",
    "\n",
    "\n",
    "# getting epochs data (each epoch shape is n_epochs * n_chs * n_times for each subject)\n",
    "epochs_ec_data = []\n",
    "epochs_eo_data = []\n",
    "for idx in range(len(epochs_ec)):\n",
    "    epochs_ec_data.append(epochs_ec[idx].get_data(picks='eeg')) # not ecg channels\n",
    "    epochs_eo_data.append(epochs_eo[idx].get_data(picks='eeg')) # not ecg channels\n",
    "\n",
    "# cleaning and renaming for ML (choose ec or eo) ... for now I choose ec\n",
    "data_list = epochs_ec_data\n",
    "label_list = epochs_ec_labels\n",
    "group_list = [[i]*len(j) for i,j in enumerate(data_list)]\n",
    "data_array = np.vstack(data_list)\n",
    "label_array = np.hstack(label_list)\n",
    "group_array = np.hstack(group_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. b. Checking data array size from combining lists: should be n_epochs * n_channels * n_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. a. Feature extraction (PSD in channels = 62*5 = 310)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for extracting power bands in each channel\n",
    "def eeg_power_band(epochs):\n",
    "    \"\"\"\n",
    "    This function takes an ``mne.Epochs`` object and creates EEG features based\n",
    "    on relative power in specific frequency bands that are compatible with\n",
    "    scikit-learn. the output of function is a numpy array of shape (n_samples, 5)\n",
    "    \"\"\"\n",
    "    FREQ_BANDS = {\"delta\": [0.5, 4.5], \"theta\": [4.5, 8.5], \"alpha\": [8.5, 13.5],\n",
    "                \"beta\": [15.5, 30], \"gamma\": [30, 80]}\n",
    "    \n",
    "    psds, freqs = epochs.compute_psd(method='multitaper', picks='eeg', fmin=0.5, fmax=80, verbose=False).get_data(return_freqs=True)\n",
    "    psds /= np.sum(psds, axis=-1, keepdims=True)  # normalize the PSDs\n",
    "    X = []\n",
    "    for fmin, fmax in FREQ_BANDS.values():\n",
    "        psds_band = psds[:, :, (freqs >= fmin) & (freqs < fmax)].mean(axis=-1)\n",
    "        X.append(psds_band)\n",
    "    print(len(psds_band[0]))\n",
    "    print(len(X))\n",
    "    print(len(X[0][0]))\n",
    "    return np.concatenate(X, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. b. Feature extraction (Power in labels using minimu norm dSPM = 68*5 = 340)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making equal channel size\n",
    "for ep in epochs_ec:\n",
    "    ep.drop_channels(['HRli', 'HRre'], on_missing='warn')\n",
    "\n",
    "# Compute PSD in brain labels of inverse solution from single epochs\n",
    "fs_dir = fetch_fsaverage(verbose=False)\n",
    "trans = \"fsaverage\"  \n",
    "src = op.join(fs_dir, \"bem\", \"fsaverage-ico-5-src.fif\")\n",
    "bem = op.join(fs_dir, \"bem\", \"fsaverage-5120-5120-5120-bem-sol.fif\")\n",
    "brain_labels = mne.read_labels_from_annot(subject='fsaverage', parc='aparc')[:-1] # 69 labels\n",
    "# brain_labels = mne.read_labels_from_annot(subject='fsaverage', parc='aparc.a2009s')[:-2]\n",
    "method = \"dSPM\"\n",
    "snr = 3.0\n",
    "lambda2 = 1.0 / snr**2\n",
    "counter = 0\n",
    "for ep in tqdm(epochs_ec[:30]):\n",
    "    ep.set_eeg_reference('average', projection=True)\n",
    "    fwd = mne.make_forward_solution(ep.info, trans=trans, src=src, bem=bem, eeg=True, mindist=5.0, verbose=False)\n",
    "    # noise_cov = mne.compute_covariance(ep, tmax=0.0, method=[\"shrunk\", \"empirical\"], rank=None, verbose=False)\n",
    "    noise_cov = mne.make_ad_hoc_cov(ep.info, std=None, verbose=False)\n",
    "    inverse_operator = make_inverse_operator(ep.info, fwd, noise_cov, loose=0.2, depth=0.8, verbose=False)\n",
    "    label_ts = []\n",
    "    for bl in tqdm(brain_labels):\n",
    "        stc_power = compute_source_psd_epochs(ep, inverse_operator, lambda2=lambda2,\n",
    "                                                    method=method, fmin=0.5, fmax=80, label=bl,\n",
    "                                                    return_generator=False, verbose=False)\n",
    "        label_ts.append(mne.extract_label_time_course(stc_power, bl, inverse_operator['src'], mode='mean',\n",
    "                                                allow_empty=True, return_generator=False, verbose=False))\n",
    "    \n",
    "    file = f'/Users/payamsadeghishabestari/eeg_data/Regensburg/labels_ts_teas_mean/label_ts_{counter}'\n",
    "    np.save(file=file, arr=np.array(label_ts))      \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and extracting power bands in each  brain label\n",
    "all_deltas = []; all_thetas = []; all_alphas = []; all_betas = []; all_gammas = []\n",
    "for counter_idx in tqdm(range(29)):\n",
    "    # if counter_idx != 15: # just for removing subject 555 in teas   \n",
    "        file = f'/Users/payamsadeghishabestari/eeg_data/Regensburg/labels_ts_teas_mean/label_ts_{counter_idx}.npy'\n",
    "        arr = np.squeeze(np.load(file=file, allow_pickle=True))\n",
    "        deltas = []; thetas = []; alphas = []; betas = []; gammas = []\n",
    "        for ch_idx in range(arr.shape[0]):\n",
    "            delta = []; theta = []; alpha = []; beta = []; gamma = []\n",
    "            for epo_idx in range(arr.shape[1]):\n",
    "                delta.append(np.average(arr[ch_idx][epo_idx][0:8]))\n",
    "                theta.append(np.average(arr[ch_idx][epo_idx][8:16]))\n",
    "                alpha.append(np.average(arr[ch_idx][epo_idx][16:26]))\n",
    "                beta.append(np.average(arr[ch_idx][epo_idx][30:59]))\n",
    "                gamma.append(np.average(arr[ch_idx][epo_idx][59:159]))\n",
    "            \n",
    "            deltas.append(delta)\n",
    "            thetas.append(theta)\n",
    "            alphas.append(alpha)\n",
    "            betas.append(beta)\n",
    "            gammas.append(gamma)\n",
    "        \n",
    "        all_deltas.append(deltas)\n",
    "        all_thetas.append(thetas)\n",
    "        all_alphas.append(alphas)\n",
    "        all_betas.append(betas)\n",
    "        all_gammas.append(gammas)\n",
    "\n",
    "delta_powers_array = np.concatenate(all_deltas, axis=1).T\n",
    "theta_powers_array = np.concatenate(all_thetas, axis=1).T\n",
    "alpha_powers_array = np.concatenate(all_alphas, axis=1).T\n",
    "beta_powers_array = np.concatenate(all_betas, axis=1).T\n",
    "gamma_powers_array = np.concatenate(all_gammas, axis=1).T\n",
    "power_brain_features = np.concatenate([delta_powers_array, theta_powers_array,\n",
    "                alpha_powers_array, beta_powers_array, gamma_powers_array], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. b. 2. splitting to more band powers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and extracting power bands in each  brain label\n",
    "all_deltas = []; all_thetas = []; all_l_alphas = []; all_u_alphas = []\n",
    "all_m_betas = []; all_u_betas = []; all_l_gammas = []; all_u_gammas = []\n",
    "\n",
    "for counter_idx in tqdm(range(73)):\n",
    "    file = f'/Users/payamsadeghishabestari/eeg_data/Regensburg/labels_ts/label_ts_{counter_idx}.npy'\n",
    "    arr = np.squeeze(np.load(file=file, allow_pickle=True))\n",
    "    deltas = []; thetas = []; l_alphas = []; u_alphas = []\n",
    "    m_betas = []; u_betas = []; l_gammas = []; u_gammas = [] \n",
    "    for ch_idx in range(arr.shape[0]):\n",
    "        delta = []; theta = []; l_alpha = []; u_alpha = []\n",
    "        m_beta = []; u_beta = []; l_gamma = []; u_gamma = []\n",
    "        for epo_idx in range(arr.shape[1]):\n",
    "            delta.append(np.average(arr[ch_idx][epo_idx][0:8])) # 0.5 - 4.5 Hz\n",
    "            theta.append(np.average(arr[ch_idx][epo_idx][8:16])) # 4.5 - 8.5 Hz\n",
    "            l_alpha.append(np.average(arr[ch_idx][epo_idx][16:20])) # 8.5 - 10.5 Hz\n",
    "            u_alpha.append(np.average(arr[ch_idx][epo_idx][20:26])) # 10.5 - 13.5 Hz\n",
    "            m_beta.append(np.average(arr[ch_idx][epo_idx][29:39])) # 15 - 20 Hz \n",
    "            u_beta.append(np.average(arr[ch_idx][epo_idx][39:59])) # 20 - 30 Hz\n",
    "            l_gamma.append(np.average(arr[ch_idx][epo_idx][59:99])) # 30 - 50 Hz\n",
    "            u_gamma.append(np.average(arr[ch_idx][epo_idx][99:159])) # 50 - 80 Hz \n",
    "        \n",
    "        deltas.append(delta)\n",
    "        thetas.append(theta)\n",
    "        l_alphas.append(l_alpha)\n",
    "        u_alphas.append(u_alpha)\n",
    "        m_betas.append(m_beta)\n",
    "        u_betas.append(u_beta)\n",
    "        l_gammas.append(l_gamma)\n",
    "        u_gammas.append(u_gamma)\n",
    "    \n",
    "    all_deltas.append(deltas)\n",
    "    all_thetas.append(thetas)\n",
    "    all_l_alphas.append(l_alphas)\n",
    "    all_u_alphas.append(u_alphas)\n",
    "    all_m_betas.append(m_betas)\n",
    "    all_u_betas.append(u_betas)\n",
    "    all_l_gammas.append(l_gammas)\n",
    "    all_u_gammas.append(u_gammas)\n",
    "\n",
    "delta_powers_array = np.concatenate(all_deltas, axis=1).T\n",
    "theta_powers_array = np.concatenate(all_thetas, axis=1).T\n",
    "alpha_l_powers_array = np.concatenate(all_l_alphas, axis=1).T\n",
    "alpha_u_powers_array = np.concatenate(all_u_alphas, axis=1).T\n",
    "beta_m_powers_array = np.concatenate(all_m_betas, axis=1).T\n",
    "beta_u_powers_array = np.concatenate(all_u_betas, axis=1).T\n",
    "gamma_l_powers_array = np.concatenate(all_l_gammas, axis=1).T\n",
    "gamma_u_powers_array = np.concatenate(all_u_gammas, axis=1).T\n",
    "\n",
    "power_brain_freq_split_features = np.concatenate([delta_powers_array, theta_powers_array,\n",
    "                                                alpha_l_powers_array, alpha_u_powers_array,\n",
    "                                                beta_m_powers_array, beta_u_powers_array,\n",
    "                                                gamma_l_powers_array, gamma_u_powers_array], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. c. Feature extraction (Shannon entropy = 62 * 5 = 310)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making equal channel size\n",
    "for ep in epochs_ec:\n",
    "    ep.drop_channels(['HRli', 'HRre'], on_missing='warn')\n",
    "\n",
    "# computing app entropy features\n",
    "print('computing entropy features (be patient) ...')\n",
    "features_ent = []\n",
    "for ep in tqdm(range(len(epochs_ec))): # get features from each epoch and save in a list\n",
    "    for idx in range(len(epochs_ec[ep])):\n",
    "        features_ent.append(mne_features.univariate.compute_app_entropy(epochs_ec[ep][idx].get_data()[0], emb=2, metric='chebyshev'))\n",
    "features_ent_array = np.array(features_ent) # shape of (10332, 62) (nep * n_chs)\n",
    "fname = '/Users/payamsadeghishabestari/eeg_data/Regensburg/features_ent_array.npy'\n",
    "np.save(file=fname, arr=features_ent_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing shannon entropy for each frequency band\n",
    "# making equal channel size\n",
    "for ep in epochs_ec:\n",
    "    ep.drop_channels(['HRli', 'HRre'], on_missing='warn')\n",
    "\n",
    "features_ent = []\n",
    "deltas_ent = []; thetas_ent = []; alphas_ent = []; betas_ent = []; gammas_ent = [] \n",
    "for ep in tqdm(range(len(epochs_ec))): # get features from each epoch and save in a list\n",
    "    delta_ent = []; theta_ent = []; alpha_ent = []; beta_ent = []; gamma_ent = [] \n",
    "    epochs_ec_delta = epochs_ec[ep].copy().filter(0.5, 4.5,verbose=False)\n",
    "    \n",
    "    for idx in range(len(epochs_ec_delta)):\n",
    "        delta_ent.append(mne_features.univariate.compute_spect_entropy(sfreq=250, data=epochs_ec_delta[idx].get_data()[0], psd_method='multitaper'))\n",
    "\n",
    "    epochs_ec_theta = epochs_ec[ep].copy().filter(4.5, 8.5,verbose=False)\n",
    "    for idx in range(len(epochs_ec_theta)):\n",
    "        theta_ent.append(mne_features.univariate.compute_spect_entropy(sfreq=250, data=epochs_ec_theta[idx].get_data()[0], psd_method='multitaper'))\n",
    "\n",
    "    epochs_ec_alpha = epochs_ec[ep].copy().filter(8.5, 13.5,verbose=False)\n",
    "    for idx in range(len(epochs_ec_alpha)):\n",
    "        alpha_ent.append(mne_features.univariate.compute_spect_entropy(sfreq=250, data=epochs_ec_alpha[idx].get_data()[0], psd_method='multitaper'))\n",
    "\n",
    "    epochs_ec_beta = epochs_ec[ep].copy().filter(15, 30,verbose=False)\n",
    "    for idx in range(len(epochs_ec_beta)):\n",
    "        beta_ent.append(mne_features.univariate.compute_spect_entropy(sfreq=250, data=epochs_ec_beta[idx].get_data()[0], psd_method='multitaper'))\n",
    "\n",
    "    epochs_ec_gamma = epochs_ec[ep].copy().filter(30, 80,verbose=False)\n",
    "    for idx in range(len(epochs_ec_gamma)):\n",
    "        gamma_ent.append(mne_features.univariate.compute_spect_entropy(sfreq=250, data=epochs_ec_gamma[idx].get_data()[0], psd_method='multitaper'))\n",
    "\n",
    "    deltas_ent.append(delta_ent)\n",
    "    thetas_ent.append(theta_ent)\n",
    "    alphas_ent.append(alpha_ent)\n",
    "    betas_ent.append(beta_ent)\n",
    "    gammas_ent.append(gamma_ent)\n",
    "\n",
    "delta_feature = np.concatenate(deltas_ent, axis=0)\n",
    "theta_feature = np.concatenate(thetas_ent, axis=0)\n",
    "alpha_feature = np.concatenate(alphas_ent, axis=0)\n",
    "beta_feature = np.concatenate(betas_ent, axis=0)\n",
    "gamma_feature = np.concatenate(gammas_ent, axis=0)\n",
    "entropy_features_array = np.concatenate((delta_feature, theta_feature, alpha_feature, beta_feature, gamma_feature), axis=1)\n",
    "entropy_features_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/Users/payamsadeghishabestari/eeg_data/Regensburg/features_tinnoice_ent.npy', entropy_features_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. d. Feature extraction (fooof aperiodic = 62 * 2 = 124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making equal channel size\n",
    "for ep in epochs_ec:\n",
    "    ep.drop_channels(['HRli', 'HRre'], on_missing='warn')\n",
    "\n",
    "# computing power band features\n",
    "print('computing power band features')\n",
    "features_power = []\n",
    "for ep in tqdm(range(len(epochs_ec))): # get features from each epoch and save in a list\n",
    "    features_power.append(eeg_power_band(epochs_ec[ep]))\n",
    "features_power_array = np.concatenate(features_power) # shape of (10332, 310) (nep * (n_chs * 5))\n",
    "fname = '/Users/payamsadeghishabestari/eeg_data/Regensburg/features_tinnoice_power_array.npy'\n",
    "np.save(file=fname, arr=features_power_array)\n",
    "\n",
    "# computing fooof aperiodic parameter\n",
    "ap_params = []\n",
    "for ep in tqdm(range(len(epochs_ec))):\n",
    "    psds, freqs = epochs_ec[ep].compute_psd(method='multitaper', picks='eeg', fmin=0.5, fmax=40, verbose=False).get_data(return_freqs=True)\n",
    "    fg = FOOOFGroup(aperiodic_mode='fixed')\n",
    "    params = []\n",
    "    for ep_i in tqdm(range(len(epochs_ec[ep]))):\n",
    "        fg.fit(freqs, psds[ep_i], freq_range=[0.5, 40])\n",
    "        params.append(fg.get_params('aperiodic_params'))\n",
    "    ap_params.append(params)\n",
    "features_ap_fooof = np.concatenate(ap_params) # (n_ep * n_ch * 2) --> (n_ep * (n_ch * 2))\n",
    "fname = '/Users/payamsadeghishabestari/eeg_data/Regensburg/features_tinnoice_ap_fooof.npy'\n",
    "np.save(file=fname, arr=features_ap_fooof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. e. Feature extraction (connectivity = 20 * 20 * 4 = 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making equal channel size\n",
    "for ep in epochs_ec:\n",
    "    ep.drop_channels(['HRli', 'HRre'], on_missing='warn')\n",
    "\n",
    "fs_dir = fetch_fsaverage(verbose=False)\n",
    "trans = \"fsaverage\"  \n",
    "src = op.join(fs_dir, \"bem\", \"fsaverage-ico-5-src.fif\")\n",
    "bem = op.join(fs_dir, \"bem\", \"fsaverage-5120-5120-5120-bem-sol.fif\")\n",
    "method = \"dSPM\"\n",
    "snr = 3.0\n",
    "lambda2 = 1.0 / snr**2\n",
    "sfreq = 250\n",
    "counter = 0\n",
    "# selecting important brain labels\n",
    "brain_labels = mne.read_labels_from_annot(subject='fsaverage', parc='aparc') # 69 labels\n",
    "\n",
    "# imp_brain_label_list = ['parahippocampal-lh', 'parahippocampal-rh', 'transversetemporal-lh', 'transversetemporal-rh',\n",
    "#                         'pericalcarine-lh', 'pericalcarine-rh', 'temporalpole-lh', 'temporalpole-rh',\n",
    "#                         'medialorbitofrontal-lh', 'medialorbitofrontal-rh', 'lateraloccipital-lh', 'lateraloccipital-rh',\n",
    "#                         'inferiortemporal-lh', 'inferiortemporal-rh', 'superiortemporal-lh', 'superiortemporal-rh',\n",
    "#                         'supramarginal-lh', 'supramarginal-rh', 'frontalpole-lh', 'frontalpole-rh', 'parsopercularis-lh', 'parsopercularis-rh',\n",
    "#                         'posteriorcingulate-lh', 'posteriorcingulate-rh', 'superiorparietal-lh', 'superiorparietal-rh',\n",
    "#                         'rostralmiddlefrontal-lh', 'rostralmiddlefrontal-rh', 'inferiortemporal-lh', 'inferiortemporal-rh',\n",
    "#                         'caudalmiddlefrontal-lh', 'caudalmiddlefrontal-rh', 'parstriangularis-lh', 'parstriangularis-rh',\n",
    "#                         'lateraloccipital-lh', 'lateraloccipital-rh', 'postcentral-lh', 'postcentral-rh'] # RGB40\n",
    "\n",
    "# imp_brain_label_list = ['parahippocampal-lh', 'parahippocampal-rh', 'paracentral-lh', 'paracentral-rh',\n",
    "#                         'temporalpole-lh', 'temporalpole-rh', 'superiorfrontal-lh', 'superiorfrontal-rh',\n",
    "#                         'superiorparietal-lh', 'superiorparietal-rh', 'transversetemporal-lh', 'transversetemporal-rh',\n",
    "#                         'rostralanteriorcingulate-lh', 'rostralanteriorcingulate-rh', 'insula-lh', 'insula-rh',\n",
    "#                         'posteriorcingulate-lh', 'posteriorcingulate-rh', 'precentral-lh', 'precentral-rh',\n",
    "#                         'middletemporal-lh', 'middletemporal-rh', 'frontalpole-lh', 'frontalpole-rh',\n",
    "#                         'supramarginal-lh', 'supramarginal-rh'] \n",
    "\n",
    "imp_brain_label_list = ['superiorparietal-lh', 'superiorparietal-rh',\n",
    "            'superiortemporal-lh', 'superiortemporal-rh',\n",
    "            'transversetemporal-lh', 'transversetemporal-rh']\n",
    "\n",
    "imp_lb_idxs = []\n",
    "for im_l in imp_brain_label_list:\n",
    "    for l_idx, l_id in enumerate(brain_labels):\n",
    "        if l_id.name == im_l:\n",
    "            imp_lb_idxs.append(l_idx)\n",
    "sel_brain_labels = list(np.array(brain_labels)[imp_lb_idxs])\n",
    "\n",
    "# looping over subjects to compute connectivity\n",
    "for ep in tqdm(epochs_ec[:]):\n",
    "    # initiating\n",
    "    ep.set_eeg_reference('average', projection=True)\n",
    "    ep.apply_proj()\n",
    "    fwd = mne.make_forward_solution(ep.info, trans=trans, src=src, bem=bem, eeg=True, mindist=5.0, verbose=False)\n",
    "    # noise_cov = mne.compute_covariance(ep, tmax=0.0, method=[\"shrunk\", \"empirical\"], rank=None, verbose=False)\n",
    "    noise_cov = mne.make_ad_hoc_cov(ep.info, std=None, verbose=False)\n",
    "    inverse_operator = make_inverse_operator(ep.info, fwd, noise_cov, loose=0.2, depth=0.8, verbose=False)\n",
    "    \n",
    "    # computing source estimate over all surface\n",
    "    stc = apply_inverse_epochs(ep, inverse_operator, lambda2=lambda2,\n",
    "                                                method=method, label=None,\n",
    "                                                return_generator=False, verbose=False)\n",
    "    # project it from whole surface to labels\n",
    "    label_ts = mne.extract_label_time_course(stc, sel_brain_labels, inverse_operator['src'], mode='mean',\n",
    "                                            allow_empty=True, return_generator=False, verbose=False)\n",
    "\n",
    "    # # compute connectivity\n",
    "    # freqs = np.linspace(4.5, 8.5, 5)\n",
    "    # con = spectral_connectivity_time(np.array(label_ts), freqs=freqs, method=['coh', 'wpli'], average=False, sfreq=sfreq,\n",
    "    #                                 fmin=freqs[0], fmax=freqs[-1], mode='multitaper', n_cycles=7,\n",
    "    #                                 faverage=True, n_jobs=1, verbose=False)\n",
    "    # conmat_coh = con[0].get_data(output='dense')[:, :, :, 0]\n",
    "    # conmat_wpli = con[1].get_data(output='dense')[:, :, :, 0]\n",
    "    # fname_coh = f'/Users/payamsadeghishabestari/eeg_data/Regensburg/connectivity/20_labels/con_coh_theta_{counter}.npy'\n",
    "    # fname_wpli = f'/Users/payamsadeghishabestari/eeg_data/Regensburg/connectivity/20_labels/con_wpli_theta_{counter}.npy'\n",
    "    # np.save(fname_coh, conmat_coh)\n",
    "    # np.save(fname_wpli, conmat_wpli)\n",
    "    \n",
    "    # freqs = np.linspace(15.5, 30, 5)\n",
    "    # con = spectral_connectivity_time(np.array(label_ts), freqs=freqs, method=['coh', 'wpli'], average=False, sfreq=sfreq,\n",
    "    #                                 fmin=freqs[0], fmax=freqs[-1], mode='cwt_morlet', n_cycles=7,\n",
    "    #                                 faverage=True, n_jobs=1, verbose=False)\n",
    "    # conmat_coh = con[0].get_data(output='dense')[:, :, :, 0]\n",
    "    # conmat_wpli = con[1].get_data(output='dense')[:, :, :, 0]\n",
    "    # fname_coh = f'/Users/payamsadeghishabestari/eeg_data/Regensburg/connectivity/20_labels/con_coh_beta_{counter}.npy'\n",
    "    # fname_wpli = f'/Users/payamsadeghishabestari/eeg_data/Regensburg/connectivity/20_labels/con_wpli_beta_{counter}.npy'\n",
    "    # np.save(fname_coh, conmat_coh)\n",
    "    # np.save(fname_wpli, conmat_wpli)\n",
    "\n",
    "    freqs = np.linspace(8, 13, 5)\n",
    "    con = spectral_connectivity_time(np.array(label_ts), freqs=freqs, method='coh', average=False, sfreq=sfreq,\n",
    "                                    fmin=freqs[0], fmax=freqs[-1], mode='cwt_morlet', n_cycles=7,\n",
    "                                    faverage=True, n_jobs=1, verbose=False)\n",
    "    conmat_coh = con.get_data(output='dense')[:, :, :, 0]\n",
    "    fname_coh = f'/Users/payamsadeghishabestari/eeg_data/Regensburg/connectivity/within_aud_network/con_coh_alpha_{counter}.npy'\n",
    "    np.save(fname_coh, conmat_coh)\n",
    "    \n",
    "    \n",
    "    freqs = np.linspace(30, 80, 5)\n",
    "    con = spectral_connectivity_time(np.array(label_ts), freqs=freqs, method='coh', average=False, sfreq=sfreq,\n",
    "                                    fmin=freqs[0], fmax=freqs[-1], mode='cwt_morlet', n_cycles=7,\n",
    "                                    faverage=True, n_jobs=1, verbose=False)\n",
    "    conmat_coh = con.get_data(output='dense')[:, :, :, 0]\n",
    "    fname_coh = f'/Users/payamsadeghishabestari/eeg_data/Regensburg/connectivity/within_aud_network/con_coh_gamma_{counter}.npy'\n",
    "    np.save(fname_coh, conmat_coh)\n",
    "    \n",
    "\n",
    "    # freqs = np.linspace(8.5, 13.5, 5)\n",
    "    # con = spectral_connectivity_time(np.array(label_ts), freqs=freqs, method=['coh', 'wpli'], average=False, sfreq=sfreq,\n",
    "    #                                 fmin=freqs[0], fmax=freqs[-1], mode='cwt_morlet', n_cycles=7,\n",
    "    #                                 faverage=True, n_jobs=1, verbose=False)\n",
    "    # conmat_coh = con[0].get_data(output='dense')[:, :, :, 0]\n",
    "    # conmat_wpli = con[1].get_data(output='dense')[:, :, :, 0]\n",
    "    # fname_coh = f'/Users/payamsadeghishabestari/eeg_data/Regensburg/connectivity/20_labels/con_coh_alpha_{counter}.npy'\n",
    "    # fname_wpli = f'/Users/payamsadeghishabestari/eeg_data/Regensburg/connectivity/20_labels/con_wpli_alpha_{counter}.npy'\n",
    "    # np.save(fname_coh, conmat_coh)\n",
    "    # np.save(fname_wpli, conmat_wpli)\n",
    "    \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/Users/payamsadeghishabestari/eeg_data/Regensburg/connectivity/within_aud_network'  \n",
    "alpha_files = []; gamma_files = []\n",
    "if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "    files_in_folder = sorted(os.listdir(folder_path), key=lambda x: os.path.getmtime(os.path.join(folder_path, x)))\n",
    "    for filename in files_in_folder:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path) and file_path.endswith(\".npy\"):\n",
    "            if 'alpha' in file_path:\n",
    "                alpha_files.append(file_path)\n",
    "            if 'gamma' in file_path:\n",
    "                gamma_files.append(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsss_alpha = []\n",
    "for file in alpha_files[:]:\n",
    "    x = np.load(file, allow_pickle=True)\n",
    "    fss = []\n",
    "    for k in x:\n",
    "        fs = []\n",
    "        for i, j in itertools.product(range(k.shape[0]), range(x.shape[1])):\n",
    "            if j < i:\n",
    "                fs.append(k[i][j])\n",
    "        fss.append(fs)\n",
    "    fsss_alpha.append(fss)\n",
    "\n",
    "con_alpha_f_array = np.array(fsss_alpha[0])\n",
    "for i in fsss_alpha[1:]:\n",
    "    con_alpha_f_array = np.concatenate((con_alpha_f_array, i), axis=0)\n",
    "\n",
    "fsss_gamma = []\n",
    "for file in gamma_files[:]:\n",
    "    x = np.load(file, allow_pickle=True)\n",
    "    fss = []\n",
    "    for k in x:\n",
    "        fs = []\n",
    "        for i, j in itertools.product(range(k.shape[0]), range(x.shape[1])):\n",
    "            if j < i:\n",
    "                fs.append(k[i][j])\n",
    "        fss.append(fs)\n",
    "    fsss_gamma.append(fss)\n",
    "\n",
    "con_gamma_f_array = np.array(fsss_gamma[0])\n",
    "for i in fsss_gamma[1:]:\n",
    "    con_gamma_f_array = np.concatenate((con_gamma_f_array, i), axis=0)\n",
    "\n",
    "con_34lbs_f_array = np.concatenate((con_alpha_f_array, con_gamma_f_array), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. f. Feature extraction (Power in labels using beamformer= 68*5 = 340)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making equal channel size\n",
    "for ep in epochs_ec:\n",
    "    ep.drop_channels(['HRli', 'HRre'], on_missing='warn')\n",
    "\n",
    "# Compute PSD in brain labels of inverse solution from single epochs\n",
    "fs_dir = fetch_fsaverage(verbose=False)\n",
    "trans = \"fsaverage\"  \n",
    "src = op.join(fs_dir, \"bem\", \"fsaverage-ico-5-src.fif\")\n",
    "bem = op.join(fs_dir, \"bem\", \"fsaverage-5120-5120-5120-bem-sol.fif\")\n",
    "brain_labels = mne.read_labels_from_annot(subject='fsaverage', parc='aparc') # 69 labels\n",
    "method = \"dSPM\"\n",
    "snr = 3.0\n",
    "lambda2 = 1.0 / snr**2\n",
    "counter = 0\n",
    "for ep in tqdm(epochs_ec[:]):\n",
    "    ep.set_eeg_reference('average', projection=True)\n",
    "    fwd = mne.make_forward_solution(ep.info, trans=trans, src=src, bem=bem, eeg=True, mindist=5.0, verbose=False)\n",
    "    data_cov = mne.compute_covariance(ep, tmin=0.01, tmax=2, method=[\"shrunk\", \"empirical\"])\n",
    "    noise_cov = mne.compute_covariance(ep, tmax=0.0, method=[\"shrunk\", \"empirical\"], rank=None, verbose=False)\n",
    "    csd_signal = mne.time_frequency.csd_multitaper(ep, fmin=30, fmax=80, verbose=False).mean()\n",
    "    filters = mne.beamformer.make_dics(ep.info, fwd, csd_signal, reg=0.05, pick_ori=\"max-power\", depth=1.0,\n",
    "                                        inversion=\"single\", weight_norm=None, real_filter=True, verbose=False)\n",
    "    #stc_power = mne.beamformer.apply_dics_epochs(ep, filters, return_generator=False, verbose=False)\n",
    "    stc_power = mne.beamformer.apply_csd(csd_signal, filters, return_generator=False, verbose=False)\n",
    "\n",
    "    label_ts = []\n",
    "    for bl in brain_labels[:-1]:\n",
    "        label_ts.append(mne.extract_label_time_course(stc_power, bl, fwd['src'], mode='mean_flip',\n",
    "                                                allow_empty=True, return_generator=False, verbose=False))\n",
    "    file = f'/Users/payamsadeghishabestari/eeg_data/Regensburg/beamformer/dics_averaged/label_ts_gamma_{counter}'\n",
    "    np.save(file=file, arr=np.array(label_ts))      \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. a. Importing Saved features (saving new features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_files = []; theta_files = []; alpha_files = []; beta_files = []; gamma_files = []\n",
    "folder_path = '/Users/payamsadeghishabestari/eeg_data/Regensburg/beamformer/dics_averaged'  \n",
    "if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "    files_in_folder = sorted(os.listdir(folder_path), key=lambda x: os.path.getmtime(os.path.join(folder_path, x)))\n",
    "    for filename in files_in_folder:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path) and file_path.endswith(\".npy\"):\n",
    "            if 'delta' in file_path:\n",
    "                delta_files.append(file_path)\n",
    "            if 'theta' in file_path:\n",
    "                theta_files.append(file_path)\n",
    "            if 'alpha' in file_path:\n",
    "                alpha_files.append(file_path)\n",
    "            if 'beta' in file_path:\n",
    "                beta_files.append(file_path)\n",
    "            if 'gamma' in file_path:\n",
    "                gamma_files.append(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_arrs = []; t_arrs = []; a_arrs = []; b_arrs = []; g_arrs = []\n",
    "for f_d, f_t, f_a, f_b, f_g in zip(delta_files, theta_files, alpha_files, beta_files, gamma_files): \n",
    "    d_arrs.append(np.mean(np.squeeze(np.load(f_d, allow_pickle=True)), axis=2))\n",
    "    t_arrs.append(np.mean(np.squeeze(np.load(f_t, allow_pickle=True)), axis=2))\n",
    "    a_arrs.append(np.mean(np.squeeze(np.load(f_a, allow_pickle=True)), axis=2))\n",
    "    b_arrs.append(np.mean(np.squeeze(np.load(f_b, allow_pickle=True)), axis=2))\n",
    "    g_arrs.append(np.mean(np.squeeze(np.load(f_g, allow_pickle=True)), axis=2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_arrs_c = d_arrs[0]\n",
    "t_arrs_c = t_arrs[0]\n",
    "a_arrs_c = a_arrs[0]\n",
    "b_arrs_c = b_arrs[0]\n",
    "g_arrs_c = g_arrs[0]\n",
    "\n",
    "for d_arr, t_arr, a_arr, b_arr, g_arr in zip(d_arrs[1:], t_arrs[1:], a_arrs[1:], b_arrs[1:], g_arrs[1:]):\n",
    "    d_arrs_c = np.concatenate((d_arrs_c, d_arr), axis=1)\n",
    "    t_arrs_c = np.concatenate((t_arrs_c, t_arr), axis=1)\n",
    "    a_arrs_c = np.concatenate((a_arrs_c, a_arr), axis=1)\n",
    "    b_arrs_c = np.concatenate((b_arrs_c, b_arr), axis=1)\n",
    "    g_arrs_c = np.concatenate((g_arrs_c, g_arr), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. b. Importing Saved features (loading new features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the features\n",
    "features_power_array = np.load(file='/Users/payamsadeghishabestari/eeg_data/Regensburg/features_power_array.npy', allow_pickle=True)\n",
    "features_ent_array = np.load(file='/Users/payamsadeghishabestari/eeg_data/Regensburg/features_ent_array.npy', allow_pickle=True)\n",
    "features_ap_fooof = np.load(file='/Users/payamsadeghishabestari/eeg_data/Regensburg/features_ap_fooof.npy', allow_pickle=True) # needs reshaping\n",
    "features_bl_array = np.load(file='/Users/payamsadeghishabestari/eeg_data/Regensburg/power_brain_features.npy', allow_pickle=True)\n",
    "features_ent_array = np.load(file='/Users/payamsadeghishabestari/eeg_data/Regensburg/features_ent_sh_array.npy', allow_pickle=True)\n",
    "features_coh = np.load(file='/Users/payamsadeghishabestari/eeg_data/Regensburg/connectivity/concatenated/alpha_gamma_coh.npy', allow_pickle=True)\n",
    "features_wpli = np.load(file='/Users/payamsadeghishabestari/eeg_data/Regensburg/connectivity/concatenated/alpha_gamma_wpli.npy', allow_pickle=True)\n",
    "features_bmf = np.load(file='/Users/payamsadeghishabestari/eeg_data/Regensburg/features_bmf.npy', allow_pickle=True)\n",
    "features_bl_splitted = np.load(file='/Users/payamsadeghishabestari/eeg_data/Regensburg/power_brain_features_splitted.npy', allow_pickle=True)\n",
    "features_teas_bl_power = np.load(file='/Users/payamsadeghishabestari/eeg_data/Regensburg/power_brain_features_teas.npy', allow_pickle=True)\n",
    "features_teas_bl_power = np.load(file='/Users/payamsadeghishabestari/eeg_data/Regensburg/power_brain_features_teas_v3.npy', allow_pickle=True)\n",
    "features_tinnoice_bl_power = np.load(file='/Users/payamsadeghishabestari/eeg_data/Regensburg/power_brain_features_tinnoice.npy', allow_pickle=True)\n",
    "features_rgb_bl_con = np.load(file='/Users/payamsadeghishabestari/eeg_data/Regensburg/con_34lbs_features.npy', allow_pickle=True)\n",
    "features_teas_bl_con = np.load(file='/Users/payamsadeghishabestari/eeg_data/Regensburg/con_teas_features.npy', allow_pickle=True)\n",
    "features_bl_rgb_mean_array = np.load('/Users/payamsadeghishabestari/eeg_data/Regensburg/power_brain_features_rgb_mean.npy', allow_pickle=True)\n",
    "features_bl_teas_mean_array = np.load('/Users/payamsadeghishabestari/eeg_data/Regensburg/power_brain_features_teas_mean.npy', allow_pickle=True)\n",
    "features_con_rgb_mean = np.load('/Users/payamsadeghishabestari/eeg_data/Regensburg/features_connectivity_rgb_mean.npy', allow_pickle=True)\n",
    "features_con_teas_mean = np.load('/Users/payamsadeghishabestari/eeg_data/Regensburg/features_connectivity_teas_mean.npy', allow_pickle=True)\n",
    "features_con_rgb_network = np.load('/Users/payamsadeghishabestari/eeg_data/Regensburg/features_network_connectivity_rgb.npy', allow_pickle=True)\n",
    "features_con_within_aud = np.load('/Users/payamsadeghishabestari/eeg_data/Regensburg/features_connectivity_within_aud.npy', allow_pickle=True)\n",
    "\n",
    "features_con_teas_network = np.load('/Users/payamsadeghishabestari/eeg_data/Regensburg/features_network_connectivity_teas.npy', allow_pickle=True)\n",
    "features_con_within_aud_teas = np.load('/Users/payamsadeghishabestari/eeg_data/Regensburg/features_connectivity_within_aud_teas.npy', allow_pickle=True)\n",
    "\n",
    "features_power_teas = np.load('/Users/payamsadeghishabestari/eeg_data/Regensburg/features_teas_power_array.npy', allow_pickle=True)\n",
    "features_ent_teas = np.load('/Users/payamsadeghishabestari/eeg_data/Regensburg/features_teas_ent.npy', allow_pickle=True)\n",
    "features_ap_teas = np.load('/Users/payamsadeghishabestari/eeg_data/Regensburg/features_teas_ap_fooof.npy', allow_pickle=True)\n",
    "\n",
    "# features_power_teas = np.load('/Users/payamsadeghishabestari/eeg_data/Regensburg/features_teas_new_power_array.npy', allow_pickle=True)\n",
    "# features_ent_teas = np.load('/Users/payamsadeghishabestari/eeg_data/Regensburg/features_teas_new_ent.npy', allow_pickle=True)\n",
    "# features_ap_teas = np.load('/Users/payamsadeghishabestari/eeg_data/Regensburg/features_teas_new_ap_fooof.npy', allow_pickle=True)\n",
    "\n",
    "# features_power_teas = np.load('/Users/payamsadeghishabestari/eeg_data/Regensburg/features_tinnoice_power_array.npy', allow_pickle=True)\n",
    "# features_ent_teas = np.load('/Users/payamsadeghishabestari/eeg_data/Regensburg/features_tinnoice_ent.npy', allow_pickle=True)\n",
    "# features_ap_teas = np.load('/Users/payamsadeghishabestari/eeg_data/Regensburg/features_tinnoice_ap_fooof.npy', allow_pickle=True)\n",
    "\n",
    "# reshaping ap_fooof features\n",
    "features_ap_fooof_t = np.transpose(features_ap_fooof, axes=[0,2,1])\n",
    "features_ap_fooof_array = features_ap_fooof_t.reshape(features_ap_fooof_t.shape[0], -1)\n",
    "\n",
    "# concatenating features\n",
    "features_array = np.concatenate((features_power_array, features_ent_array, features_ap_fooof_array), axis=1)\n",
    "# features_ap_teas = features_ap_teas.reshape(features_ap_teas.shape[0], -1)\n",
    "# features_array = np.concatenate((features_power_teas, features_ent_teas, features_ap_teas), axis=1)\n",
    "\n",
    "# features_array = np.concatenate((features_con_teas_network, features_con_within_aud_teas), axis=1)\n",
    "\n",
    "\n",
    "#features_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_bl_rgb_mean_array = np.load('/Users/payamsadeghishabestari/eeg_data/Regensburg/power_brain_features_rgb_mean.npy', allow_pickle=True)\n",
    "# features_array = features_bl_teas_mean_array\n",
    "features_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Features correlation and distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we check which labels have more correlations (needs interpretation)\n",
    "# histogram\n",
    "df = pd.DataFrame(features_array)\n",
    "corr = df.corr(method='pearson')\n",
    "thr = 0.9\n",
    "n_reps = np.zeros(features_array.shape[1])\n",
    "tupels = []\n",
    "for f1, f2 in product(range(features_array.shape[1]), range(features_array.shape[1])):\n",
    "    if f1 < f2 and corr[f1][f2] > thr:\n",
    "        tupels.append((f1, f2))\n",
    "        n_reps[f1] += 1\n",
    "\n",
    "# correlation matrix\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.matshow(corr, cmap='coolwarm', fignum=1, vmin=-1, vmax=1)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,6))\n",
    "# ax.matshow(corr, cmap='coolwarm')\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.15)\n",
    "im = ax.imshow(corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "fig.colorbar(im, cax=cax, orientation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_labels = mne.read_labels_from_annot(subject='fsaverage', parc='aparc')[:-1]\n",
    "# for pair in np.array(tupels)[77:90] - 2 * 68: # alpha\n",
    "#     print(f'{brain_labels[pair[0]].name} and {brain_labels[pair[1]].name} are merged.')\n",
    "\n",
    "for pair in np.array(tupels)[-3:] - 4 * 68: # gamma\n",
    "    print(f'{brain_labels[pair[0]].name} and {brain_labels[pair[1]].name} are merged.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing features with high correlation\n",
    "thr = 0.9\n",
    "counter = 0\n",
    "rmv_idxs = []\n",
    "delta_drop, theta_drop, l_alpha_drop, u_alpha_drop, m_beta_drop, u_beta_drop, l_gamma_drop, u_gamma_drop = (0, 0, 0, 0, 0, 0, 0, 0)\n",
    "for f1, f2 in product(range(features_array.shape[1]), range(features_array.shape[1])):\n",
    "    if f1 < f2 and corr[f1][f2] > thr:\n",
    "        counter +=1\n",
    "        rmv_idxs.append(f1)\n",
    "        if 0*68 <= f1 < 1*68:\n",
    "            delta_drop += 1\n",
    "        if 1*68 <= f1 < 2*68:\n",
    "            theta_drop += 1\n",
    "        if 2*68 <= f1 < 3*68:\n",
    "            l_alpha_drop += 1\n",
    "        if 3*68 <= f1 < 4*68:\n",
    "            u_alpha_drop += 1\n",
    "        if 4*68 <= f1 < 5*68:\n",
    "            m_beta_drop += 1\n",
    "        if 5*68 <= f1 < 6*68:\n",
    "            u_beta_drop += 1\n",
    "        if 6*68 <= f1 < 7*68:\n",
    "            l_gamma_drop += 1\n",
    "        if 7*68 <= f1 < 8*68:\n",
    "            u_gamma_drop += 1\n",
    "\n",
    "# df.drop(df.columns[rmv_idxs], axis=1, inplace=True) \n",
    "keep_idxs = [i for i in range(features_array.shape[1]) if i not in rmv_idxs]\n",
    "features_array_after_corr = features_array[:,keep_idxs]\n",
    "features_array_after_corr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the barplots\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16,4))\n",
    "ax.bar(x=['delta', 'theta', 'lower alpha', 'upper alpha', 'middle beta',\n",
    "            'upper beta', 'lower gamma', 'upper gamma'], \n",
    "            height=[delta_drop, theta_drop, l_alpha_drop, u_alpha_drop,\n",
    "            m_beta_drop, u_beta_drop, l_gamma_drop, u_gamma_drop])\n",
    "ax.grid(True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Importing Classification Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBRFClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. a. Training with various CLFs (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "n_sel_features = 100\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(features_array_after_corr,\n",
    "                                                                            label_array, group_array,\n",
    "                                                                            test_size=0.3, train_size=0.7,\n",
    "                                                                            shuffle = True, random_state=None)\n",
    "mask_train = np.isnan(X_train)\n",
    "mask_test = np.isnan(X_test)\n",
    "nan_indices_train = np.where(mask_train)\n",
    "nan_indices_test = np.where(mask_test)\n",
    "for i in range(len(nan_indices_train[0])):\n",
    "    X_train[nan_indices_train[0][i]][nan_indices_train[1][i]] = 1\n",
    "for i in range(len(nan_indices_test[0])):\n",
    "    X_test[nan_indices_test[0][i]][nan_indices_test[1][i]] = 1\n",
    "\n",
    "n_scores = cross_val_score(model, X_train, y_train, groups = group_train ,scoring='accuracy', cv=10, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# reporting and features overview\n",
    "importance = model.feature_importances_\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'accuracy of the model checked with test data without eliminating features: {accuracy}')\n",
    "\n",
    "# re-training with selected features\n",
    "selected_features_idx = np.argsort(importance)[-n_sel_features:]\n",
    "new_features_array = features_array_after_corr[:,selected_features_idx]\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(new_features_array,\n",
    "                                                                            label_array, group_array,\n",
    "                                                                            test_size=0.3, train_size=0.7,\n",
    "                                                                            shuffle = True, random_state=None)\n",
    "\n",
    "mask_train = np.isnan(X_train)\n",
    "mask_test = np.isnan(X_test)\n",
    "nan_indices_train = np.where(mask_train)\n",
    "nan_indices_test = np.where(mask_test)\n",
    "for i in range(len(nan_indices_train[0])):\n",
    "    X_train[nan_indices_train[0][i]][nan_indices_train[1][i]] = 1\n",
    "for i in range(len(nan_indices_test[0])):\n",
    "    X_test[nan_indices_test[0][i]][nan_indices_test[1][i]] = 1\n",
    "\n",
    "n_scores = cross_val_score(model, X_train, y_train, groups = group_train ,scoring='accuracy', cv=10, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(f'accuracy of the model checked with test data with eliminating features: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for validation set\n",
    "fname = '/Users/payamsadeghishabestari/Prime/connectivity_teas_idxs.npy'\n",
    "np.save(fname, np.array(keep_idxs)[selected_features_idx])\n",
    "model_filename = '/Users/payamsadeghishabestari/Prime/connectivity_teas_model.pkl'\n",
    "joblib.dump(model, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now checking the importances\n",
    "#importance = model.feature_importances_\n",
    "brain_labels = mne.read_labels_from_annot(subject='fsaverage', parc='aparc') # 69 labels\n",
    "delta_imps, theta_imps, alpha_imps, beta_imps, gamma_imps = (0, 0, 0, 0, 0)\n",
    "teas_alpha_names = []; teas_alpha_imps= []; teas_gamma_names = []; teas_gamma_imps= []; teas_theta_imps = []; teas_delta_imps = []; teas_beta_imps = []\n",
    "# for f_idx, im in zip(np.array(keep_idxs)[selected_features_idx][:], np.sort(importance)[-n_sel_features:][:]):\n",
    "#     if 0*68 <= f_idx < 1*68:\n",
    "#         delta_imps += im\n",
    "#         # print(f'alpha labels {brain_labels[f_idx - 0 * 68].name} with importance {im}')\n",
    "#         teas_delta_imps.append(im)\n",
    "#     if 1*68 <= f_idx < 2*68:\n",
    "#         theta_imps += im\n",
    "#         # print(f'alpha labels {brain_labels[f_idx - 1 * 68].name} with importance {im}')\n",
    "#         teas_theta_imps.append(im)\n",
    "#     if 2*68 <= f_idx < 3*68:\n",
    "#         alpha_imps += im\n",
    "#         # print(f'alpha labels {brain_labels[f_idx - 2 * 68].name} with importance {im}')\n",
    "#         teas_alpha_names.append(brain_labels[f_idx - 2 * 68].name)\n",
    "#         teas_alpha_imps.append(im)\n",
    "#     if 3*68 <= f_idx < 4*68:\n",
    "#         beta_imps += im\n",
    "#         teas_beta_imps.append(im)\n",
    "#     if 4*68 <= f_idx < 5*68:\n",
    "#         gamma_imps += im\n",
    "#         # print(f'gamma labels {brain_labels[f_idx - 4 * 68].name} with importance {im}')\n",
    "#         teas_gamma_names.append(brain_labels[f_idx - 4 * 68].name)\n",
    "#         teas_gamma_imps.append(im)\n",
    "\n",
    "delta_imps, theta_imps, alpha_imps, beta_imps, gamma_imps = (0, 0, 0, 0, 0)\n",
    "ch_names = epochs_ec[0].info['ch_names']\n",
    "for f_idx, im in zip(np.array(keep_idxs)[selected_features_idx], np.sort(importance)[-n_sel_features:]):\n",
    "    if 0*64 <= f_idx < 1*64:\n",
    "        delta_imps += im\n",
    "    if 1*64 <= f_idx < 2*64:\n",
    "        theta_imps += im\n",
    "    if 2*64 <= f_idx < 3*64:\n",
    "        alpha_imps += im\n",
    "        # print(f'alpha labels {ch_names[f_idx - 2 * 64]} with importance {im}')\n",
    "        #print(f_idx - 2 * 62)\n",
    "    if 3*64 <= f_idx < 4*64:\n",
    "        beta_imps += im\n",
    "    if 4*64 <= f_idx < 5*64:\n",
    "        gamma_imps += im\n",
    "        print(f'gamma labels {ch_names[f_idx - 4 * 64]} with importance {im}')\n",
    "        # print(f_idx - 4 * 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the sns nodel with validation data\n",
    "model_fname = '/Users/payamsadeghishabestari/Prime/source_space_model.pkl'\n",
    "model = joblib.load(model_fname)\n",
    "sel_idxs_fname = '/Users/payamsadeghishabestari/Prime/source_space_f_idxs.npy'\n",
    "sel_idxs = np.load(sel_idxs_fname, allow_pickle=True)\n",
    "y_pred = model.predict(features_array[:, sel_idxs])\n",
    "accuracy_sns = accuracy_score(label_array, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_labels = mne.read_labels_from_annot(subject='fsaverage', parc='aparc') # 69 labels\n",
    "for f_idx, f_imp in zip(np.array(keep_idxs)[selected_features_idx][:], np.sort(importance)[-n_sel_features:][:]):\n",
    "    #if 0*68 <= f_idx < 1*68:\n",
    "    #    print(f'delta with labels {brain_labels[f_idx].name}')\n",
    "    #if 1*68 <= f_idx < 2*68:\n",
    "    #    print(f'theta with labels {brain_labels[f_idx - 1 * 68].name}')\n",
    "    # if 2*68 <= f_idx < 3*68:\n",
    "    #     print(f'low alpha with labels {brain_labels[f_idx - 2 * 68].name} with importance {f_imp}')\n",
    "    # if 3*68 <= f_idx < 4*68:\n",
    "    #     print(f'high alpha with labels {brain_labels[f_idx - 3 * 68].name} with importance {f_imp}')\n",
    "    #if 4*68 <= f_idx < 5*68:\n",
    "    #    print(f'middle beta labels {brain_labels[f_idx - 4 * 68].name}')\n",
    "    #if 5*68 <= f_idx < 6*68:\n",
    "    #    print(f'higher beta labels {brain_labels[f_idx - 5 * 68].name}')\n",
    "    #if 6*68 <= f_idx < 7*68:\n",
    "    #    print(f'low gamma labels {brain_labels[f_idx - 6 * 68].name} with importance {f_imp}')\n",
    "    #if 7*68 <= f_idx < 8*68:\n",
    "    #    print(f'high gamma labels {brain_labels[f_idx - 7 * 68].name} with importance {f_imp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. b. Training with various CLFs (XGBRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRFClassifier(n_estimators=100, subsample=0.9, colsample_bynode=0.2,\n",
    "                            objective = 'binary:logistic', eta=0.001)\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(features_array_after_corr,\n",
    "                                                                            label_array, group_array,\n",
    "                                                                            test_size=0.3, train_size=0.7,\n",
    "                                                                            shuffle = True, random_state=42)\n",
    "n_scores = cross_val_score(model, X_train, y_train, groups = group_train ,scoring='accuracy', cv=10, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# reporting and features overview\n",
    "importance = model.feature_importances_\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'accuracy of the model checked with test data without eliminating features: {accuracy}')\n",
    "\n",
    "# keeping only features with higher importance and labeling them\n",
    "Labels = ['delta_power', 'theta_power', 'alpha_power', 'beta_power', 'gamma_power',\n",
    "            'delta_ent', 'theta_ent', 'alpha_ent', 'beta_ent', 'gamma_ent',\n",
    "            'aperiodic_offset', 'aperiodic_exponent',\n",
    "            'delta_label_power', 'theta_label_power', 'alpha_label_power', 'beta_label_power', 'gamma_label_power']\n",
    "features_label = []\n",
    "selected_features_idx = np.argsort(importance)[-n_sel_features:] # selecting only 100 features with highest importance (almost equal to importance > 0.005)\n",
    "for idx in selected_features_idx:\n",
    "    for l_idx in range(len(Labels)):\n",
    "        if keep_idxs[idx] < 12 * 62:\n",
    "            if l_idx * 62 <= keep_idxs[idx] < (l_idx + 1) * 62:\n",
    "                features_label.append(Labels[l_idx])\n",
    "        if 12 * 62 <= keep_idxs[idx]:\n",
    "            if l_idx * 68 <= keep_idxs[idx] < (l_idx + 1) * 68:\n",
    "                features_label.append(Labels[l_idx])\n",
    "    \n",
    "for l_idx in range(len(Labels)):\n",
    "    print(f'{Labels[l_idx]} accounts for {int(len(np.where(np.array(features_label) == Labels[l_idx])[0]) / n_sel_features * 100)} % of selected features')\n",
    "    \n",
    "# re-training with selected features\n",
    "new_features_array = features_array_after_corr[:,selected_features_idx]\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(new_features_array,\n",
    "                                                                            label_array, group_array,\n",
    "                                                                            test_size=0.3, train_size=0.7,\n",
    "                                                                            shuffle = True, random_state=42)\n",
    "n_scores = cross_val_score(model, X_train, y_train, groups = group_train ,scoring='accuracy', cv=10, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'accuracy of the model checked with test data with eliminating features: {accuracy}')\n",
    "\n",
    "# which channels and which labels\n",
    "brain_labels = mne.read_labels_from_annot(subject='fsaverage', parc='aparc')[:-1] # 68 labels\n",
    "sensors_labels = []\n",
    "for idx in np.array(keep_idxs)[selected_features_idx]:\n",
    "    for l_idx in range(len(Labels)):\n",
    "        if idx < 5 * 62:\n",
    "            if l_idx * 62 <= idx < (l_idx + 1) * 62:\n",
    "                sensors_labels.append((Labels[l_idx], epochs_ec[0].info['ch_names'][idx - l_idx * 62]))\n",
    "        if 12 * 62 <= idx:\n",
    "            if l_idx * 68 <= idx < (l_idx + 1) * 68:\n",
    "                sensors_labels.append((Labels[l_idx], brain_labels[idx - l_idx * 68].name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. c. Training with various CLFs (Linear SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(C=1.0, random_state=42, kernel='linear')\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(features_array_after_corr,\n",
    "                                                                            label_array, group_array,\n",
    "                                                                            test_size=0.3, train_size=0.7,\n",
    "                                                                            shuffle = True, random_state=42)\n",
    "\n",
    "mask_train = np.isnan(X_train)\n",
    "mask_test = np.isnan(X_test)\n",
    "nan_indices_train = np.where(mask_train)\n",
    "nan_indices_test = np.where(mask_test)\n",
    "for i in range(len(nan_indices_train[0])):\n",
    "    X_train[nan_indices_train[0][i]][nan_indices_train[1][i]] = 1\n",
    "for i in range(len(nan_indices_test[0])):\n",
    "    X_test[nan_indices_test[0][i]][nan_indices_test[1][i]] = 1\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# reporting and features overview\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'accuracy of the model checked with test data without eliminating features: {accuracy}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. d. Training with various CLFs (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(activation='relu', alpha=1e-5, shuffle=True, \n",
    "                    hidden_layer_sizes=(100,), random_state=42)\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(features_array_after_corr,\n",
    "                                                                            label_array, group_array,\n",
    "                                                                            test_size=0.3, train_size=0.7,\n",
    "                                                                            shuffle=True, random_state=42)\n",
    "mask_train = np.isnan(X_train)\n",
    "mask_test = np.isnan(X_test)\n",
    "nan_indices_train = np.where(mask_train)\n",
    "nan_indices_test = np.where(mask_test)\n",
    "for i in range(len(nan_indices_train[0])):\n",
    "    X_train[nan_indices_train[0][i]][nan_indices_train[1][i]] = 1\n",
    "for i in range(len(nan_indices_test[0])):\n",
    "    X_test[nan_indices_test[0][i]][nan_indices_test[1][i]] = 1\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# reporting and features overview\n",
    "#importance = model.feature_importances_\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'accuracy of the model checked with test data without eliminating features: {accuracy}')\n",
    "\n",
    "# keeping only features with higher importance and labeling them\n",
    "Labels = ['delta_power', 'theta_power', 'alpha_power', 'beta_power', 'gamma_power',\n",
    "        'delta_ent', 'theta_ent', 'alpha_ent', 'beta_ent', 'gamma_ent',\n",
    "            'aperiodic_offset', 'aperiodic_exponent',\n",
    "            'delta_label_power', 'theta_label_power', 'alpha_label_power', 'beta_label_power', 'gamma_label_power']\n",
    "features_label = []\n",
    "selected_features_idx = np.argsort(importance)[-n_sel_features:] # selecting only 100 features with highest importance (almost equal to importance > 0.005)\n",
    "for idx in selected_features_idx:\n",
    "    for l_idx in range(len(Labels)):\n",
    "        if keep_idxs[idx] < 12 * 62:\n",
    "            if l_idx * 62 <= keep_idxs[idx] < (l_idx + 1) * 62:\n",
    "                features_label.append(Labels[l_idx])\n",
    "        if 12 * 62 <= keep_idxs[idx]:\n",
    "            if l_idx * 68 <= keep_idxs[idx] < (l_idx + 1) * 68:\n",
    "                features_label.append(Labels[l_idx])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. e. Training with various CLFs (K-NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier()\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(features_array_after_corr,\n",
    "                                                                            label_array, group_array,\n",
    "                                                                            test_size=0.3, train_size=0.7,\n",
    "                                                                            shuffle = True, random_state=42)\n",
    "mask_train = np.isnan(X_train)\n",
    "mask_test = np.isnan(X_test)\n",
    "nan_indices_train = np.where(mask_train)\n",
    "nan_indices_test = np.where(mask_test)\n",
    "for i in range(len(nan_indices_train[0])):\n",
    "    X_train[nan_indices_train[0][i]][nan_indices_train[1][i]] = 1\n",
    "for i in range(len(nan_indices_test[0])):\n",
    "    X_test[nan_indices_test[0][i]][nan_indices_test[1][i]] = 1\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# reporting and features overview\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'accuracy of the model checked with test data without eliminating features: {accuracy}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. f. Training with various CLFs (GP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianProcessClassifier(1.0 * RBF(1.0), random_state=42)\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(features_array_after_corr,\n",
    "                                                                            label_array, group_array,\n",
    "                                                                            test_size=0.3, train_size=0.7,\n",
    "                                                                            shuffle = True, random_state=42)\n",
    "mask_train = np.isnan(X_train)\n",
    "mask_test = np.isnan(X_test)\n",
    "nan_indices_train = np.where(mask_train)\n",
    "nan_indices_test = np.where(mask_test)\n",
    "for i in range(len(nan_indices_train[0])):\n",
    "    X_train[nan_indices_train[0][i]][nan_indices_train[1][i]] = 1\n",
    "for i in range(len(nan_indices_test[0])):\n",
    "    X_test[nan_indices_test[0][i]][nan_indices_test[1][i]] = 1\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# reporting and features overview\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'accuracy of the model checked with test data without eliminating features: {accuracy}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. g. Training with various CLFs (RBF SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(random_state=42)\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(features_array_after_corr,\n",
    "                                                                            label_array, group_array,\n",
    "                                                                            test_size=0.3, train_size=0.7,\n",
    "                                                                            shuffle = True, random_state=42)\n",
    "mask_train = np.isnan(X_train)\n",
    "mask_test = np.isnan(X_test)\n",
    "nan_indices_train = np.where(mask_train)\n",
    "nan_indices_test = np.where(mask_test)\n",
    "for i in range(len(nan_indices_train[0])):\n",
    "    X_train[nan_indices_train[0][i]][nan_indices_train[1][i]] = 1\n",
    "for i in range(len(nan_indices_test[0])):\n",
    "    X_test[nan_indices_test[0][i]][nan_indices_test[1][i]] = 1\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# reporting and features overview\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'accuracy of the model checked with test data without eliminating features: {accuracy}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. h. Training with various CLFs (Decision Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(random_state=42)\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(features_array_after_corr,\n",
    "                                                                            label_array, group_array,\n",
    "                                                                            test_size=0.3, train_size=0.7,\n",
    "                                                                            shuffle = True, random_state=42)\n",
    "mask_train = np.isnan(X_train)\n",
    "mask_test = np.isnan(X_test)\n",
    "nan_indices_train = np.where(mask_train)\n",
    "nan_indices_test = np.where(mask_test)\n",
    "for i in range(len(nan_indices_train[0])):\n",
    "    X_train[nan_indices_train[0][i]][nan_indices_train[1][i]] = 1\n",
    "for i in range(len(nan_indices_test[0])):\n",
    "    X_test[nan_indices_test[0][i]][nan_indices_test[1][i]] = 1\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# reporting and features overview\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'accuracy of the model checked with test data without eliminating features: {accuracy}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. i. Training with various CLFs (Naive Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(features_array_after_corr,\n",
    "                                                                            label_array, group_array,\n",
    "                                                                            test_size=0.3, train_size=0.7,\n",
    "                                                                            shuffle = True, random_state=42)\n",
    "mask_train = np.isnan(X_train)\n",
    "mask_test = np.isnan(X_test)\n",
    "nan_indices_train = np.where(mask_train)\n",
    "nan_indices_test = np.where(mask_test)\n",
    "for i in range(len(nan_indices_train[0])):\n",
    "    X_train[nan_indices_train[0][i]][nan_indices_train[1][i]] = 1\n",
    "for i in range(len(nan_indices_test[0])):\n",
    "    X_test[nan_indices_test[0][i]][nan_indices_test[1][i]] = 1\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# reporting and features overview\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'accuracy of the model checked with test data without eliminating features: {accuracy}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. j. Training with various CLFs (QDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QuadraticDiscriminantAnalysis()\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(features_array_after_corr,\n",
    "                                                                            label_array, group_array,\n",
    "                                                                            test_size=0.3, train_size=0.7,\n",
    "                                                                            shuffle = True, random_state=42)\n",
    "mask_train = np.isnan(X_train)\n",
    "mask_test = np.isnan(X_test)\n",
    "nan_indices_train = np.where(mask_train)\n",
    "nan_indices_test = np.where(mask_test)\n",
    "for i in range(len(nan_indices_train[0])):\n",
    "    X_train[nan_indices_train[0][i]][nan_indices_train[1][i]] = 1\n",
    "for i in range(len(nan_indices_test[0])):\n",
    "    X_test[nan_indices_test[0][i]][nan_indices_test[1][i]] = 1\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# reporting and features overview\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'accuracy of the model checked with test data without eliminating features: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. k. Training with various CLFs (penalizing with elastic net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty='elasticnet', solver='saga', C=1,\n",
    "                        l1_ratio=0.5, max_iter=10000, random_state=42)\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(features_array_after_corr,\n",
    "                                                                            label_array, group_array,\n",
    "                                                                            test_size=0.3, train_size=0.7,\n",
    "                                                                            shuffle = True, random_state=0)\n",
    "mask_train = np.isnan(X_train)\n",
    "mask_test = np.isnan(X_test)\n",
    "nan_indices_train = np.where(mask_train)\n",
    "nan_indices_test = np.where(mask_test)\n",
    "for i in range(len(nan_indices_train[0])):\n",
    "    X_train[nan_indices_train[0][i]][nan_indices_train[1][i]] = 1\n",
    "for i in range(len(nan_indices_test[0])):\n",
    "    X_test[nan_indices_test[0][i]][nan_indices_test[1][i]] = 1\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# reporting and features overview\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'accuracy of the model checked with test data without eliminating features: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Importance of features in Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_delta_imps = []\n",
    "# all_theta_imps = []\n",
    "# all_alpha_imps = []\n",
    "# all_beta_imps = []\n",
    "# all_gamma_imps = []\n",
    "# all_delta_ent_imps = []\n",
    "# all_theta_ent_imps = []\n",
    "# all_alpha_ent_imps = []\n",
    "# all_beta_ent_imps = []\n",
    "# all_gamma_ent_imps = []\n",
    "# all_ap_1_imps = []\n",
    "# all_ap_2_imps = []\n",
    "iteration = 0\n",
    "while iteration < 1:\n",
    "    # initial training\n",
    "    model = RandomForestClassifier(n_estimators=100)\n",
    "    X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(features_array_after_corr,\n",
    "                                                                                label_array, group_array,\n",
    "                                                                                test_size=0.3, train_size=0.7,\n",
    "                                                                                shuffle = True, random_state=0)\n",
    "    n_scores = cross_val_score(model, X_train, y_train, groups = group_train ,scoring='accuracy', cv=10, n_jobs=-1)\n",
    "\n",
    "    # handling Nan values for random forest\n",
    "    mask_train = np.isnan(X_train)\n",
    "    mask_test = np.isnan(X_test)\n",
    "    nan_indices_train = np.where(mask_train)\n",
    "    nan_indices_test = np.where(mask_test)\n",
    "    for i in range(len(nan_indices_train[0])):\n",
    "        X_train[nan_indices_train[0][i]][nan_indices_train[1][i]] = 1\n",
    "    for i in range(len(nan_indices_test[0])):\n",
    "        X_test[nan_indices_test[0][i]][nan_indices_test[1][i]] = 1\n",
    "\n",
    "    # selecting features with highest importance\n",
    "    model.fit(X_train, y_train)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'accuracy of the model checked with test data without eliminating features: {accuracy}')\n",
    "    importances = model.feature_importances_\n",
    "    n_sel_features = 100\n",
    "    selected_features_idx = np.argsort(importances)[-n_sel_features:]\n",
    "    new_features_array = features_array_after_corr[:,selected_features_idx]\n",
    "\n",
    "    # re-training with selected features\n",
    "    X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(new_features_array,\n",
    "                                                                                label_array, group_array,\n",
    "                                                                                test_size=0.3, train_size=0.7,\n",
    "                                                                                shuffle = True, random_state=0)\n",
    "    n_scores = cross_val_score(model, X_train, y_train, groups = group_train ,scoring='accuracy', cv=10, n_jobs=-1)\n",
    "    # handling Nan values for random forest\n",
    "    mask_train = np.isnan(X_train)\n",
    "    mask_test = np.isnan(X_test)\n",
    "    nan_indices_train = np.where(mask_train)\n",
    "    nan_indices_test = np.where(mask_test)\n",
    "    for i in range(len(nan_indices_train[0])):\n",
    "        X_train[nan_indices_train[0][i]][nan_indices_train[1][i]] = 1\n",
    "    for i in range(len(nan_indices_test[0])):\n",
    "        X_test[nan_indices_test[0][i]][nan_indices_test[1][i]] = 1\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'accuracy of the model checked with test data with eliminating features: {accuracy}')\n",
    "    importance = model.feature_importances_\n",
    "\n",
    "    delta_imps, theta_imps, alpha_imps, beta_imps, gamma_imps = (0, 0, 0, 0, 0)\n",
    "    delta_ent_imps, theta_ent_imps, alpha_ent_imps, beta_ent_imps, gamma_ent_imps = (0, 0, 0, 0, 0)\n",
    "    ap_1_imps, ap_2_imps = (0, 0)\n",
    "    for f_idx, im in zip(np.array(keep_idxs)[selected_features_idx], importance):\n",
    "        if 0*68 <= f_idx < 1*68:\n",
    "            delta_imps += im\n",
    "        if 1*68 <= f_idx < 2*68:\n",
    "            theta_imps += im\n",
    "        if 2*68 <= f_idx < 3*68:\n",
    "            alpha_imps += im\n",
    "        if 3*68 <= f_idx < 4*68:\n",
    "            beta_imps += im\n",
    "        if 4*68 <= f_idx < 5*68:\n",
    "            gamma_imps += im\n",
    "        \n",
    "        # if 0*62 <= f_idx < 1*62:\n",
    "        #     delta_imps += im\n",
    "        # if 1*62 <= f_idx < 2*62:\n",
    "        #     theta_imps += im\n",
    "        # if 2*62 <= f_idx < 3*62:\n",
    "        #     alpha_imps += im\n",
    "        # if 3*62 <= f_idx < 4*62:\n",
    "        #     beta_imps += im\n",
    "        # if 4*62 <= f_idx < 5*62:\n",
    "        #     gamma_imps += im\n",
    "        # if 5*62 <= f_idx < 6*62:\n",
    "        #     delta_ent_imps += im\n",
    "        # if 6*62 <= f_idx < 7*62:\n",
    "        #     theta_ent_imps += im\n",
    "        # if 7*62 <= f_idx < 8*62:\n",
    "        #     alpha_ent_imps += im\n",
    "        # if 8*62 <= f_idx < 9*62:\n",
    "        #     beta_ent_imps += im\n",
    "        # if 9*62 <= f_idx < 10*62:\n",
    "        #     gamma_ent_imps += im\n",
    "        # if 10*62 <= f_idx < 11*62:\n",
    "        #     ap_1_imps += im\n",
    "        # if 11*62 <= f_idx < 12*62:\n",
    "        #     ap_2_imps += im\n",
    "\n",
    "    # all_delta_imps.append(delta_imps)\n",
    "    # all_theta_imps.append(theta_imps)\n",
    "    # all_alpha_imps.append(alpha_imps)\n",
    "    # all_beta_imps.append(beta_imps)\n",
    "    # all_gamma_imps.append(gamma_imps)\n",
    "    # all_delta_ent_imps.append(delta_ent_imps)\n",
    "    # all_theta_ent_imps.append(theta_ent_imps)\n",
    "    # all_alpha_ent_imps.append(alpha_ent_imps)\n",
    "    # all_beta_ent_imps.append(beta_ent_imps)\n",
    "    # all_gamma_ent_imps.append(gamma_ent_imps)\n",
    "    # all_ap_1_imps.append(ap_1_imps)\n",
    "    # all_ap_2_imps.append(ap_2_imps)\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(19,7))\n",
    "lists = [all_delta_imps, all_theta_imps, all_alpha_imps, all_beta_imps, all_gamma_imps,\n",
    "        all_delta_ent_imps, all_theta_ent_imps, all_alpha_ent_imps, all_beta_ent_imps, all_gamma_ent_imps,\n",
    "        all_ap_1_imps, all_ap_2_imps]\n",
    "names_list = ['Delta power', 'Theta power', 'Alpha power', 'Beta power', 'Gamma power',\n",
    "        'Delta entropy', 'Theta entropy', 'Alpha entropy', 'Beta entropy', 'Gamma entropy',\n",
    "        'Aperiodic offset', 'Aperiodic exponent']\n",
    "\n",
    "data = pd.DataFrame({'Features': [item for item in names_list for _ in range(20)],\n",
    "                        'Importance': [val for sublist in lists for val in sublist]})\n",
    "\n",
    "# Create the violin plot using Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.violinplot(x='Features', y='Importance', data=data, bw='scott', palette=\"Set2\", saturation=0.75, linewidth=1, ax=ax)\n",
    "sns.swarmplot(x='Features', y='Importance', data=data, palette=\"Set2\", linewidth=1, ax=ax, size=3)\n",
    "\n",
    "ax.legend(fontsize=14, frameon=False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.grid(axis='x', color='k', linestyle='--', linewidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the importances by category\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10))\n",
    "ax.barh(y=['delta_ent', 'theta_ent', 'delta_power',\n",
    "'beta_ent','theta_power', 'alpha_ent', 'ap_1', 'ap_2', 'beta_power',\n",
    "'gamma_ent', 'alpha_power', 'gamma_power'], width=[delta_ent_imps, \n",
    "theta_ent_imps, delta_imps, beta_ent_imps, theta_imps, alpha_ent_imps,\n",
    "ap_1_imps, ap_2_imps, beta_imps, gamma_ent_imps, alpha_imps, gamma_imps])\n",
    "ax.spines['left'].set_visible(False); ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False); ax.spines['right'].set_visible(False)\n",
    "ax.grid(axis='x', linestyle=':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Partial Dependency of features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import partial_dependence\n",
    "from sklearn.inspection import PartialDependenceDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing the 10 most important indexes for each class of feature\n",
    "a_p_idxs = []; g_p_idxs = []; a_e_idxs = []; g_e_idxs = []; ap_of_idxs = []; ap_sl_idxs = []\n",
    "for idx, i in enumerate(np.array(keep_idxs)[selected_features_idx]):\n",
    "    if 2*62 <= i < 3*62:\n",
    "        a_p_idxs.append(idx)\n",
    "    if 4*62 <= i < 5*62:\n",
    "        g_p_idxs.append(idx)\n",
    "    if 7*62 <= i < 8*62:\n",
    "        a_e_idxs.append(idx)\n",
    "    if 9*62 <= i < 10*62:\n",
    "        g_e_idxs.append(idx)\n",
    "    if 10*62 <= i < 11*62:\n",
    "        ap_of_idxs.append(idx)\n",
    "    if 11*62 <= i < 12*62:\n",
    "        ap_sl_idxs.append(idx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 10 most important features (pdp apply)\n",
    "pds = {'alpha_power_values': [], 'alpha_power_pd': [],\n",
    "        'gamma_power_values': [], 'gamma_power_pd': [],\n",
    "        'alpha_ent_values': [], 'alpha_ent_pd': [],\n",
    "        'gamma_ent_values': [], 'gamma_ent_pd': [],\n",
    "        'ap_off_values': [], 'ap_off_pd': [],\n",
    "        'ap_sl_values': [], 'ap_sl_pd': []}\n",
    "\n",
    "for al, ga in zip(alpha_label_idxs[-10:], gamma_label_idxs[-10:]): \n",
    "        pd = partial_dependence(model, np.concatenate((X_train, X_test)), features=[al], kind='average')\n",
    "        pds['alpha_power_values'].append(pd['values'][0])\n",
    "        pds['alpha_power_pd'].append(pd['average'][0])\n",
    "        ##\n",
    "        pd = partial_dependence(model, np.concatenate((X_train, X_test)), features=[ga], kind='average')\n",
    "        pds['gamma_power_values'].append(pd['values'][0])\n",
    "        pds['gamma_power_pd'].append(pd['average'][0])\n",
    "        ##\n",
    "        #pd = partial_dependence(model, np.concatenate((X_train, X_test)), features=[a_e_f], kind='average')\n",
    "        #pds['alpha_ent_values'].append(pd['values'][0])\n",
    "        #pds['alpha_ent_pd'].append(pd['average'][0])\n",
    "        ##\n",
    "        #pd = partial_dependence(model, np.concatenate((X_train, X_test)), features=[g_e_f], kind='average')\n",
    "        #pds['gamma_ent_values'].append(pd['values'][0])\n",
    "        #pds['gamma_ent_pd'].append(pd['average'][0])\n",
    "        ##\n",
    "        # pd = partial_dependence(model, np.concatenate((X_train, X_test)), features=[ap_of], kind='average')\n",
    "        # pds['ap_off_values'].append(pd['values'][0])\n",
    "        # pds['ap_off_pd'].append(pd['average'][0])\n",
    "        # pd = partial_dependence(model, np.concatenate((X_train, X_test)), features=[ap_sl], kind='average')\n",
    "        # pds['ap_sl_values'].append(pd['values'][0])\n",
    "        # pds['ap_sl_pd'].append(pd['average'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_names = ['frontalpole-lh', 'supramarginal-rh', 'superiortemporal-rh', 'inferiortemporal-lh', 'lateraloccipital-rh', 'medialorbitofrontal-rh'\n",
    ", 'temporalpole-rh', 'pericalcarine-lh', 'transversetemporal-rh', 'parahippocampal-lh']\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,6))\n",
    "for i, lb_name in zip(range(10), lb_names):\n",
    "    ax.plot(pds['gamma_power_values'][i], pds['gamma_power_pd'][i], label=lb_name)\n",
    "ax.spines['top'].set_visible(False); ax.spines['right'].set_visible(False)\n",
    "ax.legend()\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing the 10 most important indexes for each class of feature\n",
    "a_p_idxs = []; g_p_idxs = []; a_e_idxs = []; g_e_idxs = []; ap_of_idxs = []; ap_sl_idxs = []\n",
    "for idx, i in enumerate(np.array(keep_idxs)[selected_features_idx]):\n",
    "    if 2*62 <= i < 3*62:\n",
    "        a_p_idxs.append(idx)\n",
    "    if 4*62 <= i < 5*62:\n",
    "        g_p_idxs.append(idx)\n",
    "    if 7*62 <= i < 8*62:\n",
    "        a_e_idxs.append(idx)\n",
    "    if 9*62 <= i < 10*62:\n",
    "        g_e_idxs.append(idx)\n",
    "    if 10*62 <= i < 11*62:\n",
    "        ap_of_idxs.append(idx)\n",
    "    if 11*62 <= i < 12*62:\n",
    "        ap_sl_idxs.append(idx) \n",
    "\n",
    "alpha_label_idxs = []; gamma_label_idxs = []\n",
    "for idx, i in enumerate(np.array(keep_idxs)[selected_features_idx]):\n",
    "    if 2*68 <= i < 3*68:\n",
    "        alpha_label_idxs.append(idx)\n",
    "    if 4*68 <= i < 5*68:\n",
    "        gamma_label_idxs.append(idx)\n",
    "yy = []\n",
    "brain_labels = mne.read_labels_from_annot(subject='fsaverage', parc='aparc')[:-1] # 68 labels\n",
    "for label in gamma_label_idxs:\n",
    "    lb_idx = np.array(keep_idxs)[selected_features_idx][label] - 4 * 68 # for gamma 4 * 62\n",
    "    yy.append(brain_labels[lb_idx].name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_fname = '/Users/payamsadeghishabestari/eeg_data/Regensburg/transfer_147155_files_ac07b771/aaeeg_rsEEG/rd7p_rest.vhdr'\n",
    "raw_rs = mne.io.read_raw_brainvision(vhdr_fname=raw_fname, preload=True, verbose=False)\n",
    "new_ch_names = raw_rs.info['ch_names'].copy()\n",
    "new_ch_names[58] = 'Fpz'\n",
    "mapping = dict(zip(raw_rs.info['ch_names'], new_ch_names))\n",
    "mne.rename_channels(raw_rs.info, mapping)\n",
    "raw_rs.set_montage('standard_1020', verbose=False, on_missing='warn')\n",
    "ch_names = raw_rs.info['ch_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_rs.info['bads'] = []\n",
    "for ch_i in g_p_idxs:\n",
    "    ch_idx = np.array(keep_idxs)[selected_features_idx][ch_i] - 4 * 62 # for gamma 4 * 62\n",
    "    raw_rs.info['bads'].append(ch_names[ch_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_rs.info['bads'][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_rs.info['bads'] = []\n",
    "raw_rs.plot_sensors(show_names=True, kind='select')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, i in enumerate(brain_labels):\n",
    "    if i.name == 'transversetemporal-rh':\n",
    "        sel_idx = idx\n",
    "sel_idx = 4 * 68 + sel_idx\n",
    "np.where(np.array(corr[sel_idx]) > 0.9)[0] - 4 * 68"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. a. Plotting 3D brain with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting brain with colored labels\n",
    "# initiating the brain\n",
    "Brain = mne.viz.get_brain_class()\n",
    "brain_kwargs = dict(alpha=0.15, background=\"white\", cortex=\"low_contrast\", size=(800, 600), views='medial')\n",
    "brain_labels = mne.read_labels_from_annot(subject='fsaverage', parc='aparc') # 69 labels\n",
    "imp_brain_label_list_gamma = ['parahippocampal-lh', 'transversetemporal-rh', 'pericalcarine-lh', 'temporalpole-rh',\n",
    "                        'medialorbitofrontal-rh', 'lateraloccipital-rh', 'inferiortemporal-lh', 'superiortemporal-rh',\n",
    "                        'supramarginal-rh', 'frontalpole-lh']\n",
    "imp_brain_label_list_alpha = ['parsopercularis-rh', 'posteriorcingulate-rh', 'superiorparietal-rh', 'parahippocampal-rh',\n",
    "                            'rostralmiddlefrontal-lh', 'inferiortemporal-lh', 'caudalmiddlefrontal-rh', 'parstriangularis-rh',\n",
    "                            'lateraloccipital-lh', 'postcentral-lh']\n",
    "imp_lb_a_rh_idxs = []; imp_lb_a_lh_idxs = []\n",
    "imp_lb_g_rh_idxs = []; imp_lb_g_lh_idxs = []\n",
    "\n",
    "# indexing right/left and alpha/gamma labels\n",
    "for im_l_a, im_l_g in zip(imp_brain_label_list_alpha, imp_brain_label_list_gamma):\n",
    "    for l_idx, l_id in enumerate(brain_labels):\n",
    "        if l_id.name == im_l_a and l_id.name[-2:] == 'rh':\n",
    "            imp_lb_a_rh_idxs.append(l_idx)\n",
    "        if l_id.name == im_l_a and l_id.name[-2:] == 'lh':\n",
    "            imp_lb_a_lh_idxs.append(l_idx)\n",
    "        if l_id.name == im_l_g and l_id.name[-2:] == 'rh':\n",
    "            imp_lb_g_rh_idxs.append(l_idx)\n",
    "        if l_id.name == im_l_g and l_id.name[-2:] == 'lh':\n",
    "            imp_lb_g_lh_idxs.append(l_idx)\n",
    "\n",
    "# plotting\n",
    "brain = Brain(\"fsaverage\", hemi=\"rh\", surf=\"pial\", title='alpha-rh', **brain_kwargs)\n",
    "for label in list(np.array(brain_labels)[imp_lb_a_rh_idxs]): # alpha-rh\n",
    "    brain.add_label(label, hemi=\"rh\", color=\"#1f77b4\", borders=False, alpha=0.5)\n",
    "brain.save_image(filename='/Users/payamsadeghishabestari/Prime/parcel-medial-alpha-rh.png', mode='rgb')\n",
    "\n",
    "brain = Brain(\"fsaverage\", hemi=\"lh\", surf=\"pial\", title='alpha-lh', **brain_kwargs)\n",
    "for label in list(np.array(brain_labels)[imp_lb_a_lh_idxs]): # alpha-lh\n",
    "    brain.add_label(label, hemi=\"lh\", color=\"#1f77b4\", borders=False, alpha=0.5)\n",
    "brain.save_image(filename='/Users/payamsadeghishabestari/Prime/parcel-medial-alpha-lh.png', mode='rgb')\n",
    "\n",
    "\n",
    "brain = Brain(\"fsaverage\", hemi=\"rh\", surf=\"pial\", title='gamma-rh', **brain_kwargs)\n",
    "for label in list(np.array(brain_labels)[imp_lb_g_rh_idxs]): # gamma-rh\n",
    "    brain.add_label(label, hemi=\"rh\", color=\"#ff7f0e\", borders=False, alpha=0.5)\n",
    "brain.save_image(filename='/Users/payamsadeghishabestari/Prime/parcel-medial-gamma-rh.png', mode='rgb')\n",
    "\n",
    "\n",
    "brain = Brain(\"fsaverage\", hemi=\"lh\", surf=\"pial\", title='gamma-lh', **brain_kwargs)\n",
    "for label in list(np.array(brain_labels)[imp_lb_g_lh_idxs]): # gamma-lh\n",
    "    brain.add_label(label, hemi=\"lh\", color=\"#ff7f0e\", borders=False, alpha=0.5)\n",
    "brain.save_image(filename='/Users/payamsadeghishabestari/Prime/parcel-medial-gamma-lh.png', mode='rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting only parcelations\n",
    "Brain = mne.viz.get_brain_class()\n",
    "brain = Brain(\"fsaverage\", hemi=\"lh\", surf=\"pial\", cortex=\"low_contrast\",\n",
    "    background=\"white\",\n",
    "    size=(800, 600))\n",
    "brain.add_annotation(\"aparc\", borders=False, alpha=0.6)\n",
    "brain.save_image(filename='/Users/payamsadeghishabestari/Prime/parcel-lh.png', mode='rgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. b. Plotting accuracies of different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting barplot accuracies\n",
    "######## single 90 threshold\n",
    "clfs = ['Random Forest', 'XGBRF', 'QDA', 'Naive Bayes', 'Desicion Tree', 'RBF SVM', 'GP', 'KNN', 'CNN', 'linear SVM']\n",
    "heights = [98.6, 94.1, 99.3, 67.0, 89.7, 71.2, 84.3, 84.1, 96.2, 93.7]\n",
    "alphas = [ 0.9, 0.7, 1, 0.1, 0.5, 0.2, 0.4, 0.3, 0.8, 0.6]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "p = []\n",
    "for ci in range(len(clfs)):\n",
    "       ax.barh(y=clfs[ci], width=heights[ci], height=0.4, color='#1f77b4', left=None, edgecolor=None, linewidth=1,\n",
    "              alpha=alphas[ci])      \n",
    "ax.grid(axis='x')\n",
    "ax.spines['top'].set_visible(False); ax.spines['right'].set_visible(False)\n",
    "ax.set_xlim(50, 100)\n",
    "ax.set_title('threshold 90 (single)')\n",
    "for i, v in enumerate(heights):\n",
    "       ax.text(v + 1, i-.15, f'{v}%', color='red', fontfamily='monospace')\n",
    "\n",
    "######## single 80 threshold\n",
    "clfs = ['Random Forest', 'XGBRF', 'QDA', 'Naive Bayes', 'Desicion Tree', 'RBF SVM', 'GP', 'KNN', 'CNN', 'linear SVM']\n",
    "heights = [97.6, 89.1, 96.81, 70.2, 90.4, 68.1, 84.6, 84.7, 96.2, 94.4]\n",
    "alphas = [ 0.9, 0.7, 1, 0.1, 0.5, 0.2, 0.4, 0.3, 0.8, 0.6]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "p = []\n",
    "for ci in range(len(clfs)):\n",
    "       ax.barh(y=clfs[ci], width=heights[ci], height=0.4, color='#1f77b4', left=None, edgecolor=None, linewidth=1,\n",
    "              alpha=alphas[ci])\n",
    "ax.grid(axis='x')\n",
    "ax.spines['top'].set_visible(False); ax.spines['right'].set_visible(False)\n",
    "ax.set_xlim(50, 100)\n",
    "ax.set_title('threshold 80 (single)')\n",
    "for i, v in enumerate(heights):\n",
    "       ax.text(v + 1, i-.15, f'{v}%', color='red', fontfamily='monospace')\n",
    "\n",
    "######## single 70 threshold\n",
    "clfs = ['Random Forest', 'XGBRF', 'QDA', 'Naive Bayes', 'Desicion Tree', 'RBF SVM', 'GP', 'KNN', 'CNN', 'linear SVM']\n",
    "heights = [96.8, 90.3, 93.9, 77.5, 89.7, 72.9, 85.8, 86.5, 96.4, 95.8]\n",
    "alphas = [ 0.9, 0.7, 1, 0.1, 0.5, 0.2, 0.4, 0.3, 0.8, 0.6]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "p = []\n",
    "for ci in range(len(clfs)):\n",
    "       ax.barh(y=clfs[ci], width=heights[ci], height=0.4, color='#1f77b4', left=None, edgecolor=None, linewidth=1,\n",
    "              alpha=alphas[ci])\n",
    "ax.grid(axis='x')\n",
    "ax.spines['top'].set_visible(False); ax.spines['right'].set_visible(False)\n",
    "ax.set_xlim(50, 100)\n",
    "ax.set_title('threshold 70 (single)')\n",
    "for i, v in enumerate(heights):\n",
    "       ax.text(v + 1, i-.15, f'{v}%', color='red', fontfamily='monospace')\n",
    "\n",
    "######## multi class (larger than 90, 90-70, 70-50, less than 50)\n",
    "clfs = ['Random Forest', 'XGBRF', 'QDA', 'Naive Bayes', 'Desicion Tree', 'RBF SVM', 'GP', 'KNN', 'CNN', 'linear SVM']\n",
    "heights = [97.0, 89.8, 65.0, 38.1, 85.8, 55.4, 18.6, 78.5, 95.4, 94.8]\n",
    "alphas = [ 0.9, 0.7, 1, 0.1, 0.5, 0.2, 0.4, 0.3, 0.8, 0.6]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "p = []\n",
    "for ci in range(len(clfs)):\n",
    "       ax.barh(y=clfs[ci], width=heights[ci], height=0.4, color='#1f77b4', left=None, edgecolor=None, linewidth=1,\n",
    "              alpha=alphas[ci])\n",
    "ax.grid(axis='x')\n",
    "ax.spines['top'].set_visible(False); ax.spines['right'].set_visible(False)\n",
    "ax.set_xlim(50, 100)\n",
    "ax.set_title('multi class threshold')\n",
    "for i, v in enumerate(heights):\n",
    "       ax.text(v + 1, i-.15, f'{v}%', color='red', fontfamily='monospace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting spider plots\n",
    "df = pd.DataFrame({\n",
    "'group': ['Thr 90','Thr 80','Thr 70', 'Thr 60', 'Thr 50'],\n",
    "'Random Forest': [98.6, 97.6, 96.8, 96.7, 96.2],\n",
    "'XGBRF': [94.1, 89.1, 90.3, 92.12, 91.7],\n",
    "'QDA': [99.3, 96.8, 93.9, 86, 84.5],\n",
    "'NB': [67.0, 70.2, 77.5, 81.97, 84.7],\n",
    "'Desicion Tree': [89.7, 90.4, 89.7, 90.67, 92.8],\n",
    "'RBF SVM': [71.2, 68.1, 72.9, 79.80, 83.7],\n",
    "'GP': [84.3, 84.6, 85.8, 90.12, 91.7],\n",
    "'KNN': [84.1, 84.7, 86.5, 91, 92.1],\n",
    "'CNN': [96.2, 96.2, 96.4, 97.6, 97.7],\n",
    "'linear SVM': [93.7, 94.4, 95.8, 96.41, 97.6],\n",
    "})\n",
    "\n",
    "# df = pd.DataFrame({\n",
    "# 'group': ['Thr 90', 'Thr 70', 'Thr 50'],\n",
    "# 'Random Forest': [99, 98, 96],\n",
    "# 'XGBRF': [91, 86, 93],\n",
    "# 'QDA': [84, 80, 80],\n",
    "# 'NB': [57, 73, 82],\n",
    "# 'Desicion Tree': [85, 90, 91],\n",
    "# 'RBF SVM': [73, 77, 86],\n",
    "# 'GP': [85, 87, 88],\n",
    "# 'KNN': [80, 84, 90],\n",
    "# 'CNN': [94, 96, 97],\n",
    "# 'linear SVM': [82, 87, 92],\n",
    "# })\n",
    "\n",
    "def make_spider(row, title, color):\n",
    "\n",
    "    # number of variable\n",
    "    categories=list(df)[1:]\n",
    "    N = len(categories)\n",
    "\n",
    "    # What will be the angle of each axis in the plot? (we divide the plot / number of variable)\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "\n",
    "    # Initialise the spider plot\n",
    "    ax = plt.subplot(1,5,row+1, polar=True)\n",
    "\n",
    "    # If you want the first axis to be on top:\n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    # Draw one axe per variable + add labels labels yet\n",
    "    # plt.xticks(angles[:-1], categories, color='black', size=10)\n",
    "\n",
    "    # Draw ylabels\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([50,60,70,80,90], [\"50\",\"60\",\"70\",\"80\",\"90\"], color=\"black\", size=11)\n",
    "    plt.ylim(50,100)\n",
    "\n",
    "    # Ind1\n",
    "    values=df.loc[row].drop('group').values.flatten().tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, color=color, linewidth=3, linestyle='solid')\n",
    "    ax.fill(angles, values, color=color, alpha=0.4)\n",
    "\n",
    "    # Add a title\n",
    "    plt.title(title, size=11, color=color, y=1.1)\n",
    "\n",
    "    \n",
    "# ------- PART 2: Apply the function to all individuals\n",
    "# initialize the figure\n",
    "my_dpi=116\n",
    "plt.figure(figsize=(12, 4), dpi=my_dpi)\n",
    " \n",
    "# Create a color palette:\n",
    "my_palette = plt.cm.get_cmap(\"Set2\", len(df.index))\n",
    " \n",
    "# Loop to plot\n",
    "for row in range(0, len(df.index)):\n",
    "    make_spider( row=row, title=''+df['group'][row], color=my_palette(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. c. Plotting Connectivity circle plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading labeling\n",
    "coh_alpha_files = []; coh_gamma_files = []\n",
    "wpli_alpha_files = []; wpli_gamma_files = []\n",
    "lb_list_unique = []\n",
    "for label in label_list:\n",
    "    lb_list_unique.append(label[0])\n",
    "\n",
    "# loading and sorting files\n",
    "folder_path = '/Users/payamsadeghishabestari/eeg_data/Regensburg/connectivity/rgb_mean_labels'  \n",
    "if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "    files_in_folder = sorted(os.listdir(folder_path), key=lambda x: os.path.getmtime(os.path.join(folder_path, x)))\n",
    "    for filename in files_in_folder:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path) and file_path.endswith(\".npy\"):\n",
    "            if 'coh_alpha' in file_path:\n",
    "                coh_alpha_files.append(file_path)\n",
    "            if 'coh_gamma' in file_path:\n",
    "                coh_gamma_files.append(file_path)\n",
    "            if 'wpli_alpha' in file_path:\n",
    "                wpli_alpha_files.append(file_path)\n",
    "            if 'wpli_gamma' in file_path:\n",
    "                wpli_gamma_files.append(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coherence alpha\n",
    "inhibit_files = []\n",
    "non_inhibit_files = []\n",
    "for f_idx, f in enumerate(coh_alpha_files): \n",
    "    if lb_list_unique[f_idx] == 0:\n",
    "        non_inhibit_files.append(f)\n",
    "    if lb_list_unique[f_idx] == 1:\n",
    "        inhibit_files.append(f)\n",
    "\n",
    "con_arrs = []\n",
    "for arr_file in inhibit_files:\n",
    "    con_arr = np.load(arr_file, allow_pickle=True)\n",
    "    con_arrs.append(np.mean(con_arr, axis=0))\n",
    "coh_inhibit_alpha_avg = np.mean(np.array(con_arrs), axis=0)\n",
    "con_arrs = []\n",
    "for arr_file in non_inhibit_files:\n",
    "    con_arr = np.load(arr_file, allow_pickle=True)\n",
    "    con_arrs.append(np.mean(con_arr, axis=0))\n",
    "coh_non_inhibit_alpha_avg = np.mean(np.array(con_arrs), axis=0)\n",
    "\n",
    "# coherence gamma\n",
    "inhibit_files = []\n",
    "non_inhibit_files = []\n",
    "for f_idx, f in enumerate(coh_gamma_files): \n",
    "    if lb_list_unique[f_idx] == 0:\n",
    "        non_inhibit_files.append(f)\n",
    "    if lb_list_unique[f_idx] == 1:\n",
    "        inhibit_files.append(f)\n",
    "\n",
    "con_arrs = []\n",
    "for arr_file in inhibit_files:\n",
    "    con_arr = np.load(arr_file, allow_pickle=True)\n",
    "    con_arrs.append(np.mean(con_arr, axis=0))\n",
    "coh_inhibit_gamma_avg = np.mean(np.array(con_arrs), axis=0)\n",
    "con_arrs = []\n",
    "for arr_file in non_inhibit_files:\n",
    "    con_arr = np.load(arr_file, allow_pickle=True)\n",
    "    con_arrs.append(np.mean(con_arr, axis=0))\n",
    "coh_non_inhibit_gamma_avg = np.mean(np.array(con_arrs), axis=0)\n",
    "\n",
    "# wpli alpha\n",
    "inhibit_files = []\n",
    "non_inhibit_files = []\n",
    "for f_idx, f in enumerate(wpli_alpha_files): \n",
    "    if lb_list_unique[f_idx] == 0:\n",
    "        non_inhibit_files.append(f)\n",
    "    if lb_list_unique[f_idx] == 1:\n",
    "        inhibit_files.append(f)\n",
    "\n",
    "con_arrs = []\n",
    "for arr_file in inhibit_files:\n",
    "    con_arr = np.load(arr_file, allow_pickle=True)\n",
    "    con_arrs.append(np.mean(con_arr, axis=0))\n",
    "wpli_inhibit_alpha_avg = np.mean(np.array(con_arrs), axis=0)\n",
    "con_arrs = []\n",
    "for arr_file in non_inhibit_files:\n",
    "    con_arr = np.load(arr_file, allow_pickle=True)\n",
    "    con_arrs.append(np.mean(con_arr, axis=0))\n",
    "wpli_non_inhibit_alpha_avg = np.mean(np.array(con_arrs), axis=0)\n",
    "\n",
    "# wpli gamma\n",
    "inhibit_files = []\n",
    "non_inhibit_files = []\n",
    "for f_idx, f in enumerate(wpli_gamma_files): \n",
    "    if lb_list_unique[f_idx] == 0:\n",
    "        non_inhibit_files.append(f)\n",
    "    if lb_list_unique[f_idx] == 1:\n",
    "        inhibit_files.append(f)\n",
    "\n",
    "con_arrs = []\n",
    "for arr_file in inhibit_files:\n",
    "    con_arr = np.load(arr_file, allow_pickle=True)\n",
    "    con_arrs.append(np.mean(con_arr, axis=0))\n",
    "wpli_inhibit_gamma_avg = np.mean(np.array(con_arrs), axis=0)\n",
    "con_arrs = []\n",
    "for arr_file in non_inhibit_files:\n",
    "    con_arr = np.load(arr_file, allow_pickle=True)\n",
    "    con_arrs.append(np.mean(con_arr, axis=0))\n",
    "wpli_non_inhibit_gamma_avg = np.mean(np.array(con_arrs), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical check of connectivity (first load and reshape data)\n",
    "inhibit_files = []\n",
    "non_inhibit_files = []\n",
    "for f_idx, f in enumerate(coh_gamma_files): \n",
    "    if lb_list_unique[f_idx] == 0:\n",
    "        non_inhibit_files.append(f)\n",
    "    if lb_list_unique[f_idx] == 1:\n",
    "        inhibit_files.append(f)\n",
    "\n",
    "con_coh_inhibit_alpha = []\n",
    "for arr_file in inhibit_files:\n",
    "    con_arr = np.load(arr_file, allow_pickle=True)\n",
    "    con_coh_inhibit_alpha.append(np.mean(con_arr, axis=0))\n",
    "coh_inhibit_alpha_avg = np.mean(np.array(con_coh_inhibit_alpha), axis=0)\n",
    "\n",
    "con_coh_non_inhibit_alpha = []\n",
    "for arr_file in non_inhibit_files:\n",
    "    con_arr = np.load(arr_file, allow_pickle=True)\n",
    "    con_coh_non_inhibit_alpha.append(np.mean(con_arr, axis=0))\n",
    "coh_non_inhibit_alpha_avg = np.mean(np.array(con_coh_non_inhibit_alpha), axis=0)\n",
    "\n",
    "con_coh_inhibit_alpha_reshaped = np.transpose(np.array(con_coh_inhibit_alpha), axes=[1,2,0])\n",
    "con_coh_non_inhibit_alpha_reshaped = np.transpose(np.array(con_coh_non_inhibit_alpha), axes=[1,2,0])\n",
    "\n",
    "# second step check them in zip format\n",
    "\n",
    "imp_brain_label_list = ['parahippocampal-lh', 'parahippocampal-rh', 'paracentral-lh', 'paracentral-rh',\n",
    "                        'temporalpole-lh', 'temporalpole-rh', 'superiorfrontal-lh', 'superiorfrontal-rh',\n",
    "                        'superiorparietal-lh', 'superiorparietal-rh', 'transversetemporal-lh', 'transversetemporal-rh',\n",
    "                        'rostralanteriorcingulate-lh', 'rostralanteriorcingulate-rh', 'insula-lh', 'insula-rh',\n",
    "                        'posteriorcingulate-lh', 'posteriorcingulate-rh', 'precentral-lh', 'precentral-rh',\n",
    "                        'middletemporal-lh', 'middletemporal-rh', 'frontalpole-lh', 'frontalpole-rh',\n",
    "                        'supramarginal-lh', 'supramarginal-rh'] \n",
    "\n",
    "\n",
    "p_thr = 0.1\n",
    "for idx_i, idx_j in itertools.product(range(len(imp_brain_label_list)), range(len(imp_brain_label_list))):\n",
    "    array_1 = con_coh_inhibit_alpha_reshaped[idx_i][idx_j]\n",
    "    array_2 = con_coh_non_inhibit_alpha_reshaped[idx_i][idx_j]\n",
    "    t_statistic, p_value = scipy.stats.ttest_ind(array_1, array_2, permutations=1000)\n",
    "    if p_value < p_thr:\n",
    "        print(f\"significant difference between inhibition/non-inhibition within: {imp_brain_label_list[idx_i]} and {imp_brain_label_list[idx_j]} with p-value: {p_value}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting circles\n",
    "node_names = ['parahippocampal-lh', 'transversetemporal-rh', 'pericalcarine-lh', 'temporalpole-rh',\n",
    "                        'medialorbitofrontal-rh', 'lateraloccipital-rh', 'inferiortemporal-lh', 'superiortemporal-rh',\n",
    "                        'supramarginal-rh', 'frontalpole-lh', 'parsopercularis-rh', 'posteriorcingulate-rh',\n",
    "                        'superiorparietal-rh', 'parahippocampal-rh', 'rostralmiddlefrontal-lh',\n",
    "                        'caudalmiddlefrontal-rh', 'parstriangularis-rh', 'lateraloccipital-lh', 'postcentral-lh'][:10]\n",
    "# choose one of the following:\n",
    "# coh_inhibit_alpha_avg, coh_non_inhibit_alpha_avg, coh_inhibit_gamma_avg, coh_non_inhibit_gamma_avg\n",
    "# wpli_inhibit_alpha_avg, wpli_non_inhibit_alpha_avg, wpli_inhibit_gamma_avg, wpli_non_inhibit_gamma_avg\n",
    "target = coh_inhibit_alpha_avg - coh_non_inhibit_alpha_avg\n",
    "\n",
    "x = np.delete(target, 15, axis=0) # removing inferiortemporal-lh\n",
    "target = np.delete(x, 15, axis=1)\n",
    "#plot_connectivity_circle(target, node_names=node_names, n_lines=20,\n",
    "#                        textcolor='black', node_edgecolor='white', colormap='viridis', fontsize_names=9,\n",
    "#                        title=None, facecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reod_node_names = ['temporalpole-rh', 'superiortemporal-rh', 'supramarginal-rh', 'lateraloccipital-rh',\n",
    "                    'caudalmiddlefrontal-rh', 'parsopercularis-rh', 'frontalpole-lh', 'rostralmiddlefrontal-lh',\n",
    "                    'lateraloccipital-lh', 'pericalcarine-lh', 'postcentral-lh', 'inferiortemporal-lh', \n",
    "                    'parahippocampal-lh', 'parstriangularis-rh', 'transversetemporal-rh', 'superiorparietal-rh',\n",
    "                    'posteriorcingulate-rh', 'parahippocampal-rh', 'medialorbitofrontal-rh'][:10] # added to separate alpha and gamma\n",
    "\n",
    "new_array = np.zeros((10,10)) # 19 * 19\n",
    "for new_id_1 in reod_node_names:\n",
    "    for new_id_2 in reod_node_names:\n",
    "        old_idx_1 = node_names.index(new_id_1)\n",
    "        old_idx_2 = node_names.index(new_id_2)\n",
    "        if old_idx_1 > old_idx_2:\n",
    "            new_idx_1 = reod_node_names.index(new_id_1)\n",
    "            new_idx_2 = reod_node_names.index(new_id_2)\n",
    "            i = max(new_idx_1, new_idx_2)\n",
    "            j = min(new_idx_1, new_idx_2)\n",
    "            new_array[i][j] = target[old_idx_1][old_idx_2]\n",
    "\n",
    "d_r = 180 / 13 # degree\n",
    "d_l = 180 / 8\n",
    "hd = d_r / 2 # half degree\n",
    "node_angles = np.array([hd, 1*d_r+hd, 2*d_r+hd, 3*d_r+hd, 4*d_r+hd, 5*d_r+hd,\n",
    "                        1*d_l+90, 2*d_l+90, 3*d_l+90, 4*d_l+90, 5*d_l+90, 6*d_l+90, 7*d_l+90,\n",
    "                        -hd-5*d_r, -hd-4*d_r, -hd-3*d_r, -hd-2*d_r, -hd-1*d_r, -hd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_connectivity_circle(new_array, node_names=reod_node_names, n_lines=10, vmin=0.0, vmax=0.1,\n",
    "                        textcolor='black', node_edgecolor='white', colormap='hot', fontsize_names=9, node_angles=node_angles,\n",
    "                        title=None, facecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new with symmetric lh/rh\n",
    "node_names = ['parahippocampal-lh', 'parahippocampal-rh', 'transversetemporal-lh', 'transversetemporal-rh',\n",
    "                        'pericalcarine-lh', 'pericalcarine-rh', 'temporalpole-lh', 'temporalpole-rh',\n",
    "                        'medialorbitofrontal-lh', 'medialorbitofrontal-rh', 'lateraloccipital-lh', 'lateraloccipital-rh',\n",
    "                        'inferiortemporal-lh', 'inferiortemporal-rh', 'superiortemporal-lh', 'superiortemporal-rh',\n",
    "                        'supramarginal-lh', 'supramarginal-rh', 'frontalpole-lh', 'frontalpole-rh', 'parsopercularis-lh', 'parsopercularis-rh',\n",
    "                        'posteriorcingulate-lh', 'posteriorcingulate-rh', 'superiorparietal-lh', 'superiorparietal-rh',\n",
    "                        'rostralmiddlefrontal-lh', 'rostralmiddlefrontal-rh', 'inferiortemporal-lh', 'inferiortemporal-rh',\n",
    "                        'caudalmiddlefrontal-lh', 'caudalmiddlefrontal-rh', 'parstriangularis-lh', 'parstriangularis-rh',\n",
    "                        'lateraloccipital-lh', 'lateraloccipital-rh', 'postcentral-lh', 'postcentral-rh']\n",
    "\n",
    "target = coh_inhibit_alpha_avg - coh_non_inhibit_alpha_avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature explanation with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(shap_values).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for connectivity\n",
    "shap.summary_plot(shap_values[1], features=X_test, feature_names=None,\n",
    "                    max_display=20, alpha=0.1, cmap='coolwarm') # barplots\n",
    "# selected_features_idx[[96, 98, 99, 95, 94, 97, 92]] - 556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the connection corresponding to this index\n",
    "imp_brain_label_list = ['parahippocampal-lh', 'parahippocampal-rh', 'paracentral-lh', 'paracentral-rh',\n",
    "                        'temporalpole-lh', 'temporalpole-rh', 'superiorfrontal-lh', 'superiorfrontal-rh',\n",
    "                        'superiorparietal-lh', 'superiorparietal-rh', 'transversetemporal-lh', 'transversetemporal-rh',\n",
    "                        'rostralanteriorcingulate-lh', 'rostralanteriorcingulate-rh', 'insula-lh', 'insula-rh',\n",
    "                        'posteriorcingulate-lh', 'posteriorcingulate-rh', 'precentral-lh', 'precentral-rh',\n",
    "                        'middletemporal-lh', 'middletemporal-rh', 'frontalpole-lh', 'frontalpole-rh',\n",
    "                        'supramarginal-lh', 'supramarginal-rh'] \n",
    "\n",
    "imp_brain_label_list = ['vsn', 'smn', 'aun', 'van', 'dan', 'fpn', 'dmn', 'dgn', 'lbn']\n",
    "\n",
    "texts = []\n",
    "for index, imp in zip(np.array(keep_idxs)[selected_features_idx], np.sort(importance)[-n_sel_features:][:]):\n",
    "    counter = 0\n",
    "    if index - 36 >= 0:\n",
    "        clas = 'gamma'\n",
    "        new_index = index - 36\n",
    "    else:\n",
    "        clas = 'alpha'  \n",
    "        new_index = index  \n",
    "    for i, j in itertools.product(range(len(imp_brain_label_list)), range(len(imp_brain_label_list))):\n",
    "        if j < i:\n",
    "            counter += 1\n",
    "            if counter == new_index:\n",
    "                texts.append(f'{imp_brain_label_list[i]} and {imp_brain_label_list[j]} in {clas} with imp {imp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/Users/payamsadeghishabestari/Prime/imp_connections_teas_300.npy', np.array(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing the 10 most important indexes for each class of feature\n",
    "d_p_idxs = []; t_p_idxs = []; a_p_idxs = []; b_p_idxs = []; g_p_idxs = []\n",
    "d_e_idxs = []; t_e_idxs = []; a_e_idxs = []; b_e_idxs = []; g_e_idxs = []; ap_of_idxs = []; ap_sl_idxs = []\n",
    "for idx, i in enumerate(np.array(keep_idxs)[selected_features_idx]):\n",
    "    if 0*64 <= i < 1*64:\n",
    "        d_p_idxs.append(idx)\n",
    "    if 1*64 <= i < 2*64:\n",
    "        t_p_idxs.append(idx)\n",
    "    if 2*64 <= i < 3*64:\n",
    "        a_p_idxs.append(idx)\n",
    "    if 3*64 <= i < 4*64:\n",
    "        b_p_idxs.append(idx)\n",
    "    if 4*64 <= i < 5*64:\n",
    "        g_p_idxs.append(idx)\n",
    "    if 5*64 <= i < 6*64:\n",
    "        d_e_idxs.append(idx)\n",
    "    if 6*64 <= i < 7*64:\n",
    "        t_e_idxs.append(idx)\n",
    "    if 7*64 <= i < 8*64:\n",
    "        a_e_idxs.append(idx)\n",
    "    if 8*64 <= i < 9*64:\n",
    "        b_e_idxs.append(idx)\n",
    "    if 9*64 <= i < 10*64:\n",
    "        g_e_idxs.append(idx)\n",
    "    if 10*64 <= i < 11*64:\n",
    "        ap_of_idxs.append(idx)\n",
    "    if 11*64 <= i < 12*64:\n",
    "        ap_sl_idxs.append(idx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sum_d_p = np.sum(X_test[:,d_p_idxs], axis=1)[:,np.newaxis]\n",
    "feature_sum_t_p = np.sum(X_test[:,t_p_idxs], axis=1)[:,np.newaxis]\n",
    "feature_sum_a_p = np.sum(X_test[:,a_p_idxs], axis=1)[:,np.newaxis]\n",
    "feature_sum_b_p = np.sum(X_test[:,b_p_idxs], axis=1)[:,np.newaxis]\n",
    "feature_sum_g_p = np.sum(X_test[:,g_p_idxs], axis=1)[:,np.newaxis]\n",
    "feature_sum_d_e = np.sum(X_test[:,d_e_idxs], axis=1)[:,np.newaxis]\n",
    "feature_sum_t_e = np.sum(X_test[:,t_e_idxs], axis=1)[:,np.newaxis]\n",
    "feature_sum_a_e = np.sum(X_test[:,a_e_idxs], axis=1)[:,np.newaxis]\n",
    "feature_sum_b_e = np.sum(X_test[:,b_e_idxs], axis=1)[:,np.newaxis]\n",
    "feature_sum_g_e = np.sum(X_test[:,g_e_idxs], axis=1)[:,np.newaxis]\n",
    "feature_sum_ap_of = np.sum(X_test[:,ap_of_idxs], axis=1)[:,np.newaxis]\n",
    "feature_sum_ap_sl = np.sum(X_test[:,ap_sl_idxs], axis=1)[:,np.newaxis]\n",
    "X_test_new = np.concatenate((feature_sum_d_p, feature_sum_t_p, feature_sum_a_p, feature_sum_b_p, feature_sum_g_p,\n",
    "                            feature_sum_d_e, feature_sum_t_e, feature_sum_a_e, feature_sum_b_e, feature_sum_g_e,\n",
    "                            feature_sum_ap_of, feature_sum_ap_sl),axis=1)\n",
    "\n",
    "shap_d_p = np.sum(np.array(shap_values)[:,:,d_p_idxs], axis=2)[:,:,np.newaxis]\n",
    "shap_t_p = np.sum(np.array(shap_values)[:,:,t_p_idxs], axis=2)[:,:,np.newaxis]\n",
    "shap_a_p = np.sum(np.array(shap_values)[:,:,a_p_idxs], axis=2)[:,:,np.newaxis]\n",
    "shap_b_p = np.sum(np.array(shap_values)[:,:,b_p_idxs], axis=2)[:,:,np.newaxis]\n",
    "shap_g_p = np.sum(np.array(shap_values)[:,:,g_p_idxs], axis=2)[:,:,np.newaxis]\n",
    "shap_d_e = np.sum(np.array(shap_values)[:,:,d_e_idxs], axis=2)[:,:,np.newaxis]\n",
    "shap_t_e = np.sum(np.array(shap_values)[:,:,t_e_idxs], axis=2)[:,:,np.newaxis]\n",
    "shap_a_e = np.sum(np.array(shap_values)[:,:,a_e_idxs], axis=2)[:,:,np.newaxis]\n",
    "shap_b_e = np.sum(np.array(shap_values)[:,:,b_e_idxs], axis=2)[:,:,np.newaxis]\n",
    "shap_g_e = np.sum(np.array(shap_values)[:,:,g_e_idxs], axis=2)[:,:,np.newaxis]\n",
    "shap_ap_of = np.sum(np.array(shap_values)[:,:,ap_of_idxs], axis=2)[:,:,np.newaxis]\n",
    "shap_ap_sl = np.sum(np.array(shap_values)[:,:,ap_sl_idxs], axis=2)[:,:,np.newaxis]\n",
    "\n",
    "shap_new = np.concatenate((shap_d_p, shap_t_p, shap_a_p, shap_b_p, shap_g_p,\n",
    "                            shap_d_e, shap_t_e, shap_a_e, shap_b_e, shap_g_e,\n",
    "                            shap_ap_of, shap_ap_sl),axis=2)\n",
    "\n",
    "feature_names = ['Delta power', 'Theta power', 'Alpha power', 'Beta power', 'Gamma power',\n",
    "                    'Delta entropy', 'Theta entropy', 'Alpha entropy', 'Beta entropy', 'Gamma entropy',\n",
    "                    'Aperiodic offset', 'Aperiodic Slope']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_new = np.load('/Users/payamsadeghishabestari/Prime/shap/shap_new.npy',allow_pickle=True)\n",
    "X_test_new = np.load('/Users/payamsadeghishabestari/Prime/shap/shap_features_rescaled_from_100_12.npy',allow_pickle=True)\n",
    "feature_names = ['Delta power', 'Theta power', 'Alpha power', 'Beta power', 'Gamma power',\n",
    "                    'Delta entropy', 'Theta entropy', 'Alpha entropy', 'Beta entropy', 'Gamma entropy',\n",
    "                    'Aperiodic offset', 'Aperiodic Slope']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(list(shap_new), features=X_test_new, feature_names=feature_names,\n",
    "                    max_display=12, cmap='coolwarm', plot_size=(8,5)) # barplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(list(shap_new)[1], features=X_test_new, feature_names=feature_names,\n",
    "                    max_display=12, alpha=0.6, cmap='coolwarm', plot_size=(8,5)) # barplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"Alpha power\", list(shap_new)[1], X_test_new, alpha=0.8,\n",
    "                        feature_names=feature_names, interaction_index=\"Alpha entropy\", cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Gamma power correlation (maybe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB\n",
    "fig, axs = plt.subplots(1,2)\n",
    "epoch_ec = epochs_ec[0]\n",
    "epoch_ec.info['bads'] = ['PO8', 'TP10', 'TP8', 'TP9', 'PO7', 'Fp2', 'Iz', 'FC6', 'CP5', 'TP7']\n",
    "mne.viz.plot_sensors(epoch_ec.info, kind='topomap', pointsize=50, axes=axs[0], sphere=(0, 0.02, 0.01, 0.10))\n",
    "epoch_ec.info['bads'] = ['PO8', 'AF4', 'AF8', 'C5', 'FC5', 'O2', 'TP9', 'F6', 'P8']\n",
    "mne.viz.plot_sensors(epoch_ec.info, kind='topomap', pointsize=50, axes=axs[1], sphere=(0, 0.02, 0.01, 0.10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEAS\n",
    "fig, axs = plt.subplots(1,2)\n",
    "epoch_ec = epochs_ec[0]\n",
    "epoch_ec.info['bads'] = ['CP3', 'CP5', 'P3', 'CP1', 'CP2', 'CPz', 'P2', 'P5', 'POz', 'P1']\n",
    "mne.viz.plot_sensors(epoch_ec.info, kind='topomap', pointsize=50, axes=axs[0], sphere=(0, 0.02, 0.01, 0.10))\n",
    "epoch_ec.info['bads'] = ['TP9', 'CP5', 'POz', 'AF7', 'T7', 'P5', 'FT7', 'TP8', 'F7', 'P7']\n",
    "mne.viz.plot_sensors(epoch_ec.info, kind='topomap', pointsize=50, axes=axs[1], sphere=(0, 0.02, 0.01, 0.10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_dir = fetch_fsaverage(verbose=True)\n",
    "src = op.join(fs_dir, \"bem\", \"fsaverage-ico-5-src.fif\")\n",
    "sensors_colors = []\n",
    "for sensor in epochs_ec[0].info['ch_names']:\n",
    "    if sensor in ['PO8', 'TP10', 'TP8', 'TP9', 'PO7', 'Fp2', 'Iz', 'FC6', 'CP5', 'TP7']:\n",
    "        sensors_colors.append('#ff7f0e')\n",
    "    else:\n",
    "        sensors_colors.append('#1f77b4')\n",
    "fig = mne.viz.create_3d_figure(size=(800,800), bgcolor=(255, 255, 255))\n",
    "fig= mne.viz.plot_alignment(\n",
    "    epochs_ec[0].info, trans=\"fsaverage\", subject=\"fsaverage\", surfaces=dict(brain=0.2, outer_skull=0.2, head=0.1), coord_frame='head',\n",
    "    eeg=[\"original\"], meg=[], src=src, sensor_colors=sensors_colors,\n",
    "    subjects_dir=None, fig=fig)\n",
    "\n",
    "# Set viewing angle\n",
    "mne.viz.set_3d_view(figure=fig, azimuth=135, elevation=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connectivity plot with 20 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_names = ['parahippocampal-lh', 'parahippocampal-rh', 'paracentral-lh', 'paracentral-rh',\n",
    "            'temporalpole-lh', 'temporalpole-rh', 'superiorfrontal-lh', 'superiorfrontal-rh',\n",
    "            'superiorparietal-lh', 'superiorparietal-rh', 'transversetemporal-lh', 'transversetemporal-rh',\n",
    "            'rostralanteriorcingulate-lh', 'rostralanteriorcingulate-rh', 'insula-lh', 'insula-rh',\n",
    "            'posteriorcingulate-lh', 'posteriorcingulate-rh', 'precentral-lh', 'precentral-rh',\n",
    "            'middletemporal-lh', 'middletemporal-rh', 'frontalpole-lh', 'frontalpole-rh',\n",
    "            'supramarginal-lh', 'supramarginal-rh']\n",
    "\n",
    "# choose one of the following:\n",
    "# coh_inhibit_alpha_avg, coh_non_inhibit_alpha_avg, coh_inhibit_gamma_avg, coh_non_inhibit_gamma_avg\n",
    "# wpli_inhibit_alpha_avg, wpli_non_inhibit_alpha_avg, wpli_inhibit_gamma_avg, wpli_non_inhibit_gamma_avg\n",
    "target = coh_inhibit_gamma_avg - coh_non_inhibit_gamma_avg\n",
    "\n",
    "# x = np.delete(target, obj=[10, 11, 12, 13], axis=0) # removing lateraloccipital and inferiortemporal\n",
    "# target = np.delete(x, obj=[10, 11, 12, 13], axis=1)\n",
    "\n",
    "reod_node_names = ['frontalpole-lh', 'frontalpole-rh', 'precentral-lh', 'precentral-rh',\n",
    "                'superiorfrontal-lh', 'superiorfrontal-rh', 'rostralanteriorcingulate-lh', 'rostralanteriorcingulate-rh',\n",
    "                'insula-lh', 'insula-rh', 'transversetemporal-lh', 'transversetemporal-rh',\n",
    "                'middletemporal-lh', 'middletemporal-rh', 'temporalpole-lh', 'temporalpole-rh',\n",
    "                'parahippocampal-lh', 'parahippocampal-rh', 'posteriorcingulate-lh', 'posteriorcingulate-rh',\n",
    "                'superiorparietal-lh', 'superiorparietal-rh', 'supramarginal-lh', 'supramarginal-rh',\n",
    "                'paracentral-lh', 'paracentral-rh'] \n",
    "\n",
    "new_array = np.zeros((len(reod_node_names),len(reod_node_names)))\n",
    "for new_id_1 in reod_node_names:\n",
    "    for new_id_2 in reod_node_names:\n",
    "        old_idx_1 = node_names.index(new_id_1)\n",
    "        old_idx_2 = node_names.index(new_id_2)\n",
    "        if old_idx_1 > old_idx_2:\n",
    "            new_idx_1 = reod_node_names.index(new_id_1)\n",
    "            new_idx_2 = reod_node_names.index(new_id_2)\n",
    "            i = max(new_idx_1, new_idx_2)\n",
    "            j = min(new_idx_1, new_idx_2)\n",
    "            new_array[i][j] = target[old_idx_1][old_idx_2]\n",
    "\n",
    "angle_dist = 180 / ((len(reod_node_names) / 2) + 1)\n",
    "\n",
    "node_angles = np.array([90+1*angle_dist, 90-1*angle_dist, 90+2*angle_dist, 90-2*angle_dist, 90+3*angle_dist, 90-3*angle_dist,\n",
    "                        90+4*angle_dist, 90-4*angle_dist, 90+5*angle_dist, 90-5*angle_dist, 90+6*angle_dist, 90-6*angle_dist,\n",
    "                        90+7*angle_dist, 90-7*angle_dist, 90+8*angle_dist, 90-8*angle_dist, 90+9*angle_dist, 90-9*angle_dist,\n",
    "                        90+10*angle_dist, 90-10*angle_dist, 90+11*angle_dist, 90-11*angle_dist, 90+12*angle_dist, 90-12*angle_dist,\n",
    "                        90+13*angle_dist, 90-13*angle_dist])\n",
    "\n",
    "new_array_pos = np.maximum(new_array, 0)\n",
    "new_array_neg = -np.minimum(new_array, 0)\n",
    "fig, ax = plt.subplots(1,1, figsize=(11,11), subplot_kw={'projection': 'polar'})\n",
    "\n",
    "# plot_connectivity_circle(new_array_neg, node_names=reod_node_names, n_lines=5, vmin=0.1, vmax=-0.1,\n",
    "#                         textcolor='black', node_edgecolor='white', colormap='coolwarm', fontsize_names=13, node_angles=node_angles,\n",
    "#                         title=None, facecolor='white', colorbar=False, linewidth=2, ax=ax)\n",
    "\n",
    "plot_connectivity_circle(new_array_pos, node_names=reod_node_names, n_lines=5, vmin=-0.05, vmax=0.05,\n",
    "                        textcolor='black', node_edgecolor='white', colormap='coolwarm',\n",
    "                        fontsize_names=13, node_angles=node_angles, title=None, facecolor='white',\n",
    "                        colorbar_pos=(0.9, 0.1), linewidth=2.5, ax=ax, node_width=10, node_height=1, node_colors=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping labels\n",
    "node_names = ['parahippocampal-lh', 'parahippocampal-rh', 'paracentral-lh', 'paracentral-rh',\n",
    "            'temporalpole-lh', 'temporalpole-rh', 'superiorfrontal-lh', 'superiorfrontal-rh',\n",
    "            'superiorparietal-lh', 'superiorparietal-rh', 'transversetemporal-lh', 'transversetemporal-rh',\n",
    "            'rostralanteriorcingulate-lh', 'rostralanteriorcingulate-rh', 'insula-lh', 'insula-rh',\n",
    "            'posteriorcingulate-lh', 'posteriorcingulate-rh', 'precentral-lh', 'precentral-rh',\n",
    "            'middletemporal-lh', 'middletemporal-rh', 'frontalpole-lh', 'frontalpole-rh',\n",
    "            'supramarginal-lh', 'supramarginal-rh']\n",
    "\n",
    "# choose one of the following:\n",
    "# coh_inhibit_alpha_avg, coh_non_inhibit_alpha_avg, coh_inhibit_gamma_avg, coh_non_inhibit_gamma_avg\n",
    "# wpli_inhibit_alpha_avg, wpli_non_inhibit_alpha_avg, wpli_inhibit_gamma_avg, wpli_non_inhibit_gamma_avg\n",
    "target = coh_inhibit_gamma_avg - coh_non_inhibit_gamma_avg\n",
    "\n",
    "# x = np.delete(target, obj=[10, 11, 12, 13], axis=0) # removing lateraloccipital and inferiortemporal\n",
    "# target = np.delete(x, obj=[10, 11, 12, 13], axis=1)\n",
    "\n",
    "reod_node_names = ['frontalpole-lh', 'frontalpole-rh', 'precentral-lh', 'precentral-rh', 'superiorfrontal-lh', 'superiorfrontal-rh',\n",
    "                    'transversetemporal-lh', 'transversetemporal-rh', 'middletemporal-lh', 'middletemporal-rh', 'temporalpole-lh', 'temporalpole-rh',\n",
    "                    'supramarginal-lh', 'supramarginal-rh', 'superiorparietal-lh', 'superiorparietal-rh', 'paracentral-lh', 'paracentral-rh',\n",
    "                    'rostralanteriorcingulate-lh', 'rostralanteriorcingulate-rh', 'insula-lh', 'insula-rh',\n",
    "                    'parahippocampal-lh', 'parahippocampal-rh', 'posteriorcingulate-lh', 'posteriorcingulate-rh'] \n",
    "\n",
    "new_array = np.zeros((len(reod_node_names),len(reod_node_names)))\n",
    "for new_id_1 in reod_node_names:\n",
    "    for new_id_2 in reod_node_names:\n",
    "        old_idx_1 = node_names.index(new_id_1)\n",
    "        old_idx_2 = node_names.index(new_id_2)\n",
    "        if old_idx_1 > old_idx_2:\n",
    "            new_idx_1 = reod_node_names.index(new_id_1)\n",
    "            new_idx_2 = reod_node_names.index(new_id_2)\n",
    "            i = max(new_idx_1, new_idx_2)\n",
    "            j = min(new_idx_1, new_idx_2)\n",
    "            new_array[i][j] = target[old_idx_1][old_idx_2]\n",
    "\n",
    "angle_dist = 180 / 20\n",
    "\n",
    "node_angles = np.array([90+1*angle_dist+10, 90-1*angle_dist-10, 90+2*angle_dist+10, 90-2*angle_dist-10, 90+3*angle_dist+10, 90-3*angle_dist-10,\n",
    "                        90+6*angle_dist+5, 90-6*angle_dist-5, 90+7*angle_dist+5, 90-7*angle_dist-5, 90+8*angle_dist+5, 90-8*angle_dist-5,\n",
    "                        90+11*angle_dist, 90-11*angle_dist, 90+12*angle_dist, 90-12*angle_dist, 90+13*angle_dist, 90-13*angle_dist,\n",
    "                        90+16*angle_dist-10, 90-16*angle_dist+10, 90+17*angle_dist-10, 90-17*angle_dist+10, 90+18*angle_dist-10, 90-18*angle_dist+10,\n",
    "                        90+19*angle_dist-10, 90-19*angle_dist+10])\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap('Set1')\n",
    "\n",
    "node_colors_one_side = np.concatenate((np.array(cmap.colors[0])[:, np.newaxis], np.array(cmap.colors[0])[:, np.newaxis], np.array(cmap.colors[0])[:, np.newaxis],\n",
    "                np.array(cmap.colors[1])[:, np.newaxis], np.array(cmap.colors[1])[:, np.newaxis], np.array(cmap.colors[1])[:, np.newaxis],\n",
    "                np.array(cmap.colors[2])[:, np.newaxis], np.array(cmap.colors[2])[:, np.newaxis], np.array(cmap.colors[2])[:, np.newaxis],\n",
    "                np.array(cmap.colors[3])[:, np.newaxis], np.array(cmap.colors[3])[:, np.newaxis],\n",
    "                np.array(cmap.colors[3])[:, np.newaxis], np.array(cmap.colors[3])[:, np.newaxis],), axis=1)\n",
    "\n",
    "node_colors = []\n",
    "for line in node_colors_one_side.T:\n",
    "    for _ in range(2):\n",
    "        node_colors.append(line)\n",
    "node_colors = np.array(node_colors)\n",
    "\n",
    "new_array_pos = np.maximum(new_array, 0)\n",
    "new_array_neg = -np.minimum(new_array, 0)\n",
    "fig, ax = plt.subplots(1,1, figsize=(7,7), subplot_kw={'projection': 'polar'})\n",
    "\n",
    "# plot_connectivity_circle(new_array_neg, node_names=reod_node_names, n_lines=5, vmin=0.1, vmax=-0.1,\n",
    "#                         textcolor='black', node_edgecolor='white', colormap='coolwarm', fontsize_names=13, node_angles=node_angles,\n",
    "#                         title=None, facecolor='white', colorbar=False, linewidth=2, ax=ax)\n",
    "\n",
    "plot_connectivity_circle(new_array_pos, node_names=reod_node_names, n_lines=5, vmin=-0.05, vmax=0.05,\n",
    "                        textcolor='black', node_edgecolor='white', colormap='coolwarm',\n",
    "                        fontsize_names=10, node_angles=node_angles, title=None, facecolor='white',\n",
    "                        colorbar_pos=(0.9, 0.1), linewidth=2.5, ax=ax, node_width=None, node_height=1, node_colors=node_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check label matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match number between RGB and Teas (alpha)\n",
    "teas_alpha_names = ['lateralorbitofrontal-lh', 'supramarginal-lh', 'lingual-lh', 'pericalcarine-rh', 'entorhinal-rh', 'transversetemporal-rh', 'transversetemporal-lh', 'precentral-rh', 'parsopercularis-lh', 'bankssts-lh', 'inferiorparietal-rh', 'superiorfrontal-lh', 'temporalpole-lh', 'parahippocampal-lh', 'rostralmiddlefrontal-lh', 'postcentral-lh', 'posteriorcingulate-rh', 'parstriangularis-lh', 'supramarginal-rh', 'superiorparietal-rh', 'superiortemporal-rh', 'lateraloccipital-lh', 'rostralanteriorcingulate-lh', 'caudalmiddlefrontal-lh']\n",
    "RGB_alpha_names = ['temporalpole-rh', 'supramarginal-rh', 'inferiortemporal-lh', 'postcentral-lh', 'lingual-rh', 'lateraloccipital-rh', 'precentral-rh', 'parahippocampal-lh', 'rostralmiddlefrontal-lh', 'caudalmiddlefrontal-rh', 'posteriorcingulate-rh', 'parahippocampal-rh', 'parstriangularis-rh', 'lateraloccipital-lh', 'middletemporal-rh', 'superiorparietal-rh', 'parsopercularis-rh']\n",
    "teas_gamma_names = ['inferiortemporal-rh', 'superiortemporal-rh', 'parahippocampal-rh', 'pericalcarine-rh', 'middletemporal-rh', 'transversetemporal-rh', 'superiorfrontal-rh', 'precentral-rh', 'entorhinal-rh', 'postcentral-rh', 'lingual-lh', 'bankssts-rh', 'transversetemporal-lh', 'parahippocampal-lh', 'rostralmiddlefrontal-lh', 'frontalpole-lh', 'middletemporal-lh', 'isthmuscingulate-rh', 'supramarginal-lh', 'lateraloccipital-lh', 'cuneus-lh', 'pericalcarine-lh', 'precentral-lh', 'lingual-rh', 'lateraloccipital-rh', 'insula-lh', 'rostralanteriorcingulate-lh', 'supramarginal-rh', 'superiortemporal-lh', 'cuneus-rh', 'superiorparietal-lh', 'caudalmiddlefrontal-lh', 'temporalpole-rh', 'inferiorparietal-rh', 'bankssts-lh', 'temporalpole-lh', 'parsopercularis-lh']\n",
    "RGB_gamma_names = ['parsorbitalis-lh', 'rostralmiddlefrontal-lh', 'superiorfrontal-lh', 'inferiortemporal-rh', 'superiortemporal-lh', 'cuneus-rh', 'temporalpole-lh', 'posteriorcingulate-rh', 'inferiorparietal-rh', 'parahippocampal-rh', 'frontalpole-rh', 'superiorparietal-rh', 'parsorbitalis-rh', 'parstriangularis-lh', 'middletemporal-lh', 'supramarginal-lh', 'parsopercularis-rh', 'lateraloccipital-lh', 'rostralanteriorcingulate-rh', 'pericalcarine-lh', 'rostralmiddlefrontal-rh', 'superiortemporal-rh', 'parsopercularis-lh', 'medialorbitofrontal-rh', 'transversetemporal-lh', 'superiorfrontal-rh', 'lateraloccipital-rh', 'supramarginal-rh', 'superiorparietal-lh', 'transversetemporal-rh', 'pericalcarine-rh', 'frontalpole-lh', 'temporalpole-rh', 'inferiortemporal-lh', 'parahippocampal-lh']\n",
    "\n",
    "counter_in = 0\n",
    "counter_out = 0\n",
    "for name in RGB_gamma_names[-10:]:\n",
    "    if name in teas_gamma_names:\n",
    "        counter_in += 1\n",
    "    else:\n",
    "        counter_out += 1\n",
    "        print(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Correlation between high gamma power and other params (e.g HL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/Users/payamsadeghishabestari/Prime/PRIME_beh-data_complete.xlsx')\n",
    "\n",
    "# sum gamma power for each epoch over 10 labels\n",
    "features_array = np.load(file='/Users/payamsadeghishabestari/eeg_data/Regensburg/power_brain_features_rgb_mean.npy', allow_pickle=True)\n",
    "brain_labels = mne.read_labels_from_annot(subject='fsaverage', parc='aparc')[:-1] # 69 labels\n",
    "bl_idxs = range(len(brain_labels)) # 10\n",
    "gammma_power_bl_idxs = 4 * 68 + np.array(bl_idxs)\n",
    "summed_gamma = np.sum(features_array[:,gammma_power_bl_idxs], axis=1)\n",
    "\n",
    "# which epoch is for which subject?\n",
    "subject_ep_lens = []\n",
    "for subj_file, subj_ep in zip(files_list_ec, epochs_ec): \n",
    "    subject_id = subj_file[-15:-11]\n",
    "    subject_ep_lens.append(len(subj_ep))\n",
    "\n",
    "def split_array(arr, lengths):\n",
    "    if sum(lengths) != len(arr):\n",
    "        raise ValueError(\"Sum of lengths must equal the length of the input array\")\n",
    "    split_arrays = []\n",
    "    start = 0\n",
    "    for length in lengths:\n",
    "        end = start + length\n",
    "        split_arrays.append(arr[start:end])\n",
    "        start = end\n",
    "    return split_arrays\n",
    "\n",
    "result = split_array(summed_gamma, subject_ep_lens)\n",
    "gamma_powers = [np.mean(i) for i in result]\n",
    "df['gamma_powers'] = gamma_powers\n",
    "\n",
    "# choose only gamma values larger than threshold\n",
    "gamma_thr = np.median(gamma_powers) \n",
    "sel_df = df[df['gamma_powers'] > gamma_thr]\n",
    "print(len(sel_df))\n",
    "\n",
    "new_df_1 = sel_df[sel_df['study']=='AAEEG']\n",
    "new_df_1['Binary suppresion'] = (new_df_1['WN_TL_0'] <= 90).astype(int)\n",
    "new_df_2 = sel_df[sel_df['study']=='neuropren']\n",
    "new_df_2['Binary suppresion'] = (new_df_2['b1_WN_0'] <= 90).astype(int)\n",
    "new_sel_df = pd.concat([new_df_1, new_df_2], axis=0)\n",
    "\n",
    "correlation = new_sel_df['HA'].corr(new_sel_df['Binary suppresion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = {}\n",
    "chi_values = {}\n",
    "for item in ['age', 'sex', 'THI', 'HL', 'tin_duration', 'tin_frequency', 'MML', 'tin_loud', 'HA']:\n",
    "    contingency_table = pd.crosstab(new_sel_df[item], new_sel_df['Binary suppresion'])\n",
    "    chi2, p, _, _ = chi2_contingency(contingency_table, correction=False) # or True\n",
    "    p_values[item] = p\n",
    "    chi_values[item] = chi2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(3,4))\n",
    "sns.violinplot(data=new_sel_df, y='HA', x='Binary suppresion', bw=.3, saturation=0.3, palette='Set2', inner=None, ax=ax)\n",
    "sns.swarmplot(data=new_sel_df, y='HA', x='Binary suppresion', dodge=False, size=4, palette='Set2', ax=ax)\n",
    "ax.set_ylim([-10, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_in = 0\n",
    "counter_out = 0\n",
    "for name in rgb_list_alpha[:10]:\n",
    "    if name[:-2] in [t[:-2] for t in tinnoice_list_alpha]:\n",
    "        counter_in += 1\n",
    "    else:\n",
    "        counter_out += 1\n",
    "        # print(name)\n",
    "print(f'number of matches:{counter_in}')\n",
    "print(f'number of mismatches:{counter_out}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check connectivity in teas and tinnoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading labeling\n",
    "coh_alpha_files = []; coh_gamma_files = []\n",
    "lb_list_unique = []\n",
    "for label in label_list:\n",
    "    lb_list_unique.append(label[0])\n",
    "\n",
    "# loading and sorting files\n",
    "folder_path = '/Users/payamsadeghishabestari/eeg_data/Regensburg/connectivity/both_hemi/Tinnoice'  \n",
    "if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "    files_in_folder = sorted(os.listdir(folder_path), key=lambda x: os.path.getmtime(os.path.join(folder_path, x)))\n",
    "    for filename in files_in_folder:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path) and file_path.endswith(\".npy\"):\n",
    "            if 'coh_alpha' in file_path:\n",
    "                coh_alpha_files.append(file_path)\n",
    "            if 'coh_gamma' in file_path:\n",
    "                coh_gamma_files.append(file_path)\n",
    "\n",
    "# coherence alpha\n",
    "inhibit_files = []\n",
    "non_inhibit_files = []\n",
    "for f_idx, f in enumerate(coh_alpha_files): \n",
    "    if lb_list_unique[f_idx] == 0:\n",
    "        non_inhibit_files.append(f)\n",
    "    if lb_list_unique[f_idx] == 1:\n",
    "        inhibit_files.append(f)\n",
    "\n",
    "con_arrs = []\n",
    "for arr_file in inhibit_files:\n",
    "    con_arr = np.load(arr_file, allow_pickle=True)\n",
    "    con_arrs.append(np.mean(con_arr, axis=0))\n",
    "coh_inhibit_alpha_avg = np.mean(np.array(con_arrs), axis=0)\n",
    "con_arrs = []\n",
    "for arr_file in non_inhibit_files:\n",
    "    con_arr = np.load(arr_file, allow_pickle=True)\n",
    "    con_arrs.append(np.mean(con_arr, axis=0))\n",
    "coh_non_inhibit_alpha_avg = np.mean(np.array(con_arrs), axis=0)\n",
    "\n",
    "# coherence gamma\n",
    "inhibit_files = []\n",
    "non_inhibit_files = []\n",
    "for f_idx, f in enumerate(coh_gamma_files): \n",
    "    if lb_list_unique[f_idx] == 0:\n",
    "        non_inhibit_files.append(f)\n",
    "    if lb_list_unique[f_idx] == 1:\n",
    "        inhibit_files.append(f)\n",
    "\n",
    "con_arrs = []\n",
    "for arr_file in inhibit_files:\n",
    "    con_arr = np.load(arr_file, allow_pickle=True)\n",
    "    con_arrs.append(np.mean(con_arr, axis=0))\n",
    "coh_inhibit_gamma_avg = np.mean(np.array(con_arrs), axis=0)\n",
    "con_arrs = []\n",
    "for arr_file in non_inhibit_files:\n",
    "    con_arr = np.load(arr_file, allow_pickle=True)\n",
    "    con_arrs.append(np.mean(con_arr, axis=0))\n",
    "coh_non_inhibit_gamma_avg = np.mean(np.array(con_arrs), axis=0)\n",
    "\n",
    "# statistical check of connectivity (first load and reshape data)\n",
    "inhibit_files = []\n",
    "non_inhibit_files = []\n",
    "for f_idx, f in enumerate(coh_alpha_files): \n",
    "    if lb_list_unique[f_idx] == 0:\n",
    "        non_inhibit_files.append(f)\n",
    "    if lb_list_unique[f_idx] == 1:\n",
    "        inhibit_files.append(f)\n",
    "\n",
    "con_coh_inhibit_alpha = []\n",
    "for arr_file in inhibit_files:\n",
    "    con_arr = np.load(arr_file, allow_pickle=True)\n",
    "    con_coh_inhibit_alpha.append(np.mean(con_arr, axis=0))\n",
    "coh_inhibit_alpha_avg = np.mean(np.array(con_coh_inhibit_alpha), axis=0)\n",
    "\n",
    "con_coh_non_inhibit_alpha = []\n",
    "for arr_file in non_inhibit_files:\n",
    "    con_arr = np.load(arr_file, allow_pickle=True)\n",
    "    con_coh_non_inhibit_alpha.append(np.mean(con_arr, axis=0))\n",
    "coh_non_inhibit_alpha_avg = np.mean(np.array(con_coh_non_inhibit_alpha), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEAS\n",
    "\n",
    "con_coh_inhibit_alpha_reshaped = np.transpose(np.array(con_coh_inhibit_alpha), axes=[1,2,0])\n",
    "con_coh_non_inhibit_alpha_reshaped = np.transpose(np.array(con_coh_non_inhibit_alpha), axes=[1,2,0])\n",
    "\n",
    "# second step check them in zip format\n",
    "\n",
    "brain_labels = mne.read_labels_from_annot(subject='fsaverage', parc='aparc')[:-1] # 69 labels\n",
    "node_names = [lb.name for lb in brain_labels]\n",
    "p_thr = 0.02\n",
    "for idx_i, idx_j in itertools.product(range(68), range(68)):\n",
    "    array_1 = con_coh_inhibit_alpha_reshaped[idx_i][idx_j]\n",
    "    array_2 = con_coh_non_inhibit_alpha_reshaped[idx_i][idx_j]\n",
    "    t_statistic, p_value = scipy.stats.ttest_ind(array_1, array_2, permutations=1000)\n",
    "    if p_value < p_thr:\n",
    "        print(f\"significant difference between inhibition/non-inhibition within: {brain_labels[idx_i].name} and {brain_labels[idx_j].name} with p-value: {p_value}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_array = coh_non_inhibit_alpha_avg - coh_inhibit_alpha_avg\n",
    "new_array_pos = np.maximum(new_array, 0)\n",
    "new_array_neg = -np.minimum(new_array, 0)\n",
    "fig, ax = plt.subplots(1,1, figsize=(11,11), subplot_kw={'projection': 'polar'})\n",
    "\n",
    "# plot_connectivity_circle(new_array_neg, node_names=node_names, n_lines=10, vmin=0.1, vmax=-0.1,\n",
    "#                         textcolor='black', node_edgecolor='white', colormap='coolwarm', fontsize_names=9, node_angles=None,\n",
    "#                         title=None, facecolor='white', colorbar=False, ax=ax)\n",
    "\n",
    "plot_connectivity_circle(new_array_pos, node_names=node_names, n_lines=10, vmin=-0.1, vmax=0.1,\n",
    "                        textcolor='black', node_edgecolor='white', colormap='coolwarm', fontsize_names=9, node_angles=None,\n",
    "                        title=None, facecolor='white', colorbar_pos=(0.9, 0.1), ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making connectivity plots with important connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('/Users/payamsadeghishabestari/Prime/imp_connections_rgb.npy', allow_pickle=True)\n",
    "for item in x:\n",
    "    if 'alpha' in item:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reod_node_names = ['frontalpole-lh', 'frontalpole-rh', 'precentral-lh', 'precentral-rh', 'superiorfrontal-lh', 'superiorfrontal-rh',\n",
    "                    'transversetemporal-lh', 'transversetemporal-rh', 'middletemporal-lh', 'middletemporal-rh', 'temporalpole-lh', 'temporalpole-rh',\n",
    "                    'supramarginal-lh', 'supramarginal-rh', 'superiorparietal-lh', 'superiorparietal-rh', 'paracentral-lh', 'paracentral-rh',\n",
    "                    'rostralanteriorcingulate-lh', 'rostralanteriorcingulate-rh', 'insula-lh', 'insula-rh',\n",
    "                    'parahippocampal-lh', 'parahippocampal-rh', 'posteriorcingulate-lh', 'posteriorcingulate-rh'] \n",
    "\n",
    "new_array = np.zeros((26,26))\n",
    "\n",
    "# alpha\n",
    "# new_array = np.zeros((26,26))\n",
    "# new_array[25][12]= 0.004180300120799728\n",
    "# new_array[25][21]= 0.004051962505621442\n",
    "# new_array[13][10]= 0.003706030012896151\n",
    "# new_array[23][2]= 0.0033120632978689253\n",
    "# new_array[10][9]= 0.003056123686813693\n",
    "# new_array[7][1]= 0.0027807497494880978\n",
    "# new_array[19][4]= 0.0027416196706699213\n",
    "# new_array[23][14]= 0.002682994188305871\n",
    "# new_array[5][2]= 0.0024406396970428875\n",
    "# new_array[22][11]= 0.0024049593114350656\n",
    "\n",
    "# gamma\n",
    "# new_array = np.zeros((26,26))\n",
    "# new_array[23][2]= 0.008337426164523739\n",
    "# new_array[17][16]= 0.006929944791816353\n",
    "# new_array[25][21]= 0.005707022689821889\n",
    "# new_array[19][2]= 0.005145763659568517\n",
    "# new_array[19][4]= 0.0046119613185669\n",
    "# new_array[20][7]= 0.0044442687154322615\n",
    "# new_array[20][13]= 0.004399014207988937\n",
    "# new_array[15][5]= 0.004312046073316636\n",
    "# new_array[5][2]= 0.004205914372212887\n",
    "# new_array[16][15]= 0.004198167838068725\n",
    "\n",
    "\n",
    "\n",
    "new_array *= 100\n",
    "\n",
    "angle_dist = 180 / 20\n",
    "\n",
    "node_angles = np.array([90+1*angle_dist+10, 90-1*angle_dist-10, 90+2*angle_dist+10, 90-2*angle_dist-10, 90+3*angle_dist+10, 90-3*angle_dist-10,\n",
    "                        90+6*angle_dist+5, 90-6*angle_dist-5, 90+7*angle_dist+5, 90-7*angle_dist-5, 90+8*angle_dist+5, 90-8*angle_dist-5,\n",
    "                        90+11*angle_dist, 90-11*angle_dist, 90+12*angle_dist, 90-12*angle_dist, 90+13*angle_dist, 90-13*angle_dist,\n",
    "                        90+16*angle_dist-10, 90-16*angle_dist+10, 90+17*angle_dist-10, 90-17*angle_dist+10, 90+18*angle_dist-10, 90-18*angle_dist+10,\n",
    "                        90+19*angle_dist-10, 90-19*angle_dist+10])\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap('Set1')\n",
    "\n",
    "node_colors_one_side = np.concatenate((np.array(cmap.colors[0])[:, np.newaxis], np.array(cmap.colors[0])[:, np.newaxis], np.array(cmap.colors[0])[:, np.newaxis],\n",
    "                np.array(cmap.colors[1])[:, np.newaxis], np.array(cmap.colors[1])[:, np.newaxis], np.array(cmap.colors[1])[:, np.newaxis],\n",
    "                np.array(cmap.colors[2])[:, np.newaxis], np.array(cmap.colors[2])[:, np.newaxis], np.array(cmap.colors[2])[:, np.newaxis],\n",
    "                np.array(cmap.colors[3])[:, np.newaxis], np.array(cmap.colors[3])[:, np.newaxis],\n",
    "                np.array(cmap.colors[3])[:, np.newaxis], np.array(cmap.colors[3])[:, np.newaxis],), axis=1)\n",
    "\n",
    "node_colors = []\n",
    "for line in node_colors_one_side.T:\n",
    "    for _ in range(2):\n",
    "        node_colors.append(line)\n",
    "node_colors = np.array(node_colors)\n",
    "\n",
    "new_array_pos = np.maximum(new_array, 0)\n",
    "new_array_neg = -np.minimum(new_array, 0)\n",
    "fig, ax = plt.subplots(1,1, figsize=(7,7), subplot_kw={'projection': 'polar'})\n",
    "\n",
    "# plot_connectivity_circle(new_array_neg, node_names=reod_node_names, n_lines=5, vmin=0.1, vmax=-0.1,\n",
    "#                         textcolor='black', node_edgecolor='white', colormap='coolwarm', fontsize_names=13, node_angles=node_angles,\n",
    "#                         title=None, facecolor='white', colorbar=False, linewidth=2, ax=ax)\n",
    "\n",
    "plot_connectivity_circle(new_array_pos, node_names=reod_node_names, n_lines=10, vmin=0, vmax=0.5,\n",
    "                        textcolor='black', node_edgecolor='white', colormap='coolwarm',\n",
    "                        fontsize_names=10, node_angles=node_angles, title=None, facecolor='white',\n",
    "                        colorbar_pos=(0.9, 0.1), linewidth=2.5, ax=ax, node_width=None, node_height=1, node_colors=node_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = np.load('/Users/payamsadeghishabestari/Prime/imp_connections_rgb.npy', allow_pickle=True)\n",
    "g_text = [text for text in texts if 'gamma' in text]\n",
    "g_text[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamma\n",
    "new_array = np.zeros((26,26))\n",
    "new_array[23][2]= 0.008337426164523739\n",
    "new_array[17][16]= 0.006929944791816353\n",
    "new_array[25][21]= 0.005707022689821889\n",
    "new_array[19][2]= 0.005145763659568517\n",
    "new_array[19][4]= 0.0046119613185669\n",
    "new_array[20][7]= 0.0044442687154322615\n",
    "new_array[20][13]= 0.004399014207988937"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labeling with alpha and gamma (brain labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting brain with colored labels\n",
    "# initiating the brain\n",
    "Brain = mne.viz.get_brain_class()\n",
    "brain_kwargs = dict(alpha=0.15, background=\"white\", cortex=\"low_contrast\", size=(800, 600), views='medial')\n",
    "brain_labels = mne.read_labels_from_annot(subject='fsaverage', parc='aparc') # 69 labels\n",
    "imp_brain_label_list_gamma = ['parahippocampal-lh', 'transversetemporal-rh', 'pericalcarine-lh', 'temporalpole-rh',\n",
    "                        'medialorbitofrontal-rh', 'lateraloccipital-rh', 'inferiortemporal-lh', 'superiortemporal-rh',\n",
    "                        'supramarginal-rh', 'frontalpole-lh']\n",
    "imp_brain_label_list_alpha = ['parsopercularis-rh', 'posteriorcingulate-rh', 'superiorparietal-rh', 'parahippocampal-rh',\n",
    "                            'rostralmiddlefrontal-lh', 'inferiortemporal-lh', 'caudalmiddlefrontal-rh', 'parstriangularis-rh',\n",
    "                            'lateraloccipital-lh', 'postcentral-lh']\n",
    "imp_lb_a_rh_idxs = []; imp_lb_a_lh_idxs = []\n",
    "imp_lb_g_rh_idxs = []; imp_lb_g_lh_idxs = []\n",
    "\n",
    "# indexing right/left and alpha/gamma labels\n",
    "for im_l_a, im_l_g in zip(imp_brain_label_list_alpha, imp_brain_label_list_gamma):\n",
    "    for l_idx, l_id in enumerate(brain_labels):\n",
    "        if l_id.name == im_l_a and l_id.name[-2:] == 'rh':\n",
    "            imp_lb_a_rh_idxs.append(l_idx)\n",
    "        if l_id.name == im_l_a and l_id.name[-2:] == 'lh':\n",
    "            imp_lb_a_lh_idxs.append(l_idx)\n",
    "        if l_id.name == im_l_g and l_id.name[-2:] == 'rh':\n",
    "            imp_lb_g_rh_idxs.append(l_idx)\n",
    "        if l_id.name == im_l_g and l_id.name[-2:] == 'lh':\n",
    "            imp_lb_g_lh_idxs.append(l_idx)\n",
    "\n",
    "# plotting\n",
    "brain = Brain(\"fsaverage\", hemi=\"rh\", surf=\"pial\", title='alpha-rh', **brain_kwargs)\n",
    "for label in list(np.array(brain_labels)[imp_lb_a_rh_idxs]): # alpha-rh\n",
    "    brain.add_label(label, hemi=\"rh\", color=\"#1f77b4\", borders=False, alpha=0.5)\n",
    "# brain.save_image(filename='/Users/payamsadeghishabestari/Prime/parcel-medial-alpha-rh.png', mode='rgb')\n",
    "\n",
    "brain = Brain(\"fsaverage\", hemi=\"lh\", surf=\"pial\", title='alpha-lh', **brain_kwargs)\n",
    "for label in list(np.array(brain_labels)[imp_lb_a_lh_idxs]): # alpha-lh\n",
    "    brain.add_label(label, hemi=\"lh\", color=\"#1f77b4\", borders=False, alpha=0.5)\n",
    "# brain.save_image(filename='/Users/payamsadeghishabestari/Prime/parcel-medial-alpha-lh.png', mode='rgb')\n",
    "\n",
    "\n",
    "brain = Brain(\"fsaverage\", hemi=\"rh\", surf=\"pial\", title='gamma-rh', **brain_kwargs)\n",
    "for label in list(np.array(brain_labels)[imp_lb_g_rh_idxs]): # gamma-rh\n",
    "    brain.add_label(label, hemi=\"rh\", color=\"#ff7f0e\", borders=False, alpha=0.5)\n",
    "# brain.save_image(filename='/Users/payamsadeghishabestari/Prime/parcel-medial-gamma-rh.png', mode='rgb')\n",
    "\n",
    "\n",
    "brain = Brain(\"fsaverage\", hemi=\"lh\", surf=\"pial\", title='gamma-lh', **brain_kwargs)\n",
    "for label in list(np.array(brain_labels)[imp_lb_g_lh_idxs]): # gamma-lh\n",
    "    brain.add_label(label, hemi=\"lh\", color=\"#ff7f0e\", borders=False, alpha=0.5)\n",
    "# brain.save_image(filename='/Users/payamsadeghishabestari/Prime/parcel-medial-gamma-lh.png', mode='rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Brain = mne.viz.get_brain_class()\n",
    "clr = 0.85\n",
    "brain_kwargs = dict(alpha=1, background=\"white\", cortex=[(clr, clr, clr), (clr, clr, clr)], size=(800, 600), views='lateral')\n",
    "silhouette_dict = {'color':(0.8, 0.8, 0.8), 'linewidth':1, 'alpha':0.8, 'decimate':0.9}\n",
    "brain = Brain(\"fsaverage\", hemi=\"rh\", surf=\"pial_semi_inflated\", silhouette=silhouette_dict, **brain_kwargs)\n",
    "# brain.save_image(filename='/Users/payamsadeghishabestari/Prime/parcel-lateral-rh.png', mode='rgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare TEAS and RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_con_rgb = np.load('/Users/payamsadeghishabestari/Prime/imp_connections_rgb.npy', allow_pickle=True)\n",
    "imp_con_teas = np.load('/Users/payamsadeghishabestari/Prime/imp_connections_teas_300.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in imp_con_teas:\n",
    "    if 'superiorparietal' in element and 'paracentral' in element and 'gamma' in element:\n",
    "        print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking and plotting Aperiodic component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = 2\n",
    "ep_idx = 0\n",
    "psds, freqs = epochs_ec[ep][ep_idx].compute_psd(method='multitaper', picks='eeg', fmin=0.5, fmax=80, bandwidth=6, verbose=False).get_data(return_freqs=True)\n",
    "fm = FOOOF(max_n_peaks=5)\n",
    "spectrum = np.squeeze(psds)[19] # channel Iz\n",
    "fm.fit(freqs, spectrum, freq_range=[0.5, 80])\n",
    "fm.report(freqs, spectrum, [0.5, 80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(5,5))\n",
    "plot_kwargs = {'color' : 'k', 'alpha' : 0.7, 'lw' : 1.5}\n",
    "fooof.plts.fm.plot_fm(fm, plot_peaks='shade', ax=ax)\n",
    "ax.grid(visible=False, which='minor', axis='y')\n",
    "ax.legend(fontsize=10, frameon=False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_comp = 10**(-9.5404) * 1/(0 + freqs**0.8647)\n",
    "\n",
    "delta_fill = freqs[:9]\n",
    "theta_fill = freqs[8:17]\n",
    "alpha_fill = freqs[16:27]\n",
    "beta_fill = freqs[30:60]\n",
    "gamma_fill = freqs[59:]\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,5))\n",
    "\n",
    "\n",
    "\n",
    "ax.plot(freqs, spectrum, linewidth=0.5, color='grey')\n",
    "ax.plot(freqs, ap_comp, linewidth=2.5, linestyle='--')\n",
    "ax.plot(freqs, 10 ** fm.fooofed_spectrum_, linewidth=2)\n",
    "\n",
    "ax.fill_between(x=freqs[1:10], y1=ap_comp[1:10], y2=10 ** fm.fooofed_spectrum_[1:10], alpha=0.2, color='grey')\n",
    "ax.fill_between(x=freqs[6:28], y1=ap_comp[6:28], y2=10 ** fm.fooofed_spectrum_[6:28], alpha=0.2, color='grey')\n",
    "ax.fill_between(x=freqs[73:110], y1=ap_comp[73:110], y2=10 ** fm.fooofed_spectrum_[73:110], alpha=0.2, color='grey')\n",
    "ax.fill_between(x=freqs[115:134], y1=ap_comp[115:134], y2=10 ** fm.fooofed_spectrum_[115:134], alpha=0.2, color='grey')\n",
    "ax.fill_between(x=freqs[114:159], y1=ap_comp[114:159], y2=10 ** fm.fooofed_spectrum_[114:159], alpha=0.2, color='grey')\n",
    "\n",
    "ax.fill_between(delta_fill, y1=ap_comp[:9], y2=0, alpha=0.5, color='#d62728', label='delta')\n",
    "ax.fill_between(theta_fill, y1=ap_comp[8:17], y2=0, alpha=0.5, color='#2ca02c', label='theta') # theta\n",
    "ax.fill_between(alpha_fill, y1=ap_comp[16:27], y2=0, alpha=0.5, color='#9467bd', label='alpha') # alpha\n",
    "ax.fill_between(beta_fill, y1=ap_comp[30:60], y2=0, alpha=0.5, color='#bcbd22', label='beta') # beta\n",
    "ax.fill_between(gamma_fill, y1=ap_comp[59:], y2=0, alpha=0.5, color='#17becf', label='gamma') # gamma\n",
    "ax.set_yscale('log')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.legend(frameon=False)\n",
    "ax.set_ylim([0, 10**-9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connectivity between networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making equal channel size\n",
    "for ep in epochs_ec:\n",
    "    ep.drop_channels(['HRli', 'HRre'], on_missing='warn')\n",
    "\n",
    "fs_dir = fetch_fsaverage(verbose=False)\n",
    "trans = \"fsaverage\"  \n",
    "src = op.join(fs_dir, \"bem\", \"fsaverage-ico-5-src.fif\")\n",
    "bem = op.join(fs_dir, \"bem\", \"fsaverage-5120-5120-5120-bem-sol.fif\")\n",
    "method = \"dSPM\"\n",
    "snr = 3.0\n",
    "lambda2 = 1.0 / snr**2\n",
    "sfreq = 250\n",
    "counter = 0\n",
    "brain_labels = mne.read_labels_from_annot(subject='fsaverage', parc='aparc')[:-1] # 69 labels\n",
    "bl_names = [bl.name for bl in brain_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_labels = mne.read_labels_from_annot(subject='fsaverage', parc='aparc')[:-1] # 69 labels\n",
    "bl_names = [bl.name for bl in brain_labels]\n",
    "\n",
    "vsn_lbs = ['cuneus-lh', 'cuneus-rh',\n",
    "'lingual-lh', 'lingual-rh',\n",
    "'lateraloccipital-lh', 'lateraloccipital-rh',\n",
    "'pericalcarine-lh', 'pericalcarine-rh'] \n",
    "\n",
    "smn_lbs = ['caudalmiddlefrontal-lh', 'caudalmiddlefrontal-rh',\n",
    "'postcentral-lh', 'postcentral-rh',\n",
    "'precentral-lh', 'precentral-rh',\n",
    "'paracentral-lh', 'paracentral-rh',\n",
    "'transversetemporal-lh', 'transversetemporal-rh']\n",
    "\n",
    "aun_lbs = ['superiorparietal-lh', 'superiorparietal-rh',\n",
    "'superiortemporal-lh', 'superiortemporal-rh',\n",
    "'transversetemporal-lh', 'transversetemporal-rh']\n",
    "\n",
    "van_lbs = ['fusiform-lh', 'fusiform-rh',\n",
    "'inferiorparietal-lh', 'inferiorparietal-rh',\n",
    "'lingual-lh', 'lingual-rh',\n",
    "'lateraloccipital-lh', 'lateraloccipital-rh']\n",
    "\n",
    "dan_lbs = ['caudalmiddlefrontal-lh', 'caudalmiddlefrontal-rh',\n",
    "'lateraloccipital-lh', 'lateraloccipital-rh',\n",
    "'paracentral-lh', 'paracentral-rh',\n",
    "'superiorparietal-lh', 'superiorparietal-rh',\n",
    "'superiortemporal-lh', 'superiortemporal-rh']\n",
    "\n",
    "fpn_lbs = ['lateralorbitofrontal-lh', 'lateralorbitofrontal-rh',\n",
    "'parsopercularis-lh', 'parsopercularis-rh',\n",
    "'parsorbitalis-lh', 'parsorbitalis-rh',\n",
    "'parstriangularis-lh', 'parstriangularis-rh',\n",
    "'rostralmiddlefrontal-lh', 'rostralmiddlefrontal-rh',\n",
    "'superiorfrontal-lh', 'superiorfrontal-rh',\n",
    "'superiortemporal-lh', 'superiortemporal-rh']\n",
    "\n",
    "dmn_lbs = ['caudalanteriorcingulate-lh', 'caudalanteriorcingulate-rh',\n",
    "'entorhinal-lh', 'entorhinal-rh',\n",
    "'frontalpole-lh', 'frontalpole-rh',\n",
    "'isthmuscingulate-lh', 'isthmuscingulate-rh',\n",
    "'medialorbitofrontal-lh', 'medialorbitofrontal-rh',\n",
    "'parahippocampal-lh', 'parahippocampal-rh',\n",
    "'posteriorcingulate-lh', 'posteriorcingulate-rh',\n",
    "'precuneus-lh', 'precuneus-rh',\n",
    "'rostralanteriorcingulate-lh', 'rostralanteriorcingulate-rh']\n",
    "\n",
    "dgn_lbs = ['caudalmiddlefrontal-lh', 'caudalmiddlefrontal-rh',\n",
    "'paracentral-lh', 'paracentral-rh',\n",
    "'postcentral-lh', 'postcentral-rh',\n",
    "'precentral-lh', 'precentral-rh',\n",
    "'superiorparietal-lh', 'superiorparietal-rh',\n",
    "'superiortemporal-lh', 'superiortemporal-rh']\n",
    "\n",
    "lbn_lbs = ['caudalanteriorcingulate-lh', 'caudalanteriorcingulate-rh',\n",
    "'entorhinal-lh', 'entorhinal-rh',\n",
    "'frontalpole-lh', 'frontalpole-rh',\n",
    "'isthmuscingulate-lh', 'isthmuscingulate-rh',\n",
    "'medialorbitofrontal-lh', 'medialorbitofrontal-rh',\n",
    "'parahippocampal-lh', 'parahippocampal-rh',\n",
    "'posteriorcingulate-lh', 'posteriorcingulate-rh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = bl_names.index(vsn_lbs[0])\n",
    "vsn_n = brain_labels[index]\n",
    "for bl in vsn_lbs[1:]:\n",
    "    index = bl_names.index(bl)\n",
    "    vsn_n += brain_labels[index]\n",
    "\n",
    "index = bl_names.index(smn_lbs[0])\n",
    "smn_n = brain_labels[index]\n",
    "for bl in smn_lbs[1:]:\n",
    "    index = bl_names.index(bl)\n",
    "    smn_n += brain_labels[index]\n",
    "\n",
    "index = bl_names.index(aun_lbs[0])\n",
    "aun_n = brain_labels[index]\n",
    "for bl in aun_lbs[1:]:\n",
    "    index = bl_names.index(bl)\n",
    "    aun_n += brain_labels[index]\n",
    "\n",
    "index = bl_names.index(van_lbs[0])\n",
    "van_n = brain_labels[index]\n",
    "for bl in van_lbs[1:]:\n",
    "    index = bl_names.index(bl)\n",
    "    van_n += brain_labels[index]\n",
    "\n",
    "index = bl_names.index(dan_lbs[0])\n",
    "dan_n = brain_labels[index]\n",
    "for bl in dan_lbs[1:]:\n",
    "    index = bl_names.index(bl)\n",
    "    dan_n += brain_labels[index]\n",
    "\n",
    "index = bl_names.index(fpn_lbs[0])\n",
    "fpn_n = brain_labels[index]\n",
    "for bl in fpn_lbs[1:]:\n",
    "    index = bl_names.index(bl)\n",
    "    fpn_n += brain_labels[index]\n",
    "\n",
    "index = bl_names.index(dmn_lbs[0])\n",
    "dmn_n = brain_labels[index]\n",
    "for bl in dmn_lbs[1:]:\n",
    "    index = bl_names.index(bl)\n",
    "    dmn_n += brain_labels[index]\n",
    "\n",
    "index = bl_names.index(dgn_lbs[0])\n",
    "dgn_n = brain_labels[index]\n",
    "for bl in dgn_lbs[1:]:\n",
    "    index = bl_names.index(bl)\n",
    "    dgn_n += brain_labels[index]\n",
    "\n",
    "index = bl_names.index(lbn_lbs[0])\n",
    "lbn_n = brain_labels[index]\n",
    "for bl in lbn_lbs[1:]:\n",
    "    index = bl_names.index(bl)\n",
    "    lbn_n += brain_labels[index]\n",
    "\n",
    "sel_brain_labels = []\n",
    "sel_brain_labels.append(vsn_n)\n",
    "sel_brain_labels.append(smn_n)\n",
    "sel_brain_labels.append(aun_n)\n",
    "sel_brain_labels.append(van_n)\n",
    "sel_brain_labels.append(dan_n)\n",
    "sel_brain_labels.append(fpn_n)\n",
    "sel_brain_labels.append(dmn_n)\n",
    "sel_brain_labels.append(dgn_n)\n",
    "sel_brain_labels.append(lbn_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping over subjects to compute connectivity\n",
    "for ep in tqdm(epochs_ec[:]):\n",
    "    # initiating\n",
    "    ep.set_eeg_reference('average', projection=True)\n",
    "    ep.apply_proj()\n",
    "    fwd = mne.make_forward_solution(ep.info, trans=trans, src=src, bem=bem, eeg=True, mindist=5.0, verbose=False)\n",
    "    noise_cov = mne.make_ad_hoc_cov(ep.info, std=None, verbose=False)\n",
    "    inverse_operator = make_inverse_operator(ep.info, fwd, noise_cov, loose=0.2, depth=0.8, verbose=False)\n",
    "    # computing source estimate over all surface\n",
    "    stc = apply_inverse_epochs(ep, inverse_operator, lambda2=lambda2,\n",
    "                                                method=method, label=None,\n",
    "                                                return_generator=False, verbose=False)\n",
    "    # project it from whole surface to labels\n",
    "    label_ts = mne.extract_label_time_course(stc, sel_brain_labels, inverse_operator['src'], mode='mean',\n",
    "                                            allow_empty=True, return_generator=False, verbose=False)\n",
    "\n",
    "    freqs = np.linspace(8, 13, 5)\n",
    "    con = spectral_connectivity_time(np.array(label_ts), freqs=freqs, method='coh', average=False, sfreq=sfreq,\n",
    "                                    fmin=freqs[0], fmax=freqs[-1], mode='cwt_morlet', n_cycles=7,\n",
    "                                    faverage=True, n_jobs=1, verbose=False)\n",
    "    conmat_coh = con.get_data(output='dense')[:, :, :, 0]\n",
    "    fname_coh = f'/Users/payamsadeghishabestari/eeg_data/Regensburg/connectivity/networks_connectivity/con_coh_alpha_{counter}.npy'\n",
    "    np.save(fname_coh, conmat_coh)\n",
    "    \n",
    "    freqs = np.linspace(30, 80, 5)\n",
    "    con = spectral_connectivity_time(np.array(label_ts), freqs=freqs, method='coh', average=False, sfreq=sfreq,\n",
    "                                    fmin=freqs[0], fmax=freqs[-1], mode='cwt_morlet', n_cycles=7,\n",
    "                                    faverage=True, n_jobs=1, verbose=False)\n",
    "    conmat_coh = con.get_data(output='dense')[:, :, :, 0]\n",
    "    fname_coh = f'/Users/payamsadeghishabestari/eeg_data/Regensburg/connectivity/networks_connectivity/con_coh_gamma_{counter}.npy'\n",
    "    np.save(fname_coh, conmat_coh)\n",
    "    \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the connection corresponding to this index\n",
    "imp_brain_label_list = ['parahippocampal-lh', 'parahippocampal-rh', 'paracentral-lh', 'paracentral-rh',\n",
    "                        'temporalpole-lh', 'temporalpole-rh', 'superiorfrontal-lh', 'superiorfrontal-rh',\n",
    "                        'superiorparietal-lh', 'superiorparietal-rh', 'transversetemporal-lh', 'transversetemporal-rh',\n",
    "                        'rostralanteriorcingulate-lh', 'rostralanteriorcingulate-rh', 'insula-lh', 'insula-rh',\n",
    "                        'posteriorcingulate-lh', 'posteriorcingulate-rh', 'precentral-lh', 'precentral-rh',\n",
    "                        'middletemporal-lh', 'middletemporal-rh', 'frontalpole-lh', 'frontalpole-rh',\n",
    "                        'supramarginal-lh', 'supramarginal-rh'] \n",
    "\n",
    "imp_brain_label_list = ['vsn', 'smn', 'aun', 'van', 'dan', 'fpn', 'dmn', 'dgn', 'lbn']\n",
    "\n",
    "texts = []\n",
    "for index, imp in zip(np.array(keep_idxs)[selected_features_idx], np.sort(importance)[-n_sel_features:][:]):\n",
    "    counter = 0\n",
    "    if index >= 36:\n",
    "        clas = 'gamma'\n",
    "        new_index = index - 36\n",
    "    \n",
    "    if index < 36:\n",
    "        clas = 'alpha'\n",
    "        new_index = index\n",
    "    \n",
    "\n",
    "    for i, j in itertools.product(range(len(imp_brain_label_list)), range(len(imp_brain_label_list))):\n",
    "        if j < i:\n",
    "            # counter += 1\n",
    "            if counter == new_index and clas=='alpha':\n",
    "                texts.append(f'{imp_brain_label_list[i]} and {imp_brain_label_list[j]} in {clas} with imp {imp}')\n",
    "            counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interconnections between networks + Auditory intra network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(features_array_after_corr,\n",
    "                                                                            label_array, group_array,\n",
    "                                                                            test_size=0.3, train_size=0.7,\n",
    "                                                                            shuffle = True, random_state=None)\n",
    "mask_train = np.isnan(X_train)\n",
    "mask_test = np.isnan(X_test)\n",
    "nan_indices_train = np.where(mask_train)\n",
    "nan_indices_test = np.where(mask_test)\n",
    "for i in range(len(nan_indices_train[0])):\n",
    "    X_train[nan_indices_train[0][i]][nan_indices_train[1][i]] = 1\n",
    "for i in range(len(nan_indices_test[0])):\n",
    "    X_test[nan_indices_test[0][i]][nan_indices_test[1][i]] = 1\n",
    "\n",
    "n_scores = cross_val_score(model, X_train, y_train, groups = group_train ,scoring='accuracy', cv=10, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# reporting and features overview\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'accuracy of the model checked with test data without eliminating features: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks = ['vsn', 'smn', 'aun', 'van', 'dan', 'fpn', 'dmn', 'dgn', 'lbn']\n",
    "aud_network = ['superiorparietal-lh', 'superiorparietal-rh',\n",
    "                'superiortemporal-lh', 'superiortemporal-rh',\n",
    "                'transversetemporal-lh', 'transversetemporal-rh']\n",
    "\n",
    "feature_names = []\n",
    "for k_idx in np.array(keep_idxs):\n",
    "    counter = 0\n",
    "    if k_idx < 36: # corresponds to inter networks (alpha)\n",
    "        for i, j in product(networks, networks):\n",
    "            if networks.index(i) < networks.index(j):\n",
    "                if k_idx == counter:\n",
    "                    feature_names.append(f'{i} & {j} in alpha')\n",
    "                counter += 1\n",
    "    \n",
    "    if 36 <= k_idx < 72: # corresponds to inter networks (gamma) \n",
    "        for i, j in product(networks, networks):\n",
    "            if networks.index(i) < networks.index(j):\n",
    "                if k_idx == counter + 36:\n",
    "                    feature_names.append(f'{i} & {j} in gamma')\n",
    "                counter += 1\n",
    "    \n",
    "    if 72 <= k_idx < 87: # corresponds to aud network (alpha)\n",
    "        for i, j in product(aud_network, aud_network):\n",
    "            if aud_network.index(i) < aud_network.index(j):\n",
    "                if k_idx == counter + 72:\n",
    "                    feature_names.append(f'{i} & {j} in alpha')\n",
    "                counter += 1\n",
    "    \n",
    "    if 87 <= k_idx < 102: # corresponds to aud network (gamma)\n",
    "        for i, j in product(aud_network, aud_network):\n",
    "            if aud_network.index(i) < aud_network.index(j):\n",
    "                if k_idx == counter + 87:\n",
    "                    feature_names.append(f'{i} & {j} in gamma')\n",
    "                counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[1], features=X_test, feature_names=feature_names,\n",
    "                    max_display=20, alpha=0.1, cmap='coolwarm') # barplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/Users/payamsadeghishabestari/Prime/shap/network+aun/X_test.npy', np.array(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nilearn\n",
    "from nilearn.plotting import view_connectome, plot_connectome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting shap\n",
    "X_test = np.load('/Users/payamsadeghishabestari/Prime/shap/network+aun/X_test.npy', allow_pickle=True)\n",
    "shap_values = np.load('/Users/payamsadeghishabestari/Prime/shap/network+aun/shap.npy', allow_pickle=True)\n",
    "feature_names = np.load('/Users/payamsadeghishabestari/Prime/shap/network+aun/features_name.npy', allow_pickle=True)\n",
    "\n",
    "shap.summary_plot(shap_values[1], features=X_test, feature_names=feature_names,\n",
    "                    max_display=15, alpha=0.1, cmap='coolwarm', plot_size=(11,15)) # barplots\n",
    "plt.yticks(fontsize=9) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### extracting adjacency matrix in alpha and gamma\n",
    "aud_network = ['superiorparietal-lh', 'superiorparietal-rh',\n",
    "                'superiortemporal-lh', 'superiortemporal-rh',\n",
    "                'transversetemporal-lh', 'transversetemporal-rh']\n",
    "folder_path = '/Users/payamsadeghishabestari/eeg_data/Regensburg/connectivity/within_aud_network'  \n",
    "alpha_files = []; gamma_files = []\n",
    "if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "    files_in_folder = sorted(os.listdir(folder_path), key=lambda x: os.path.getmtime(os.path.join(folder_path, x)))\n",
    "    for filename in files_in_folder:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path) and file_path.endswith(\".npy\"):\n",
    "            if 'alpha' in file_path:\n",
    "                alpha_files.append(file_path)\n",
    "            if 'gamma' in file_path:\n",
    "                gamma_files.append(file_path)\n",
    "\n",
    "alpha_as = []\n",
    "gamma_as = []\n",
    "for file_a, file_g in zip(alpha_files, gamma_files):\n",
    "    alpha_ss = np.load(file_a, allow_pickle=True)\n",
    "    alpha_as.append(alpha_ss.mean(axis=0))\n",
    "    gamma_ss = np.load(file_g, allow_pickle=True)\n",
    "    gamma_as.append(gamma_ss.mean(axis=0))\n",
    "\n",
    "alpha_con_aud = np.array(alpha_as).mean(axis=0)\n",
    "gamma_con_aud = np.array(gamma_as).mean(axis=0)\n",
    "\n",
    "### converting to mni coordinate\n",
    "atlas = 'aparc'\n",
    "labels_atlas = mne.read_labels_from_annot(subject='fsaverage', parc=atlas,\n",
    "                                    subjects_dir=None, verbose=False)[:-1]\n",
    "aud_lbs = []\n",
    "for i in aud_network:\n",
    "    for lb in labels_atlas:\n",
    "        if i == lb.name:\n",
    "            aud_lbs.append(lb)\n",
    "\n",
    "node_coords = []\n",
    "for label in aud_lbs:\n",
    "    if label.hemi == 'lh':\n",
    "        hemi = 0\n",
    "    if label.hemi == 'rh':\n",
    "        hemi = 1\n",
    "    center_vertex = label.center_of_mass(subject='fsaverage', \n",
    "                                        restrict_vertices=False, \n",
    "                                        subjects_dir=None)\n",
    "    mni_pos = mne.vertex_to_mni(center_vertex, hemis=hemi,\n",
    "                                subject='fsaverage', subjects_dir=None)\n",
    "    node_coords.append(mni_pos)\n",
    "node_coords = np.array(node_coords)\n",
    "\n",
    "### visualizing\n",
    "adjacency_matrix1 = alpha_con_aud + alpha_con_aud.T\n",
    "for i, j in product(range(6), range(6)):\n",
    "    if (i, j)==(3, 5) or (i, j)==(1, 2):\n",
    "        print('hello')\n",
    "    else:\n",
    "        adjacency_matrix1[i][j] = 0\n",
    "\n",
    "adjacency_matrix1 = adjacency_matrix1 + adjacency_matrix1.T\n",
    "\n",
    "adjacency_matrix2 = alpha_con_aud + alpha_con_aud.T\n",
    "for i, j in product(range(6), range(6)):\n",
    "    if (i, j)==(0, 3):\n",
    "        print('hello')\n",
    "    else:\n",
    "        adjacency_matrix2[i][j] = 0\n",
    "\n",
    "adjacency_matrix2 = adjacency_matrix2 + adjacency_matrix2.T\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(10, 6))\n",
    "plot_connectome(adjacency_matrix=adjacency_matrix1,\n",
    "                node_coords=node_coords, node_color='k',\n",
    "                edge_cmap='PuOr', node_size=30, axes=axes,\n",
    "                alpha=0.7, edge_kwargs={'linewidth': 3},\n",
    "                edge_threshold='0%', display_mode=\"lzry\", edge_vmax=4,\n",
    "                edge_vmin=-200)\n",
    "\n",
    "plot_connectome(adjacency_matrix=adjacency_matrix2,\n",
    "                node_coords=node_coords, node_color='k',\n",
    "                edge_cmap='PuOr', node_size=30, axes=axes, \n",
    "                alpha=0.7, edge_kwargs={'linewidth': 3},\n",
    "                edge_threshold='0%', display_mode=\"lzry\", edge_vmax=15,\n",
    "                edge_vmin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting networks parcels\n",
    "brain_labels = mne.read_labels_from_annot(subject='fsaverage', parc='aparc')[:-1] # 69 labels\n",
    "bl_names = [bl.name for bl in brain_labels]\n",
    "clr = 0.85 \n",
    "silhouette_dict = {'color':(0.8, 0.8, 0.8), 'linewidth':1, 'alpha':0.8, 'decimate':0.9}\n",
    "hemi = 'both'\n",
    "color = 'red'\n",
    "mode = 'rgb'\n",
    "alpha = 0.6\n",
    "\n",
    "# vsn\n",
    "views = 'medial'\n",
    "Brain = mne.viz.get_brain_class()\n",
    "brain_kwargs = dict(alpha=1, background=\"white\", cortex=[(clr, clr, clr), (clr, clr, clr)], size=(800, 600), views=views)\n",
    "brain = Brain(\"fsaverage\", hemi=hemi, surf=\"pial_semi_inflated\", silhouette=silhouette_dict, **brain_kwargs)\n",
    "for lb in vsn_lbs:\n",
    "    for lb_n in bl_names:\n",
    "        if lb == lb_n:\n",
    "            idx = bl_names.index(lb)\n",
    "            brain.add_label(label=brain_labels[idx], hemi=hemi, color=color, borders=False, alpha=alpha)\n",
    "            brain.save_image(filename='/Users/payamsadeghishabestari/Prime/shap/network+aun/vsn.png', mode=mode)\n",
    "\n",
    "# smn\n",
    "views = 'dorsal'\n",
    "Brain = mne.viz.get_brain_class()\n",
    "brain_kwargs = dict(alpha=1, background=\"white\", cortex=[(clr, clr, clr), (clr, clr, clr)], size=(800, 600), views=views)\n",
    "brain = Brain(\"fsaverage\", hemi=hemi, surf=\"pial_semi_inflated\", silhouette=silhouette_dict, **brain_kwargs)\n",
    "for lb in smn_lbs:\n",
    "    for lb_n in bl_names:\n",
    "        if lb == lb_n:\n",
    "            idx = bl_names.index(lb)\n",
    "            brain.add_label(label=brain_labels[idx], hemi=hemi, color=color, borders=False, alpha=alpha)\n",
    "            brain.save_image(filename='/Users/payamsadeghishabestari/Prime/shap/network+aun/smn.png', mode=mode)\n",
    "\n",
    "\n",
    "# aun\n",
    "views = 'lateral'\n",
    "Brain = mne.viz.get_brain_class()\n",
    "brain_kwargs = dict(alpha=1, background=\"white\", cortex=[(clr, clr, clr), (clr, clr, clr)], size=(800, 600), views=views)\n",
    "brain = Brain(\"fsaverage\", hemi=hemi, surf=\"pial_semi_inflated\", silhouette=silhouette_dict, **brain_kwargs)\n",
    "for lb in aun_lbs:\n",
    "    for lb_n in bl_names:\n",
    "        if lb == lb_n:\n",
    "            idx = bl_names.index(lb)\n",
    "            brain.add_label(label=brain_labels[idx], hemi=hemi, color=color, borders=False, alpha=alpha)\n",
    "            brain.save_image(filename='/Users/payamsadeghishabestari/Prime/shap/network+aun/aun.png', mode=mode)\n",
    "\n",
    "# van\n",
    "views = 'lateral'\n",
    "Brain = mne.viz.get_brain_class()\n",
    "brain_kwargs = dict(alpha=1, background=\"white\", cortex=[(clr, clr, clr), (clr, clr, clr)], size=(800, 600), views=views)\n",
    "brain = Brain(\"fsaverage\", hemi=hemi, surf=\"pial_semi_inflated\", silhouette=silhouette_dict, **brain_kwargs)\n",
    "for lb in van_lbs:\n",
    "    for lb_n in bl_names:\n",
    "        if lb == lb_n:\n",
    "            idx = bl_names.index(lb)\n",
    "            brain.add_label(label=brain_labels[idx], hemi=hemi, color=color, borders=False, alpha=alpha)\n",
    "            brain.save_image(filename='/Users/payamsadeghishabestari/Prime/shap/network+aun/van.png', mode=mode)\n",
    "\n",
    "# # dan\n",
    "views = 'lateral'\n",
    "Brain = mne.viz.get_brain_class()\n",
    "brain_kwargs = dict(alpha=1, background=\"white\", cortex=[(clr, clr, clr), (clr, clr, clr)], size=(800, 600), views=views)\n",
    "brain = Brain(\"fsaverage\", hemi=hemi, surf=\"pial_semi_inflated\", silhouette=silhouette_dict, **brain_kwargs)\n",
    "for lb in dan_lbs:\n",
    "    for lb_n in bl_names:\n",
    "        if lb == lb_n:\n",
    "            idx = bl_names.index(lb)\n",
    "            brain.add_label(label=brain_labels[idx], hemi=hemi, color=color, borders=False, alpha=alpha)\n",
    "            brain.save_image(filename='/Users/payamsadeghishabestari/Prime/shap/network+aun/dan.png', mode=mode)\n",
    "\n",
    "# # fpn\n",
    "views = 'dorsal'\n",
    "Brain = mne.viz.get_brain_class()\n",
    "brain_kwargs = dict(alpha=1, background=\"white\", cortex=[(clr, clr, clr), (clr, clr, clr)], size=(800, 600), views=views)\n",
    "brain = Brain(\"fsaverage\", hemi=hemi, surf=\"pial_semi_inflated\", silhouette=silhouette_dict, **brain_kwargs)\n",
    "for lb in fpn_lbs:\n",
    "    for lb_n in bl_names:\n",
    "        if lb == lb_n:\n",
    "            idx = bl_names.index(lb)\n",
    "            brain.add_label(label=brain_labels[idx], hemi=hemi, color=color, borders=False, alpha=alpha)\n",
    "            brain.save_image(filename='/Users/payamsadeghishabestari/Prime/shap/network+aun/fpn.png', mode=mode)\n",
    "\n",
    "# dmn\n",
    "views = 'medial'\n",
    "Brain = mne.viz.get_brain_class()\n",
    "brain_kwargs = dict(alpha=1, background=\"white\", cortex=[(clr, clr, clr), (clr, clr, clr)], size=(800, 600), views=views)\n",
    "brain = Brain(\"fsaverage\", hemi='lh', surf=\"pial_semi_inflated\", silhouette=silhouette_dict, **brain_kwargs)\n",
    "for lb in dmn_lbs:\n",
    "    for lb_n in bl_names:\n",
    "        if lb == lb_n and lb[-2:]=='lh':\n",
    "            idx = bl_names.index(lb)\n",
    "            brain.add_label(label=brain_labels[idx], hemi='hemi', color=color, borders=False, alpha=alpha)\n",
    "            brain.save_image(filename='/Users/payamsadeghishabestari/Prime/shap/network+aun/dmn.png', mode=mode)\n",
    "\n",
    "# dgn\n",
    "views = 'medial'\n",
    "Brain = mne.viz.get_brain_class()\n",
    "brain_kwargs = dict(alpha=1, background=\"white\", cortex=[(clr, clr, clr), (clr, clr, clr)], size=(800, 600), views=views)\n",
    "brain = Brain(\"fsaverage\", hemi='lh', surf=\"pial_semi_inflated\", silhouette=silhouette_dict, **brain_kwargs)\n",
    "for lb in dgn_lbs:\n",
    "    for lb_n in bl_names:\n",
    "        if lb == lb_n and lb[-2:]=='lh':\n",
    "            idx = bl_names.index(lb)\n",
    "            brain.add_label(label=brain_labels[idx], hemi='hemi', color=color, borders=False, alpha=alpha)\n",
    "            brain.save_image(filename='/Users/payamsadeghishabestari/Prime/shap/network+aun/dgn.png', mode=mode)\n",
    "\n",
    "# lbn\n",
    "views = 'medial'\n",
    "Brain = mne.viz.get_brain_class()\n",
    "brain_kwargs = dict(alpha=1, background=\"white\", cortex=[(clr, clr, clr), (clr, clr, clr)], size=(800, 600), views=views)\n",
    "brain = Brain(\"fsaverage\", hemi='lh', surf=\"pial_semi_inflated\", silhouette=silhouette_dict, **brain_kwargs)\n",
    "for lb in lbn_lbs:\n",
    "    for lb_n in bl_names:\n",
    "        if lb == lb_n and lb[-2:]=='lh':\n",
    "            idx = bl_names.index(lb)\n",
    "            brain.add_label(label=brain_labels[idx], hemi='hemi', color=color, borders=False, alpha=alpha)\n",
    "            brain.save_image(filename='/Users/payamsadeghishabestari/Prime/shap/network+aun/lbn.png', mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### extracting adjacency matrix in alpha and gamma for networks\n",
    "folder_path = '/Users/payamsadeghishabestari/eeg_data/Regensburg/connectivity/networks_connectivity'  \n",
    "alpha_files = []; gamma_files = []\n",
    "if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "    files_in_folder = sorted(os.listdir(folder_path), key=lambda x: os.path.getmtime(os.path.join(folder_path, x)))\n",
    "    for filename in files_in_folder:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path) and file_path.endswith(\".npy\"):\n",
    "            if 'alpha' in file_path:\n",
    "                alpha_files.append(file_path)\n",
    "            if 'gamma' in file_path:\n",
    "                gamma_files.append(file_path)\n",
    "\n",
    "alpha_as = []\n",
    "gamma_as = []\n",
    "for file_a, file_g in zip(alpha_files, gamma_files):\n",
    "    alpha_ss = np.load(file_a, allow_pickle=True)\n",
    "    alpha_as.append(alpha_ss.mean(axis=0))\n",
    "    gamma_ss = np.load(file_g, allow_pickle=True)\n",
    "    gamma_as.append(gamma_ss.mean(axis=0))\n",
    "\n",
    "alpha_con_net = np.array(alpha_as).mean(axis=0)\n",
    "gamma_con_net = np.array(gamma_as).mean(axis=0)\n",
    "\n",
    "## plotting network circular graph\n",
    "# color='#9467bd' # #ff7f0e\n",
    "networks = ['vsn', 'smn', 'aun', 'van', 'dan', 'fpn', 'dmn', 'dgn', 'lbn']\n",
    "set2_palette = sns.color_palette(\"tab10\")\n",
    "\n",
    "for i, j in product(range(9), range(9)):\n",
    "    if not ((i, j)==(6, 3) or (i, j)==(4, 2) or (i, j)==(7, 3)):\n",
    "        alpha_con_net[i][j] = 0\n",
    "\n",
    "for i, j in product(range(9), range(9)):\n",
    "    if not ((i, j)==(2, 1) or (i, j)==(7, 3) or (i, j)==(8, 2) or (i, j)==(5, 2) or (i, j)==(7, 2)):\n",
    "        gamma_con_net[i][j] = 0\n",
    "\n",
    "gamma_con_net = np.zeros(shape=(9, 9))\n",
    "\n",
    "gamma_con_net[2][1] = 1\n",
    "gamma_con_net[7][3] = -1\n",
    "gamma_con_net[8][2] = 1\n",
    "gamma_con_net[5][2] = -1\n",
    "gamma_con_net[7][2] = 1\n",
    "gamma_con_net[5][1] = 1\n",
    "gamma_con_net[4][3] = 1\n",
    "gamma_con_net[8][3] = -1\n",
    "gamma_con_net[3][1] = -1\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(11,11), subplot_kw={'projection': 'polar'})\n",
    "plot_connectivity_circle(con=alpha_con_net, node_names=networks, n_lines=3, vmin=-1, vmax=100,\n",
    "                        textcolor='black', node_edgecolor='white', colormap='coolwarm',\n",
    "                        fontsize_names=10, title=None, facecolor='white',\n",
    "                        colorbar=False, linewidth=2.5, node_width=30,\n",
    "                        node_height=1.8, node_colors=set2_palette, ax=ax)\n",
    "\n",
    "# plot_connectivity_circle(con=gamma_con_net, node_names=networks, n_lines=5, vmin=-10, vmax=0,\n",
    "#                         textcolor='black', node_edgecolor='white', colormap='PuOr',\n",
    "#                         fontsize_names=10, title=None, facecolor='white',\n",
    "#                         colorbar=False, linewidth=2.5, node_width=30,\n",
    "#                         node_height=1.8, node_colors=set2_palette, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "shap.dependence_plot(\"Alpha power\", list(shap_new)[1], X_test_new, alpha=0.7,\n",
    "                    feature_names=feature_names, interaction_index=\"Gamma power\",\n",
    "                    cmap='coolwarm', ax=ax)\n",
    "ax.set_ylim([-0.3, 0.3])\n",
    "ax.hlines(y=0, xmin=0 , xmax=1.1, colors='k', linestyles='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "shap.dependence_plot(\"Gamma power\", list(shap_new)[1], X_test_new, alpha=0.7,\n",
    "                    feature_names=feature_names, interaction_index=\"Gamma entropy\",\n",
    "                    cmap='coolwarm', ax=ax)\n",
    "ax.set_ylim([-0.3, 0.3])\n",
    "ax.hlines(y=0, xmin=0 , xmax=1.1, colors='k', linestyles='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(list(shap_new)[1], features=X_test_new, feature_names=feature_names, alpha=0.1,\n",
    "                    max_display=12, cmap='coolwarm', plot_size=(8,5)) # barplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP plot for Source space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "n_sel_features = 100\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(features_array_after_corr,\n",
    "                                                                            label_array, group_array,\n",
    "                                                                            test_size=0.3, train_size=0.7,\n",
    "                                                                            shuffle = True, random_state=None)\n",
    "mask_train = np.isnan(X_train)\n",
    "mask_test = np.isnan(X_test)\n",
    "nan_indices_train = np.where(mask_train)\n",
    "nan_indices_test = np.where(mask_test)\n",
    "for i in range(len(nan_indices_train[0])):\n",
    "    X_train[nan_indices_train[0][i]][nan_indices_train[1][i]] = 1\n",
    "for i in range(len(nan_indices_test[0])):\n",
    "    X_test[nan_indices_test[0][i]][nan_indices_test[1][i]] = 1\n",
    "\n",
    "n_scores = cross_val_score(model, X_train, y_train, groups = group_train ,scoring='accuracy', cv=10, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# reporting and features overview\n",
    "importance = model.feature_importances_\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'accuracy of the model checked with test data without eliminating features: {accuracy}')\n",
    "\n",
    "selected_features_idx = np.argsort(importance)[-n_sel_features:]\n",
    "\n",
    "# results align with the plot\n",
    "brain_labels = mne.read_labels_from_annot(subject='fsaverage', parc='aparc', verbose=False)\n",
    "for index, imp in zip(np.array(keep_idxs)[selected_features_idx], np.sort(importance)[-n_sel_features:][:]):\n",
    "    if 2*68 <= index < 3*68:\n",
    "        clas = 'alpha'\n",
    "        new_index = index - 2*68\n",
    "        print(f\"{brain_labels[new_index].name} in class {clas} with importance: {round(imp, 3)}\")\n",
    "    \n",
    "    if 4*68 <= index < 5*68:\n",
    "        clas = 'gamma'  \n",
    "        new_index = index - 4*68 \n",
    "        print(f\"{brain_labels[new_index].name} in class {clas} with importance: {round(imp, 3)}\")\n",
    "\n",
    "# only select some indexes and plot summaryplot for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer.shap_values(X_test) # computed for all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = shap.summary_plot(shap_values[1], features=X_test, feature_names=None,\n",
    "                    max_display=20, alpha=0.1, cmap='coolwarm') # barplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling the Clf results and recheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## original classification\n",
    "n_sel_features = 100\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(features_array_after_corr,\n",
    "                                                                            label_array, group_array,\n",
    "                                                                            test_size=0.3, train_size=0.7,\n",
    "                                                                            shuffle = True, random_state=None)\n",
    "mask_train = np.isnan(X_train)\n",
    "mask_test = np.isnan(X_test)\n",
    "nan_indices_train = np.where(mask_train)\n",
    "nan_indices_test = np.where(mask_test)\n",
    "for i in range(len(nan_indices_train[0])):\n",
    "    X_train[nan_indices_train[0][i]][nan_indices_train[1][i]] = 1\n",
    "for i in range(len(nan_indices_test[0])):\n",
    "    X_test[nan_indices_test[0][i]][nan_indices_test[1][i]] = 1\n",
    "\n",
    "n_scores = cross_val_score(model, X_train, y_train, groups = group_train ,scoring='accuracy', cv=10, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# reporting and features overview\n",
    "importance = model.feature_importances_\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'accuracy of the model checked with test data without eliminating features: {accuracy}')\n",
    "\n",
    "## after shuffling\n",
    "label_array_copy = label_array\n",
    "random.shuffle(label_array_copy)\n",
    "\n",
    "n_sel_features = 100\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(features_array_after_corr,\n",
    "                                                                            label_array_copy, group_array,\n",
    "                                                                            test_size=0.3, train_size=0.7,\n",
    "                                                                            shuffle = True, random_state=None)\n",
    "mask_train = np.isnan(X_train)\n",
    "mask_test = np.isnan(X_test)\n",
    "nan_indices_train = np.where(mask_train)\n",
    "nan_indices_test = np.where(mask_test)\n",
    "for i in range(len(nan_indices_train[0])):\n",
    "    X_train[nan_indices_train[0][i]][nan_indices_train[1][i]] = 1\n",
    "for i in range(len(nan_indices_test[0])):\n",
    "    X_test[nan_indices_test[0][i]][nan_indices_test[1][i]] = 1\n",
    "\n",
    "n_scores = cross_val_score(model, X_train, y_train, groups = group_train ,scoring='accuracy', cv=10, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# reporting and features overview\n",
    "importance = model.feature_importances_\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'accuracy of the model checked with test data without eliminating features: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stats on Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aaeeg = pd.read_excel('/Users/payamsadeghishabestari/eeg_data/Regensburg/transfer_147155_files_ac07b771/aaeeg_rsEEG/AAEEG_TL_data.xlsx')\n",
    "df_neuropren = pd.read_excel('/Users/payamsadeghishabestari/eeg_data/Regensburg/transfer_147155_files_ac07b771/neuropren_rsEEG/neuropren_TL_data.xlsx')\n",
    "df = pd.read_excel('/Users/payamsadeghishabestari/Prime/PRIME_beh-data_complete.xlsx')\n",
    "\n",
    "df_aaeeg['WN_TL_0'] = df_aaeeg['WN_TL_0'].apply(lambda x: 0 if x > 90 else 1)\n",
    "df_neuropren['b1_WN_0'] = df_neuropren['b1_WN_0'].apply(lambda x: 0 if x > 90 else 1)\n",
    "df_aaeeg = df_aaeeg.rename(columns={'WN_TL_0': 'BATS'})\n",
    "df_neuropren = df_neuropren.rename(columns={'b1_WN_0': 'BATS'})\n",
    "df1 = df_aaeeg[[\"random_id\", \"BATS\"]]\n",
    "df2 = df_neuropren[[\"random_id\", \"BATS\"]]\n",
    "df3 = pd.concat((df1, df2))\n",
    "df = df.merge(right=df3, on=\"random_id\")\n",
    "df1 = df[df[\"BATS\"] == 1]\n",
    "df2 = df[df[\"BATS\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"sex\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df1.columns[5:-1]:\n",
    "    print(f\"{column} -> mean: {round(df1[column].mean(), 2)}, std: {round(df1[column].std(), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df1.columns[5:-1]:\n",
    "    print(f\"{column} -> mean: {round(df2[column].mean(), 2)}, std: {round(df2[column].std(), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_p_values = {}\n",
    "for column in df1.columns[5:-1]:\n",
    "    if column == \"sex\":\n",
    "        contingency_table = pd.crosstab(df[column], df[\"BATS\"])\n",
    "        t_statistic, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "    else:\n",
    "        t_statistic, p_value = ttest_ind(df1[column], df2[column], permutations=0)\n",
    "    \n",
    "    t_p_values[column] = {'t-statistic': round(t_statistic, 3), 'p-value': round(p_value, 3)}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "result_df = pd.DataFrame(t_p_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
